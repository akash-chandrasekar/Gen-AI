{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e17f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e746d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['lang_chain_api'] = os.getenv('lang_chain_api')\n",
    "os.environ['lang_chain_v2'] = 'true'\n",
    "os.environ['lang_chain_project'] = os.getenv('lang_chain_project')  \n",
    "os.environ['groq_api'] = os.getenv('groq_api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e855520b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['groq_api']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5912547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc9eaba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x12b2be0f0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x12b2bd9a0> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model='openai/gpt-oss-120b', api_key=os.environ['groq_api'])\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57036121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Vellore is a city in the southern Indian state of **Tamil\\u202fNadu**.  \\n\\n- **Geographic location**: It lies in the north‚Äëwestern part of Tamil\\u202fNadu, about 135\\u202fkm (‚âà84\\u202fmi) west of Chennai and roughly 200\\u202fkm (‚âà125\\u202fmi) south of Bengaluru.  \\n- **Coordinates**: Approximately **12.92¬∞\\u202fN latitude,\\u202f79.13¬∞\\u202fE longitude**.  \\n- **River**: The city is situated on the banks of the **Ponnaiyar (also called South Pennar) River**.  \\n- **Administrative district**: Vellore is the headquarters of **Vellore District** (which, as of 2023, has been split into Vellore, Tirupattur and Ranipet districts, but the city itself remains in Vellore District).  \\n\\nVellore is well‚Äëknown for institutions such as the **Christian Medical College & Hospital (CMC)**, the **Vellore Institute of Technology (VIT)**, and historic sites like the **Vellore Fort**.' additional_kwargs={'reasoning_content': 'The user asks: \"Where is vellore presented?\" Likely they mean \"Vellore\" location. Vellore is a city in Tamil Nadu, India. Could be asking about where Vellore is presented? Could be a typo. Probably answer: Vellore is in the state of Tamil Nadu, in southern India, near the border with Andhra Pradesh, located on the banks of the Ponnaiyar River, about 135 km west of Chennai. Also mention it\\'s known for Christian Medical College, etc. Provide details.\\n\\nWe should answer concisely.'} response_metadata={'token_usage': {'completion_tokens': 359, 'prompt_tokens': 77, 'total_tokens': 436, 'completion_time': 0.895675745, 'prompt_time': 0.005119441, 'queue_time': 0.053784868, 'total_time': 0.900795186, 'completion_tokens_details': {'reasoning_tokens': 117}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_1d1727abc9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--ed9fbd02-0b68-483e-836a-7f468b93fa56-0' usage_metadata={'input_tokens': 77, 'output_tokens': 359, 'total_tokens': 436}\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "respones = llm.invoke('Where is vellore presented?')\n",
    "print(respones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "befa0be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vellore is a city in the southern Indian state of **Tamil‚ÄØNadu**.  \n",
      "\n",
      "- **Geographic location**: It lies in the north‚Äëwestern part of Tamil‚ÄØNadu, about 135‚ÄØkm (‚âà84‚ÄØmi) west of Chennai and roughly 200‚ÄØkm (‚âà125‚ÄØmi) south of Bengaluru.  \n",
      "- **Coordinates**: Approximately **12.92¬∞‚ÄØN latitude,‚ÄØ79.13¬∞‚ÄØE longitude**.  \n",
      "- **River**: The city is situated on the banks of the **Ponnaiyar (also called South Pennar) River**.  \n",
      "- **Administrative district**: Vellore is the headquarters of **Vellore District** (which, as of 2023, has been split into Vellore, Tirupattur and Ranipet districts, but the city itself remains in Vellore District).  \n",
      "\n",
      "Vellore is well‚Äëknown for institutions such as the **Christian Medical College & Hospital (CMC)**, the **Vellore Institute of Technology (VIT)**, and historic sites like the **Vellore Fort**.\n"
     ]
    }
   ],
   "source": [
    "print(respones.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afaad283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3731196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'You are the expert in tour guide. Provide me best 5 places to visit in summer and then describe why should I need to visit that place'),\n",
    "        ('user','{input}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c1b74a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "268b5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({'input': 'I want to visit some places in india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "425c15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='## Top\\u202f5 Summer‚ÄëFriendly Destinations in India  \\n*(Ideal for June\\u202f‚Äì\\u202fSeptember when the plains can be scorching, these places offer cool weather, natural beauty, and unforgettable experiences.)*\\n\\n| # | Destination | Best Time in Summer | What Makes It Special | Must‚ÄëDo Highlights |\\n|---|-------------|----------------------|-----------------------|--------------------|\\n| **1** | **Leh‚ÄëLadakh, Jammu & Kashmir** | May\\u202f‚Äì\\u202fSeptember (high‚Äëaltitude desert stays comfortably 10‚Äë25\\u202f¬∞C) | Dramatic Himalayan landscapes, crystal‚Äëclear lakes, ancient Buddhist monasteries, and a unique Tibetan‚Äëinfluenced culture. | ‚Ä¢ Ride the **Manali‚ÄëLeh Highway** for epic mountain passes (e.g., Khardung La). <br>‚Ä¢ Sunrise at **Pangong Tso** and **Tso Moriri** lakes. <br>‚Ä¢ Visit **Thiksey** & **Hemis** monasteries. <br>‚Ä¢ Trek to **Markha Valley** or **Zanskar River** for adventure. |\\n| **2** | **Coorg (Kodagu), Karnataka** | October\\u202f‚Äì\\u202fMay is ideal, but the early summer months (April‚ÄëJune) are still pleasant (20‚Äë28\\u202f¬∞C) and less crowded. | Lush coffee‚Äëplantation hills, mist‚Äëcovered valleys, waterfalls, and rich Kodava heritage. | ‚Ä¢ Coffee‚Äëplantation tours & tasting at **Tibetan Monastery Caf√©**. <br>‚Ä¢ Trek to **Abbey Falls**, **Iruppu Falls**, and **Chettiar\\u202fPeak**. <br>‚Ä¢ Explore the historic **Madikeri Fort** and **Raja‚Äôs Seat**. <br>‚Ä¢ Stay in a homestay for authentic Kodava cuisine. |\\n| **3** | **Munnar, Kerala** | March\\u202f‚Äì\\u202fMay (temperatures 15‚Äë25\\u202f¬∞C) ‚Äì perfect before the monsoon hits. | Rolling tea‚Äëgarden terraces, misty hills, and abundant wildlife in the Western Ghats. | ‚Ä¢ Guided walk through **Tea Museum** and **Tea Plantations**. <br>‚Ä¢ Spot Nilgiri tahr & exotic birds in **Eravikulam National Park**. <br>‚Ä¢ Visit **Mattupetty Dam** for a boat ride. <br>‚Ä¢ Hike to **Anamudi Peak** (South Asia‚Äôs highest point). |\\n| **4** | **Darjeeling, West Bengal** | Late May\\u202f‚Äì\\u202fJune (15‚Äë22\\u202f¬∞C) ‚Äì before the heavy monsoon. | ‚ÄúQueen of the Hills‚Äù with tea estates, the iconic Darjeeling Himalayan Railway, and panoramic Himalayan vistas. | ‚Ä¢ Ride the **Toy Train** (UNESCO World Heritage). <br>‚Ä¢ Watch sunrise over **Kanchenjunga** from **Tiger Hill**. <br>‚Ä¢ Stroll through **Happy Valley Tea Gardens**. <br>‚Ä¢ Visit **Batasia Loop**, **Ghoom Monastery**, and the **Padmaja Naidu Himalayan Zoological Park**. |\\n| **5** | **Spiti Valley, Himachal Pradesh** | Late May\\u202f‚Äì\\u202fearly July (10‚Äë20\\u202f¬∞C) ‚Äì before the monsoon blocks the roads. | A high‚Äëaltitude desert with stark, moon‚Äëlike landscapes, ancient monasteries, and a tranquil, off‚Äëthe‚Äëgrid vibe. | ‚Ä¢ Explore **Key Monastery** and **Tabo Monastery** (both 1000‚Äëyear‚Äëold). <br>‚Ä¢ Drive the dramatic **Kinnaur‚ÄëSpiti Highway** (one of the world‚Äôs most scenic roads). <br>‚Ä¢ Hike to **Chandratal Lake** (the ‚ÄúMoon Lake‚Äù). <br>‚Ä¢ Experience the unique culture of the **Bhotia** people in villages like **Kaza** and **Langza**. |\\n\\n---\\n\\n### Why These Places Are Perfect for a Summer Indian Getaway\\n\\n1. **Cool Climate & Comfortable Temperatures**  \\n   - All five destinations sit at elevations above 1,500\\u202fm, where summer heat is muted. You‚Äôll enjoy crisp mornings, pleasant afternoons, and evenings that are perfect for stargazing.\\n\\n2. **Natural Beauty That Changes With the Seasons**  \\n   - From the turquoise high‚Äëaltitude lakes of Ladakh to the mist‚Äëclad tea gardens of Munnar, each location showcases a different facet of India‚Äôs geography‚Äîmountains, valleys, forests, and deserts.\\n\\n3. **Cultural Richness & Authentic Experiences**  \\n   - **Ladakh** and **Spiti** let you walk among centuries‚Äëold Buddhist monasteries.  \\n   - **Coorg** offers a glimpse into the vibrant Kodava traditions and cuisine.  \\n   - **Darjeeling** blends colonial heritage with Tibetan Buddhist culture.  \\n   - **Munnar** preserves the legacy of tea‚Äëplantation workers and indigenous tribes.\\n\\n4. **Adventure Opportunities**  \\n   - Trekking, mountain biking, river rafting, and high‚Äëaltitude camping are readily available. Even if you prefer a relaxed pace, scenic drives, wildlife safaris, and gentle walks are abundant.\\n\\n5. **Less Crowded Than Peak Tourist Seasons**  \\n   - Summer is the off‚Äëpeak period for most hill stations, meaning you‚Äôll encounter shorter lines, more personal interactions with locals, and better chances to secure boutique accommodations.\\n\\n6. **Accessibility & Infrastructure**  \\n   - All five destinations are reachable by a combination of flights (e.g., Delhi ‚Üí Leh, Bangalore ‚Üí Coorg via Mysore, Kolkata ‚Üí Bagdogra ‚Üí Darjeeling) and well‚Äëmaintained roads. Tourist facilities‚Äîfrom comfortable homestays to luxury resorts‚Äîcater to a range of budgets.\\n\\n---\\n\\n### Quick Planning Tips\\n\\n| Tip | Details |\\n|-----|---------|\\n| **Acclimatization** | For Ladakh & Spiti, spend at least 2‚Äì3 days at moderate altitude (e.g., Leh or Kaza) before heading to higher passes. |\\n| **Travel Light** | Pack layers: a warm jacket for evenings, a lightweight fleece for daytime, and breathable shirts. Sun protection (hat, sunscreen) is essential even at altitude. |\\n| **Permits** | Some areas (e.g., certain parts of Ladakh & Spiti) require Inner Line Permits for foreign nationals; Indian tourists usually need an **ID proof** and may need a **Protected Area Permit** for specific villages. |\\n| **Best Transport** | Hire a reliable local driver for mountain roads; they know the safest routes and can adapt to sudden weather changes. |\\n| **Health & Safety** | Carry a basic first‚Äëaid kit, stay hydrated, and avoid over‚Äëexertion at high altitude. In monsoon‚Äëprone zones (e.g., Munnar), keep rain gear handy. |\\n\\n---\\n\\n### Sample 10‚ÄëDay Itinerary (Mix & Match)\\n\\n| Day | Morning | Afternoon | Evening |\\n|-----|---------|-----------|---------|\\n| **1‚Äë2** | Fly to **Leh**, acclimatize, explore Leh market | Visit **Shanti Stupa**, **Leh Palace** | Sunset at **Khardung La** viewpoint |\\n| **3‚Äë4** | Drive to **Pangong Tso**, boat ride | Head to **Nubra Valley** (Hunder Sand Dunes) | Stay in a desert camp |\\n| **5** | Fly to **Bengaluru**, transfer to **Coorg** | Coffee plantation tour | Traditional **Kodava** dinner |\\n| **6‚Äë7** | Trek to **Abbey Falls**, **Talakaveri** (source of the Cauvery) | Visit **Madikeri Fort** & **Raja‚Äôs Seat** | Relax at a spice‚Äëspa |\\n| **8** | Fly to **Kochi**, drive to **Munnar** | Tea garden walk, **Tea Museum** | Sunset over **Mullaperiyar Dam** |\\n| **9** | Early hike in **Eravikulam NP** (Anamudi) | Picnic at **Mattupetty Lake** | Night under the stars (optional homestay) |\\n| **10** | Return to **Kochi** for departure or extend to **Alleppey** for a house‚Äëboat cruise |\\n\\n*(Feel free to swap destinations‚Äîe.g., replace Coorg with Darjeeling, or add Spiti after Ladakh for a longer high‚Äëaltitude adventure.)*\\n\\n---\\n\\n**Enjoy your summer adventure in India!** üåÑ‚ú® If you need help with bookings, detailed trekking permits, or customizing the itinerary to match your interests (wildlife, photography, yoga retreats, etc.), just let me know‚ÄîI‚Äôm happy to dive deeper.' additional_kwargs={'reasoning_content': 'The user wants best 5 places to visit in India in summer, and description why they should visit. As a tour guide expert. Provide list with reasons. Must follow policies. No disallowed content. Provide helpful answer.'} response_metadata={'token_usage': {'completion_tokens': 1874, 'prompt_tokens': 111, 'total_tokens': 1985, 'completion_time': 4.496888047, 'prompt_time': 0.006343943, 'queue_time': 0.054521417, 'total_time': 4.50323199, 'completion_tokens_details': {'reasoning_tokens': 46}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a28df4bce5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--5ccc2dcf-1b06-40c8-a963-c0dd47d3f7f3-0' usage_metadata={'input_tokens': 111, 'output_tokens': 1874, 'total_tokens': 1985}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e8e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1054d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a457a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f058a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5 Must‚ÄëVisit Summer Destinations in India  \n",
      "*(Ideal for June‚ÄØ‚Äì‚ÄØSeptember when the plains are scorching, these spots offer cool weather, stunning scenery, and unique cultural experiences.)*\n",
      "\n",
      "| # | Destination | What Makes It Perfect for Summer | Highlights & Activities |\n",
      "|---|-------------|----------------------------------|--------------------------|\n",
      "| **1** | **Manali, Himachal Pradesh** | Nestled in the Kullu Valley at 2,050‚ÄØm, temperatures hover around 15‚Äë25‚ÄØ¬∞C in summer. The crisp mountain air is a welcome escape from the heat of the plains. | ‚Ä¢ Rohtang & Solang Pass for snow‚Äëcapped vistas (early summer) <br>‚Ä¢ Paragliding, river rafting on the Beas <br>‚Ä¢ Old Manali‚Äôs caf√©s & Tibetan market <br>‚Ä¢ Trek to Jogini Waterfall or the Great Himalayan National Park |\n",
      "| **2** | **Munnar, Kerala** | A tea‚Äëplantation haven at 1,600‚ÄØm, with mist‚Äëcovered hills and temperatures rarely above 28‚ÄØ¬∞C. The monsoon begins late‚ÄëJune, turning the landscape into a lush carpet of green. | ‚Ä¢ Tea‚Äëgarden walks & factory tours <br>‚Ä¢ Eravikulam National Park ‚Äì home of the endangered Nilgiri Tahr <br>‚Ä¢ Anamudi Peak trek (South India‚Äôs highest) <br>‚Ä¢ Spice‚Äëplantation visits & Ayurvedic spa treatments |\n",
      "| **3** | **Coorg (Kodagu), Karnataka** | Known as the ‚ÄúScotland of India,‚Äù Coorg sits at 1,200‚ÄØm and enjoys pleasant 20‚Äë30‚ÄØ¬∞C weather. Coffee estates, waterfalls, and dense forests make it a cool, aromatic retreat. | ‚Ä¢ Coffee‚Äëplantation homestays <br>‚Ä¢ Abbey Falls, Iruppu Falls, and Talacauvery (the river‚Äôs source) <br>‚Ä¢ Dubare Elephant Camp ‚Äì interact with gentle giants <br>‚Ä¢ Trek to Tadiandamol (the highest peak in Coorg) |\n",
      "| **4** | **Rishikesh & Dehradun, Uttarakhand** | Set on the banks of the Ganges at ~300‚ÄØm, these towns stay around 25‚Äë32‚ÄØ¬∞C in summer. The Himalayan foothills provide a breezy, spiritual atmosphere. | ‚Ä¢ Yoga & meditation retreats (Rishikesh is the world‚Äôs yoga capital) <br>‚Ä¢ White‚Äëwater rafting on the Ganges <br>‚Ä¢ Bungee‚Äëjumping & zip‚Äëlining at Mohan Chatti <br>‚Ä¢ Day‚Äëtrip to the serene Rajaji National Park or the tea gardens of Doiwala |\n",
      "| **5** | **Ladakh (Leh‚ÄëPangong), Jammu & Kashmir** | At 3,500‚ÄØm, Ladakh‚Äôs summer days are sunny and dry, with night temperatures dropping to 5‚Äë10‚ÄØ¬∞C‚Äîperfect for exploring the high desert without the extreme cold of winter. | ‚Ä¢ Drive the iconic Manali‚ÄëLeh Highway (spectacular vistas) <br>‚Ä¢ Pangong Tso & Tso Moriri lakes ‚Äì surreal turquoise waters <br>‚Ä¢ Monastery circuit: Hemis, Thiksey, Shey <br>‚Ä¢ Bactrian camel safari in Nubra Valley & trekking to Markha Valley |\n",
      "\n",
      "---\n",
      "\n",
      "### Why These Places Stand Out for a Summer Trip\n",
      "\n",
      "1. **Comfortable Climate** ‚Äì All five destinations sit at elevations that naturally moderate temperature, letting you enjoy outdoor activities without battling heatstroke or humidity.\n",
      "\n",
      "2. **Diverse Experiences in One Country**  \n",
      "   *From snow‚Äëcapped passes (Manali) to tea‚Äëgarden serenity (Munnar), from spiritual yoga retreats (Rishikesh) to high‚Äëaltitude desert beauty (Ladakh), you can sample vastly different Indian landscapes in a single vacation.*\n",
      "\n",
      "3. **Adventure & Wellness Combo**  \n",
      "   - **Adventure**: trekking, rafting, paragliding, camel safaris, and mountain biking are readily available.  \n",
      "   - **Wellness**: yoga ashrams, Ayurvedic spas, and tranquil tea/coffee estates provide restorative downtime.\n",
      "\n",
      "4. **Cultural Richness**  \n",
      "   - **Manali & Rishikesh**: Himalayan folklore, Tibetan Buddhist monasteries, and vibrant local festivals.  \n",
      "   - **Munnar & Coorg**: Indigenous tribal heritage, colonial‚Äëera plantations, and unique culinary flavors (e.g., Coorgi pork, Kerala‚Äôs appam).  \n",
      "   - **Ladakh**: Buddhist monasteries with ancient murals, traditional Ladakhi festivals (like Hemis Festival in July), and warm hospitality of the local people.\n",
      "\n",
      "5. **Accessibility & Infrastructure**  \n",
      "   - **Airports**: Leh (Ladakh), Dehradun (Uttarakhand), Kochi (near Munnar), and Bhuntar (near Manali) have regular flights from major Indian metros.  \n",
      "   - **Road & Rail**: Well‚Äëmaintained highways and rail links connect these spots to Delhi, Mumbai, Bengaluru, and Kolkata, making self‚Äëdrive or train travel feasible.  \n",
      "   - **Accommodation**: Options range from luxury resorts (e.g., The Oberoi in Munnar) to budget hostels, boutique homestays, and eco‚Äëlodges‚Äîso you can tailor the trip to your style and wallet.\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Planning Tips\n",
      "\n",
      "| Tip | Details |\n",
      "|-----|---------|\n",
      "| **Best Travel Window** | Early June‚ÄØ‚Äì‚ÄØmid‚ÄëJuly (before monsoon hits most hill stations) and late September‚ÄØ‚Äì‚ÄØearly November (post‚Äëmonsoon, still warm). |\n",
      "| **Packing Essentials** | Light jackets or fleece (nights can be chilly), comfortable trekking shoes, sunscreen, sunglasses, reusable water bottle, and a basic first‚Äëaid kit. |\n",
      "| **Local Transport** | Hire a reliable local driver for mountain roads (especially in Ladakh & Manali). In Kerala & Karnataka, auto‚Äërickshaws and shared cabs work well. |\n",
      "| **Health Precautions** | Acclimatize to altitude in Ladakh (spend a day in Leh before heading higher). Stay hydrated; carry oral rehydration salts if trekking. |\n",
      "| **Cultural Respect** | Dress modestly at temples/monasteries, remove shoes where required, and ask before photographing locals‚Äîespecially in tribal areas of Coorg and Ladakh. |\n",
      "\n",
      "---\n",
      "\n",
      "**Bottom line:** These five summer gems showcase India‚Äôs extraordinary geographic and cultural variety while keeping you comfortably cool. Whether you crave adrenaline‚Äëpumping adventure, peaceful meditation, or simply a breath of fresh mountain air, you‚Äôll find a perfect match among them. Happy travels! üåÑ‚úàÔ∏èüåø\n"
     ]
    }
   ],
   "source": [
    "respones = chain.invoke({'input': 'I want to visit some places in india'})\n",
    "print(respones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82d83346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "json_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc8c2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the queries .\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables = ['query'],\n",
    "    partial_variables={'format_instructions': json_parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5449b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2974f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Can you tell me gen ai', 'answer': 'Generative AI (Gen AI) refers to a class of artificial intelligence models that can create new content‚Äîsuch as text, images, audio, video, code, or even synthetic data‚Äîby learning patterns from large datasets. These models, often based on deep learning architectures like transformers (e.g., GPT, BERT) for text or diffusion networks and GANs for images, generate outputs that resemble the data they were trained on. Common applications include chatbots, code assistants, image synthesis, music composition, and content summarization. Gen AI works by predicting the next element in a sequence (e.g., the next word in a sentence) or by iteratively refining a noisy input until it matches the learned data distribution, enabling it to produce coherent and context‚Äëaware results.'}\n"
     ]
    }
   ],
   "source": [
    "respones = chain.invoke({'query':'Can you tell me gen ai'})\n",
    "print(respones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d47b3aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to response.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('response.json','w') as json_file:\n",
    "    json.dump(respones,json_file,indent=4)\n",
    "\n",
    "print(\"Response saved to response.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c60d39ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (from bs4) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (from beautifulsoup4->bs4) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8444925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "883f14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(\"https://blogs.nvidia.com/blog/what-is-agentic-ai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b8c029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Agentic AI?\\xa0 | NVIDIA Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tArtificial Intelligence Computing Leadership from NVIDIA\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nToggle Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nAI\\nData Center\\nDriving\\nGaming\\nPro Graphics\\nRobotics\\nHealthcare\\nStartups\\nAI Podcast\\nNVIDIA Life\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Agentic AI?\\xa0\\n\\n\\n\\t\\t\\t\\tAgentic AI uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\\t\\t\\t\\n\\n\\nOctober 22, 2024 by Erik Pounds \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tShare\\t\\t\\t\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n \\nEmail0 \\n\\n\\n  Editor‚Äôs note: The name of NIM Agent Blueprints was changed to NVIDIA\\xa0Blueprints in October 2024. All references to the name have been updated in this blog.\\nAI chatbots use generative AI to provide responses based on a single interaction. A person makes a query and the chatbot uses natural language processing to reply.\\nThe next frontier of artificial intelligence is agentic AI, which uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems. And it‚Äôs set to enhance productivity and operations across industries.\\nAn AI agent for customer service, for instance, could operate beyond simple question-answering. With agentic AI, it could check a user‚Äôs outstanding balance and recommend which accounts could pay it off ‚Äî all while waiting for the user to make a decision so it could complete the transaction accordingly when prompted.\\nAgentic AI systems ingest vast amounts of data from multiple data sources and third-party applications to independently analyze challenges, develop strategies and execute tasks. Businesses are implementing agentic AI to personalize customer service, streamline software development and even facilitate patient interactions.\\n\\nAgentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\\nHow Does Agentic AI Work?\\nAgentic AI uses a four-step process for problem-solving:\\n\\nPerceive: AI agents gather and process data from various sources, such as sensors, databases and digital interfaces. This involves extracting meaningful features, recognizing objects or identifying relevant entities in the environment.\\nReason: A large language model acts as the orchestrator, or reasoning engine, that understands tasks, generates solutions and coordinates specialized models for specific functions like content creation, visual processing or recommendation systems. This step uses techniques like retrieval-augmented generation (RAG) to access proprietary data sources and deliver accurate, relevant outputs.\\nAct: By integrating with external tools and software via application programming interfaces, agentic AI can quickly execute tasks based on the plans it has formulated. Guardrails can be built into AI agents to help ensure they execute tasks correctly. For example, a customer service AI agent may be able to process claims up to a certain amount, while claims above the amount would have to be approved by a human.\\nLearn: Agentic AI continuously improves through a feedback loop, or\\n‚Äúdata flywheel,‚Äù where the data generated from its interactions is fed into the system to enhance models. This ability to adapt and become more effective over time offers businesses a powerful tool for driving better decision-making and operational efficiency.\\n\\nFueling Agentic AI With Enterprise Data\\nAcross industries and job functions, generative AI is transforming organizations by turning vast amounts of data into actionable knowledge, helping employees work more efficiently.\\nAI agents build on this potential by accessing diverse data through accelerated AI query engines, which process, store and retrieve information to enhance generative AI models. A key technique for achieving this is RAG, which allows AI to intelligently retrieve the right information from a broader range of data sources.\\nOver time, AI agents learn and improve by creating a data flywheel, where data generated through interactions is fed back into the system, refining models and increasing their effectiveness.\\nThe end-to-end NVIDIA AI platform, including NVIDIA NeMo microservices for developing custom generative AI applications, provides the ability to manage and access data efficiently, which is crucial for building responsive agentic AI applications.\\nAgentic AI in Action\\nThe potential applications of agentic AI are vast, limited only by creativity and expertise. From simple tasks like generating and distributing content to more complex use cases such as orchestrating enterprise software, AI agents are transforming industries.\\n\\nCustomer Service: AI agents are improving customer support by enhancing self-service capabilities and automating routine communications. Over half of service professionals report significant improvements in customer interactions, reducing response times and boosting satisfaction.\\nThere‚Äôs also growing interest in digital humans ‚Äî AI-powered agents that embody a company‚Äôs brand and offer lifelike, real-time interactions to help sales representatives answer customer queries or solve issues directly when call volumes are high.\\nContent Creation: Agentic AI can help quickly create high-quality, personalized marketing content. Generative AI agents can save marketers an average of three hours per content piece, allowing them to focus on strategy and innovation. By streamlining content creation, businesses can stay competitive while improving customer engagement.\\nSoftware Engineering: AI agents are boosting developer productivity by automating repetitive coding tasks. It‚Äôs projected that by 2030 AI could automate up to 30% of work hours, freeing developers to focus on more complex challenges and drive innovation.\\nHealthcare: For doctors analyzing vast amounts of medical and patient data, AI agents can distill critical information to help them make better-informed care decisions. Automating administrative tasks and capturing clinical notes in patient appointments reduces the burden of time-consuming tasks, allowing doctors to focus on developing a doctor-patient connection.\\nAI agents can also provide 24/7 support, offering information on prescribed medication usage, appointment scheduling and reminders, and more to help patients adhere to treatment plans.\\nVideo analytics: Enterprises and public sector organizations around the world are developing video analytics AI agents to boost the capabilities of workforces that rely on visual information from a growing number of devices ‚Äî including cameras, IoT sensors and vehicles. Video analytics AI agents can analyze large amounts of live or archived videos, request tasks via natural language and perform complex operations like video search, summarization and visual question-answering. These agents can also be used to deliver anomaly alerts, draft incident reports, improve quality control through visual inspection and enhance predictive maintenance.\\nHow to Get Started\\nWith its ability to plan and interact with a wide variety of tools and software, agentic AI marks the next chapter of artificial intelligence, offering the potential to enhance productivity and revolutionize the way organizations operate.\\nTo accelerate the adoption of generative AI-powered applications and agents, NVIDIA Blueprints provide sample applications, reference code, sample data, tools and comprehensive documentation.\\nNVIDIA partners including Accenture are helping enterprises use agentic AI with solutions built with NVIDIA Blueprints.\\nVisit ai.nvidia.com to learn more about the tools and software NVIDIA offers to help enterprises build their own AI agents.\\xa0\\n\\n\\nCategories: Explainer | Generative AITags: Agentic AI | Artificial Intelligence | NVIDIA Blueprints \\n\\n\\n \\n\\n\\n\\n\\nAll NVIDIA News \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tCheers to AI: ADAM Robot Bartender Makes Drinks at Vegas Golden Knights Game\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tAs AI Grows More Complex, Model Builders Rely on NVIDIA\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tRide Into Adventure With Capcom‚Äôs ‚ÄòMonster Hunter Stories‚Äô Series in the Cloud\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tOpt-In NVIDIA Software Enables Data Center Fleet Management\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tHow NVIDIA H100 GPUs on CoreWeave‚Äôs AI Cloud Platform Delivered a Record-Breaking Graph500 Run\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate Information\\nAbout NVIDIA\\nCorporate Overview\\nTechnologies\\nNVIDIA Research\\nInvestors\\nSocial Responsibility\\nNVIDIA Foundation\\n \\n\\nGet Involved\\nForums\\nCareers\\nDeveloper Home\\nJoin the Developer Program\\nNVIDIA Partner Network\\nNVIDIA Inception\\nResources for Venture Capitalists\\nVenture Capital (NVentures)\\nTechnical Training\\nTraining for IT Professionals\\nProfessional Services for Data Science\\n \\n\\nNews & Events\\nNewsroom\\nNVIDIA Blog\\nNVIDIA Technical Blog\\nWebinars\\nStay Informed\\nEvents Calendar\\nNVIDIA GTC\\nNVIDIA On-Demand\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tExplore our regional blogs and other social networks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrivacy Policy\\nYour Privacy Choices\\nTerms of Service\\nAccessibility\\nCorporate Policies\\nProduct Security\\nContact\\n \\n\\t\\t\\tCopyright ¬© 2025 NVIDIA Corporation\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\tUSA - United States\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\xa0Share This\\xa0Facebook\\xa0LinkedIn \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare on Mastodon\\n\\n\\nEnter your Mastodon instance URL (optional)\\nShare\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare this Article\\n\\nFriend's Email Address\\n\\n\\n\\nYour Name\\n\\n\\n\\nYour Email Address\\n\\n\\n\\nComments\\n\\n\\n\\n Send Email\\n\\n\\nEmail sent!\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84ca6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6592f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='What Is Agentic AI?\\xa0 | NVIDIA Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tArtificial Intelligence Computing Leadership from NVIDIA\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nToggle Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nAI\\nData Center\\nDriving\\nGaming\\nPro Graphics\\nRobotics\\nHealthcare\\nStartups\\nAI Podcast\\nNVIDIA Life\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Agentic AI?\\xa0\\n\\n\\n\\t\\t\\t\\tAgentic AI uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\\t\\t\\t\\n\\n\\nOctober 22, 2024 by Erik Pounds \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tShare\\t\\t\\t\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n \\nEmail0'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Editor‚Äôs note: The name of NIM Agent Blueprints was changed to NVIDIA\\xa0Blueprints in October 2024. All references to the name have been updated in this blog.\\nAI chatbots use generative AI to provide responses based on a single interaction. A person makes a query and the chatbot uses natural language processing to reply.\\nThe next frontier of artificial intelligence is agentic AI, which uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems. And it‚Äôs set to enhance productivity and operations across industries.\\nAn AI agent for customer service, for instance, could operate beyond simple question-answering. With agentic AI, it could check a user‚Äôs outstanding balance and recommend which accounts could pay it off ‚Äî all while waiting for the user to make a decision so it could complete the transaction accordingly when prompted.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI systems ingest vast amounts of data from multiple data sources and third-party applications to independently analyze challenges, develop strategies and execute tasks. Businesses are implementing agentic AI to personalize customer service, streamline software development and even facilitate patient interactions.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\\nHow Does Agentic AI Work?\\nAgentic AI uses a four-step process for problem-solving:'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Perceive: AI agents gather and process data from various sources, such as sensors, databases and digital interfaces. This involves extracting meaningful features, recognizing objects or identifying relevant entities in the environment.\\nReason: A large language model acts as the orchestrator, or reasoning engine, that understands tasks, generates solutions and coordinates specialized models for specific functions like content creation, visual processing or recommendation systems. This step uses techniques like retrieval-augmented generation (RAG) to access proprietary data sources and deliver accurate, relevant outputs.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Act: By integrating with external tools and software via application programming interfaces, agentic AI can quickly execute tasks based on the plans it has formulated. Guardrails can be built into AI agents to help ensure they execute tasks correctly. For example, a customer service AI agent may be able to process claims up to a certain amount, while claims above the amount would have to be approved by a human.\\nLearn: Agentic AI continuously improves through a feedback loop, or\\n‚Äúdata flywheel,‚Äù where the data generated from its interactions is fed into the system to enhance models. This ability to adapt and become more effective over time offers businesses a powerful tool for driving better decision-making and operational efficiency.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Fueling Agentic AI With Enterprise Data\\nAcross industries and job functions, generative AI is transforming organizations by turning vast amounts of data into actionable knowledge, helping employees work more efficiently.\\nAI agents build on this potential by accessing diverse data through accelerated AI query engines, which process, store and retrieve information to enhance generative AI models. A key technique for achieving this is RAG, which allows AI to intelligently retrieve the right information from a broader range of data sources.\\nOver time, AI agents learn and improve by creating a data flywheel, where data generated through interactions is fed back into the system, refining models and increasing their effectiveness.\\nThe end-to-end NVIDIA AI platform, including NVIDIA NeMo microservices for developing custom generative AI applications, provides the ability to manage and access data efficiently, which is crucial for building responsive agentic AI applications.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI in Action\\nThe potential applications of agentic AI are vast, limited only by creativity and expertise. From simple tasks like generating and distributing content to more complex use cases such as orchestrating enterprise software, AI agents are transforming industries.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Customer Service: AI agents are improving customer support by enhancing self-service capabilities and automating routine communications. Over half of service professionals report significant improvements in customer interactions, reducing response times and boosting satisfaction.\\nThere‚Äôs also growing interest in digital humans ‚Äî AI-powered agents that embody a company‚Äôs brand and offer lifelike, real-time interactions to help sales representatives answer customer queries or solve issues directly when call volumes are high.\\nContent Creation: Agentic AI can help quickly create high-quality, personalized marketing content. Generative AI agents can save marketers an average of three hours per content piece, allowing them to focus on strategy and innovation. By streamlining content creation, businesses can stay competitive while improving customer engagement.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Software Engineering: AI agents are boosting developer productivity by automating repetitive coding tasks. It‚Äôs projected that by 2030 AI could automate up to 30% of work hours, freeing developers to focus on more complex challenges and drive innovation.\\nHealthcare: For doctors analyzing vast amounts of medical and patient data, AI agents can distill critical information to help them make better-informed care decisions. Automating administrative tasks and capturing clinical notes in patient appointments reduces the burden of time-consuming tasks, allowing doctors to focus on developing a doctor-patient connection.\\nAI agents can also provide 24/7 support, offering information on prescribed medication usage, appointment scheduling and reminders, and more to help patients adhere to treatment plans.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='AI agents can also provide 24/7 support, offering information on prescribed medication usage, appointment scheduling and reminders, and more to help patients adhere to treatment plans.\\nVideo analytics: Enterprises and public sector organizations around the world are developing video analytics AI agents to boost the capabilities of workforces that rely on visual information from a growing number of devices ‚Äî including cameras, IoT sensors and vehicles. Video analytics AI agents can analyze large amounts of live or archived videos, request tasks via natural language and perform complex operations like video search, summarization and visual question-answering. These agents can also be used to deliver anomaly alerts, draft incident reports, improve quality control through visual inspection and enhance predictive maintenance.\\nHow to Get Started'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='How to Get Started\\nWith its ability to plan and interact with a wide variety of tools and software, agentic AI marks the next chapter of artificial intelligence, offering the potential to enhance productivity and revolutionize the way organizations operate.\\nTo accelerate the adoption of generative AI-powered applications and agents, NVIDIA Blueprints provide sample applications, reference code, sample data, tools and comprehensive documentation.\\nNVIDIA partners including Accenture are helping enterprises use agentic AI with solutions built with NVIDIA Blueprints.\\nVisit ai.nvidia.com to learn more about the tools and software NVIDIA offers to help enterprises build their own AI agents.'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Categories: Explainer | Generative AITags: Agentic AI | Artificial Intelligence | NVIDIA Blueprints \\n\\n\\n \\n\\n\\n\\n\\nAll NVIDIA News \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tCheers to AI: ADAM Robot Bartender Makes Drinks at Vegas Golden Knights Game\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tAs AI Grows More Complex, Model Builders Rely on NVIDIA\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tRide Into Adventure With Capcom‚Äôs ‚ÄòMonster Hunter Stories‚Äô Series in the Cloud\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tOpt-In NVIDIA Software Enables Data Center Fleet Management\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tHow NVIDIA H100 GPUs on CoreWeave‚Äôs AI Cloud Platform Delivered a Record-Breaking Graph500 Run\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCorporate Information\\nAbout NVIDIA\\nCorporate Overview\\nTechnologies\\nNVIDIA Research\\nInvestors\\nSocial Responsibility\\nNVIDIA Foundation'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Corporate Information\\nAbout NVIDIA\\nCorporate Overview\\nTechnologies\\nNVIDIA Research\\nInvestors\\nSocial Responsibility\\nNVIDIA Foundation\\n \\n\\nGet Involved\\nForums\\nCareers\\nDeveloper Home\\nJoin the Developer Program\\nNVIDIA Partner Network\\nNVIDIA Inception\\nResources for Venture Capitalists\\nVenture Capital (NVentures)\\nTechnical Training\\nTraining for IT Professionals\\nProfessional Services for Data Science\\n \\n\\nNews & Events\\nNewsroom\\nNVIDIA Blog\\nNVIDIA Technical Blog\\nWebinars\\nStay Informed\\nEvents Calendar\\nNVIDIA GTC\\nNVIDIA On-Demand\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tExplore our regional blogs and other social networks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrivacy Policy\\nYour Privacy Choices\\nTerms of Service\\nAccessibility\\nCorporate Policies\\nProduct Security\\nContact\\n \\n\\t\\t\\tCopyright ¬© 2025 NVIDIA Corporation\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\tUSA - United States\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\xa0Share This\\xa0Facebook\\xa0LinkedIn \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare on Mastodon\\n\\n\\nEnter your Mastodon instance URL (optional)\\nShare'),\n",
       " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content=\"Share This\\xa0Facebook\\xa0LinkedIn \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare on Mastodon\\n\\n\\nEnter your Mastodon instance URL (optional)\\nShare\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare this Article\\n\\nFriend's Email Address\\n\\n\\n\\nYour Name\\n\\n\\n\\nYour Email Address\\n\\n\\n\\nComments\\n\\n\\n\\n Send Email\\n\\n\\nEmail sent!\")]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(document)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4feab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a9c914e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64bd5886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras --quiet\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a9d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in /Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-macosx_14_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m739.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfdab41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd1daf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x319c93830>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = FAISS.from_documents(docs,embeddings)\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f762a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='15a27a38-a322-4d92-b83c-8e17476923a1', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='What Is Agentic AI?\\xa0 | NVIDIA Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tArtificial Intelligence Computing Leadership from NVIDIA\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nToggle Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nAI\\nData Center\\nDriving\\nGaming\\nPro Graphics\\nRobotics\\nHealthcare\\nStartups\\nAI Podcast\\nNVIDIA Life\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Agentic AI?\\xa0\\n\\n\\n\\t\\t\\t\\tAgentic AI uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\\t\\t\\t\\n\\n\\nOctober 22, 2024 by Erik Pounds \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tShare\\t\\t\\t\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n \\nEmail0'),\n",
       " Document(id='f302d752-1ea1-4e81-a8d9-365503a06258', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI in Action\\nThe potential applications of agentic AI are vast, limited only by creativity and expertise. From simple tasks like generating and distributing content to more complex use cases such as orchestrating enterprise software, AI agents are transforming industries.'),\n",
       " Document(id='ea83338d-63b8-4961-a2f9-5dbba331d62d', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\\nHow Does Agentic AI Work?\\nAgentic AI uses a four-step process for problem-solving:'),\n",
       " Document(id='0bc9bf52-3f68-4026-8260-fec8b576d169', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI systems ingest vast amounts of data from multiple data sources and third-party applications to independently analyze challenges, develop strategies and execute tasks. Businesses are implementing agentic AI to personalize customer service, streamline software development and even facilitate patient interactions.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Agentic AI?\"\n",
    "result = vectors.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d72b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    Answer the following question based on the context provided.\n",
    "    context: <context>\n",
    "    {context}\n",
    "    </context>\"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b19ad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' \\n    Answer the following question based on the context provided.\\n    context: <context>\\n    {context}\\n    </context>'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x12b2be0f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x12b2bd9a0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "872e00dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c8261320-7e99-4065-9e63-74ba20531575', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Editor‚Äôs note: The name of NIM Agent Blueprints was changed to NVIDIA\\xa0Blueprints in October 2024. All references to the name have been updated in this blog.\\nAI chatbots use generative AI to provide responses based on a single interaction. A person makes a query and the chatbot uses natural language processing to reply.\\nThe next frontier of artificial intelligence is agentic AI, which uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems. And it‚Äôs set to enhance productivity and operations across industries.\\nAn AI agent for customer service, for instance, could operate beyond simple question-answering. With agentic AI, it could check a user‚Äôs outstanding balance and recommend which accounts could pay it off ‚Äî all while waiting for the user to make a decision so it could complete the transaction accordingly when prompted.'),\n",
       " Document(id='b786097e-f733-4eb1-bfae-8eed8d4f33c6', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Perceive: AI agents gather and process data from various sources, such as sensors, databases and digital interfaces. This involves extracting meaningful features, recognizing objects or identifying relevant entities in the environment.\\nReason: A large language model acts as the orchestrator, or reasoning engine, that understands tasks, generates solutions and coordinates specialized models for specific functions like content creation, visual processing or recommendation systems. This step uses techniques like retrieval-augmented generation (RAG) to access proprietary data sources and deliver accurate, relevant outputs.'),\n",
       " Document(id='b56405e7-c979-4ec5-aeed-085cca1438fc', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Fueling Agentic AI With Enterprise Data\\nAcross industries and job functions, generative AI is transforming organizations by turning vast amounts of data into actionable knowledge, helping employees work more efficiently.\\nAI agents build on this potential by accessing diverse data through accelerated AI query engines, which process, store and retrieve information to enhance generative AI models. A key technique for achieving this is RAG, which allows AI to intelligently retrieve the right information from a broader range of data sources.\\nOver time, AI agents learn and improve by creating a data flywheel, where data generated through interactions is fed back into the system, refining models and increasing their effectiveness.\\nThe end-to-end NVIDIA AI platform, including NVIDIA NeMo microservices for developing custom generative AI applications, provides the ability to manage and access data efficiently, which is crucial for building responsive agentic AI applications.'),\n",
       " Document(id='7a2e5224-a722-47ef-9d89-86ddbef23b9d', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Customer Service: AI agents are improving customer support by enhancing self-service capabilities and automating routine communications. Over half of service professionals report significant improvements in customer interactions, reducing response times and boosting satisfaction.\\nThere‚Äôs also growing interest in digital humans ‚Äî AI-powered agents that embody a company‚Äôs brand and offer lifelike, real-time interactions to help sales representatives answer customer queries or solve issues directly when call volumes are high.\\nContent Creation: Agentic AI can help quickly create high-quality, personalized marketing content. Generative AI agents can save marketers an average of three hours per content piece, allowing them to focus on strategy and innovation. By streamlining content creation, businesses can stay competitive while improving customer engagement.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity_search(\"Note that ChatModels receive message objects as input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0ea15ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x319c93830>, search_kwargs={})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectors.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7831852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3557bcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x319c93830>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' \\n    Answer the following question based on the context provided.\\n    context: <context>\\n    {context}\\n    </context>'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x12b2be0f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x12b2bd9a0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever,documents_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fa3d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain.invoke({'input': 'What is Agentic AI?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8802127c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Agentic AI?',\n",
       " 'context': [Document(id='15a27a38-a322-4d92-b83c-8e17476923a1', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='What Is Agentic AI?\\xa0 | NVIDIA Blog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tArtificial Intelligence Computing Leadership from NVIDIA\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nToggle Search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nAI\\nData Center\\nDriving\\nGaming\\nPro Graphics\\nRobotics\\nHealthcare\\nStartups\\nAI Podcast\\nNVIDIA Life\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Agentic AI?\\xa0\\n\\n\\n\\t\\t\\t\\tAgentic AI uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\\t\\t\\t\\n\\n\\nOctober 22, 2024 by Erik Pounds \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tShare\\t\\t\\t\\n\\n\\n \\n\\n\\n\\n\\n \\n \\n \\nEmail0'),\n",
       "  Document(id='f302d752-1ea1-4e81-a8d9-365503a06258', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI in Action\\nThe potential applications of agentic AI are vast, limited only by creativity and expertise. From simple tasks like generating and distributing content to more complex use cases such as orchestrating enterprise software, AI agents are transforming industries.'),\n",
       "  Document(id='ea83338d-63b8-4961-a2f9-5dbba331d62d', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\\nHow Does Agentic AI Work?\\nAgentic AI uses a four-step process for problem-solving:'),\n",
       "  Document(id='0bc9bf52-3f68-4026-8260-fec8b576d169', metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/', 'title': 'What Is Agentic AI?\\xa0 | NVIDIA Blog', 'description': 'Agentic AI connects to enterprise data and uses sophisticated reasoning and iterative planning to\\xa0 autonomously solve complex, multi-step problems.', 'language': 'en-US'}, page_content='Agentic AI systems ingest vast amounts of data from multiple data sources and third-party applications to independently analyze challenges, develop strategies and execute tasks. Businesses are implementing agentic AI to personalize customer service, streamline software development and even facilitate patient interactions.')],\n",
       " 'answer': '**Agentic AI** (as described in NVIDIA‚Äôs blog post) is a form of artificial intelligence that can **autonomously tackle complex, multi‚Äëstep problems** by employing sophisticated reasoning and iterative planning.  \\n\\nKey characteristics:\\n\\n| Feature | Description |\\n|---------|-------------|\\n| **Autonomous problem‚Äësolving** | The system independently ingests large volumes of data from many sources, analyzes the challenge, devises a strategy, and carries out the necessary tasks without continual human direction. |\\n| **Iterative planning** | It repeatedly refines its plan as it progresses, allowing it to handle dynamic or uncertain environments. |\\n| **Multi‚Äëstep reasoning** | Unlike single‚Äëprompt models that produce a one‚Äëoff answer, agentic AI can break a problem into a sequence of logical steps, evaluate intermediate results, and adjust its approach. |\\n| **Broad applicability** | It can be used for simple automation (e.g., content generation and distribution) as well as more complex orchestration such as enterprise software management, personalized customer service, software development pipelines, and patient‚Äëinteraction workflows. |\\n\\nIn short, **Agentic AI is an AI paradigm that combines large‚Äëscale data ingestion, advanced reasoning, and self‚Äëdirected, iterative planning to solve intricate tasks end‚Äëto‚Äëend without constant human supervision.**'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ec9e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f63e65c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 1, 'page_label': '2'}, page_content='Fundamentals of Data \\nEngineering \\nPlan and Build Robust Data Systems \\nJoe Reis and Matt Housley \\n Beijing  Boston  Farnham  Sebastopol  Tokyo'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 2, 'page_label': '3'}, page_content='Table of Contents \\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . . . . .  xiii \\n \\nPart I. Foundation and Building Blocks \\n1. Data Engineering Described. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . . . .  3 \\nWhat Is Data Engineering?                                                                                             \\n3 Data Engineering Defined                                                                                          \\n4 The Data Engineering Lifecycle                                                                                 \\n5 Evolution of the Data Engineer                                                                                  \\n6 \\nData Engineering and Data Science                                                                        11 \\nData Engineering Skills and Activities                                                                        13 \\nData Maturity and the Data Engineer                                                                     13 \\nThe Background and Skills of a Data Engineer                                                     17 \\nBusiness Responsibilities                                                                                           18 \\nTechnical Responsibilities                                                                                         19 \\nThe Continuum of Data Engineering Roles, from A to B                                    21 \\nData Engineers Inside an Organization                                                                      \\n22 Internal-Facing V ersus External-Facing Data Engineers                                     \\n23 Data Engineers and Other Technical Roles                                                            \\n24 \\nData Engineers and Business Leadership                                                               28 \\nConclusion                                                                                                                      31 \\nAdditional Resources                                                                                                    32'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 3, 'page_label': '4'}, page_content='2. The Data Engineering Lifecycle. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . .  33 \\nWhat Is the Data Engineering Lifecycle?                                                                    \\n33 The Data Lifecycle Versus the Data Engineering Lifecycle                                  \\n34 Generation: Source Systems                                                                                     \\n35 Storage                                                                                                                         \\n38 Ingestion                                                                                                                      \\n39 Transformation                                                                                                           \\n43 \\nServing Data                                                                                                                44 \\nMajor Undercurrents Across the Data Engineering Lifecycle                                 48 \\nSecurity                                                                                                                        49 \\nData Management                                                                                                      50 \\nDataOps                                                                                                                       59 \\nData Architecture                                                                                                       64 \\nOrchestration                                                                                                              64 \\nSoftware Engineering                                                                                                66 \\nConclusion                                                                                                                      68 \\nAdditional Resources                                                                                                    69 \\n 3. Designing Good Data Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . . .  71 \\nWhat Is Data Architecture?                                                                                          71 \\nEnterprise Architecture Defined                                                                              72 \\nData Architecture Defined                                                                                        75 \\n‚ÄúGood‚Äù Data Architecture                                                                                         76 \\nPrinciples of Good Data Architecture                                                                        77 \\nPrinciple 1: Choose Common Components Wisely                                             78 \\nPrinciple 2: Plan for Failure                                                                                      79 \\nPrinciple 3: Architect for Scalability                                                                        80 \\nPrinciple 4: Architecture Is Leadership                                                                   80 \\nPrinciple 5: Always Be Architecting                                                                        81 \\nPrinciple 6: Build Loosely Coupled Systems                                                          81 \\nPrinciple 7: Make Reversible Decisions                                                                  83 \\nPrinciple 8: Prioritize Security                                                                                 84 \\nPrinciple 9: Embrace FinOps                                                                                    85 \\nMajor Architecture Concepts                                                                                       87 \\nDomains and Services                                                                                               87'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 4, 'page_label': '5'}, page_content='Distributed Systems, Scalability, and Designing for Failure                                88 \\nTight Versus Loose Coupling: Tiers, Monoliths, and Microservices                  90 \\nUser Access: Single Versus Multitenant                                                                  94 \\nEvent-Driven Architecture                                                                                       95 \\nBrownfield Versus Greenfield Projects                                                                   96 \\nExamples and Types of Data Architecture                                                                 98 \\nData Warehouse                                                                                                         98 \\nData Lake                                                                                                                   101 \\nConvergence, Next-Generation Data Lakes, and the Data Platform                102 \\nModern Data Stack                                                                                                  103 \\nLambda Architecture                                                                                               104 \\nKappa Architecture                                                                                                  105 \\nThe Dataflow Model and Unified Batch and Streaming                                    105 \\nArchitecture for IoT                                                                                                 106 \\nData Mesh                                                                                                                 109 \\nOther Data Architecture Examples                                                                       110 \\nWho‚Äôs Involved with Designing a Data Architecture?                                            111 \\nConclusion                                                                                                                    111 \\nAdditional Resources                                                                                                  111 \\n 4. Choosing Technologies Across the Data Engineering Lifecycle. . . . . . . . \\n. . . . . . . . . . .  115 \\nTeam Size and Capabilities                                                                                         116 \\nSpeed to Market                                                                                                           117 \\nInteroperability                                                                                                            117 \\nCost Optimization and Business Value                                                                    118 \\nTotal Cost of Ownership                                                                                         118 \\nTotal Opportunity Cost of Ownership                                                                  119 \\nFinOps                                                                                                                       120 \\nToday Versus the Future: Immutable Versus Transitory Technologies               120 \\nOur Advice                                                                                                                122 \\nLocation                                                                                                                         123 \\nOn Premises                                                                                                              123 \\nCloud                                                                                                                         124 \\nHybrid Cloud                                                                                                            127 \\nMulticloud                                                                                                                 128 \\nDecentralized: Blockchain and the Edge                                                              129 \\nOur Advice                                                                                                                129 \\nCloud Repatriation Arguments                                                                              130'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 5, 'page_label': '6'}, page_content='Build Versus Buy                                                                                                          132 \\nOpen Source Software                                                                                             133 \\nProprietary Walled Gardens                                                                                   137 \\nOur Advice                                                                                                                138 \\nMonolith Versus Modular                                                                                          139 \\nMonolith                                                                                                                    139 \\nModularity                                                                                                                140 \\nThe Distributed Monolith Pattern                                                                         142 \\nOur Advice                                                                                                                142 \\nServerless Versus Servers                                                                                            143 \\nServerless                                                                                                                   143 \\nContainers                                                                                                                 144 \\nHow to Evaluate Server Versus Serverless                                                            145 \\nOur Advice                                                                                                                146 \\nOptimization, Performance, and the Benchmark Wars                                         \\n147 Big Data...for the 1990s                                                                                           \\n148 Nonsensical Cost Comparisons                                                                             \\n148 Asymmetric Optimization                                                                                      \\n148 \\nCaveat Emptor                                                                                                          149 \\nUndercurrents and Their Impacts on Choosing Technologies                             149 \\nData Management                                                                                                    149 \\nDataOps                                                                                                                     149 \\nData Architecture                                                                                                     150 \\nOrchestration Example: Airflow                                                                            150 \\nSoftware Engineering                                                                                              151 \\nConclusion                                                                                                                    151 \\nAdditional Resources                                                                                                  151 \\n \\nPart II. The Data Engineering Lifecycle in Depth \\n5. Data Generation in Source Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . .  155 Sources of Data: How Is Data Created?                                                                    \\n156 \\nSource Systems: Main Ideas                                                                                       156 \\nFiles and Unstructured Data                                                                                  156 \\nAPIs                                                                                                                            157 \\nApplication Databases (OLTP Systems)                                                               157 \\nOnline Analytical Processing System                                                                    159'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 6, 'page_label': '7'}, page_content='Change Data Capture                                                                                              159 \\nLogs                                                                                                                            160 \\nDatabase Logs                                                                                                           161 \\nCRUD                                                                                                                        162 \\nInsert-Only                                                                                                                162 \\nMessages and Streams                                                                                             163 \\nTypes of Time                                                                                                           164 \\nSource System Practical Details                                                                                 165 \\nDatabases                                                                                                                   166 \\nAPIs                                                                                                                            174 \\nData Sharing                                                                                                             176 \\nThird-Party Data Sources                                                                                       177 \\nMessage Queues and Event-Streaming Platforms                                               177 \\nWhom You‚Äôll Work With                                                                                            \\n181 \\nUndercurrents and Their Impact on Source Systems                                            183 \\nSecurity                                                                                                                      183 \\nData Management                                                                                                    184 \\nDataOps                                                                                                                     184 \\nData Architecture                                                                                                     185 \\nOrchestration                                                                                                            186 \\nSoftware Engineering                                                                                              187 \\nConclusion                                                                                                                    187 \\nAdditional Resources                                                                                                  188 \\n 6. Storage. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . . .  189 \\nRaw Ingredients of Data Storage                                                                               191 \\nMagnetic Disk Drive                                                                                                191 \\nSolid-State Drive                                                                                                      193 \\nRandom Access Memory                                                                                        194 \\nNetworking and CPU                                                                                              195 \\nSerialization                                                                                                               195 \\nCompression                                                                                                             196 \\nCaching                                                                                                                      197 \\nData Storage Systems                                                                                                  197 \\nSingle Machine Versus Distributed Storage                                                         198 \\nEventual Versus Strong Consistency                                                                     198 \\nFile Storage                                                                                                                199 \\nBlock Storage                                                                                                            202'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 7, 'page_label': '8'}, page_content='Object Storage                                                                                                          205 \\nCache and Memory-Based Storage Systems                                                        211 \\nThe Hadoop Distributed File System                                                                    211 \\nStreaming Storage                                                                                                    212 \\nIndexes, Partitioning, and Clustering                                                                   213 \\nData Engineering Storage Abstractions                                                                    215 \\nThe Data Warehouse                                                                                               215 \\nThe Data Lake                                                                                                           216 \\nThe Data Lakehouse                                                                                                216 \\nData Platforms                                                                                                          217 \\nStream-to-Batch Storage Architecture                                                                  217 \\nBig Ideas and Trends in Storage                                                                                218 \\nData Catalog                                                                                                              218 \\nData Sharing                                                                                                             219 \\nSchema                                                                                                                       219 \\nSeparation of Compute from Storage                                                                   220 \\nData Storage Lifecycle and Data Retention                                                          223 \\nSingle-Tenant Versus Multitenant Storage                                                           226 \\nWhom You‚Äôll Work With                                                                                            227 \\nUndercurrents                                                                                                              228 \\nSecurity                                                                                                                      228 \\nData Management                                                                                                    228 \\nDataOps                                                                                                                     229 \\nData Architecture                                                                                                     230 \\nOrchestration                                                                                                            230 \\nSoftware Engineering                                                                                              230 \\nConclusion                                                                                                                    230 \\nAdditional Resources                                                                                                  231 \\n 7. Ingestion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . .  233 \\nWhat Is Data Ingestion?                                                                                              234 \\nKey Engineering Considerations for the Ingestion Phase                                     235 \\nBounded Versus Unbounded Data                                                                        236 \\nFrequency                                                                                                                  237 \\nSynchronous V ersus Asynchronous Ingestion                                                    238 \\nSerialization and Deserialization                                                                           239 \\nThroughput and Scalability                                                                                    239 \\nReliability and Durability                                                                                        240 \\nPayload                                                                                                                       241'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 8, 'page_label': '9'}, page_content='Push Versus Pull Versus Poll Patterns                                                                   244 \\nBatch Ingestion Considerations                                                                                 244 \\nSnapshot or Differential Extraction                                                                      246 \\nFile-Based Export and Ingestion                                                                            246 \\nETL Versus ELT                                                                                                        246 \\nInserts, Updates, and Batch Size                                                                            247 \\nData Migration                                                                                                         247 \\nMessage and Stream Ingestion Considerations                                                       248 \\nSchema Evolution                                                                                                    248 \\nLate-Arriving Data                                                                                                   248 \\nOrdering and Multiple Delivery                                                                            248 \\nReplay                                                                                                                         249 \\nTime to Live                                                                                                              249 \\nMessage Size                                                                                                              249 \\nError Handling and Dead-Letter Queues                                                             249 \\nConsumer Pull and Push                                                                                        250 \\nLocation                                                                                                                     250 \\nWays to Ingest Data                                                                                                     250 \\nDirect Database Connection                                                                                  251 \\nChange Data Capture                                                                                              252 \\nAPIs                                                                                                                            254 \\nMessage Queues and Event-Streaming Platforms                                               255 \\nManaged Data Connectors                                                                                     256 \\nMoving Data with Object Storage                                                                         257 \\nEDI                                                                                                                             257 \\nDatabases and File Export                                                                                      257 \\nPractical Issues with Common File Formats                                                        258 \\nShell                                                                                                                            258 \\nSSH                                                                                                                             259 \\nSFTP and SCP                                                                                                           259 \\nWebhooks                                                                                                                  259 \\nWeb Interface                                                                                                            260 \\nWeb Scraping                                                                                                            260 \\nTransfer Appliances for Data Migration                                                               261 \\nData Sharing                                                                                                             262 \\nWhom You‚Äôll Work With                                                                                            262 \\nUpstream Stakeholders                                                                                           262 \\nDownstream Stakeholders                                                                                      263 \\nUndercurrents                                                                                                              263 \\nSecurity                                                                                                                      264'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 9, 'page_label': '10'}, page_content='Data Management                                                                                                    264 \\nDataOps                                                                                                                     266 \\nOrchestration                                                                                                            268 \\nSoftware Engineering                                                                                              268 \\nConclusion                                                                                                                    268 \\nAdditional Resources                                                                                                  269 \\n 8. Queries, Modeling, and Transformation. . . . . . . . . . . . . . . . . . . . . . . . . \\n. . . . . . . . . . . .  271 \\nQueries                                                                                                                          272 \\nWhat Is a Query?                                                                                                      273 \\nThe Life of a Query                                                                                                  274 \\nThe Query Optimizer                                                                                              275 \\nImproving Query Performance                                                                              275 \\nQueries on Streaming Data                                                                                    281 \\nData Modeling                                                                                                              287 \\nWhat Is a Data Model?                                                                                            288 \\nConceptual, Logical, and Physical Data Models                                                  289 \\nNormalization                \\n \\nPART I Foundation and Buil ding \\nBlock'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 10, 'page_label': '11'}, page_content='3 \\nCHAPTE\\nR 1 \\nData \\nEngineering \\nDescribed \\nIf you work in data or software, you may have noticed data engineering emerging from \\nthe shadows and now sharing the stage with data science. Data engineering is one of \\nthe hottest fields in data and technology, and for a good reason. It builds the foundation \\nfor data science and analytics in production. This chapter explores what data \\nengineering is, how the field was born and its evolution, the skills of data engineers, \\nand with whom they work. \\nWhat Is Data Engineering? \\nDespite the current popularity of data engineering, there‚Äôs a lot of confusion about what \\ndata engineering means and what data engineers do. Data engineering has existed in \\nsome form since companies started doing things with data ‚Äîsuch as predictive \\nanalysis, descriptive analytics, and reports‚Äîand came into sharp focus alongside the \\nrise of data science in the 2010s. For the purpose of this book, it‚Äôs critical to define \\nwhat data engineering and data engineer mean.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 11, 'page_label': '12'}, page_content='| Chapter 1: Data Engineering Described \\nFirst, let‚Äôs look at the landscape of how data engineering is described and develop some \\nterminology we can use throughout this book. Endless definitions of data engineering \\nexist. In early 2022, a Google exact -match search for ‚Äúwhat is data engineering?‚Äù \\nreturns over 91,000 unique results. Before we give our definition, here are a few \\nexamples of how some experts in the field define data engineering: \\nData engineering is a set of operations aimed at creating interfaces and mechanisms \\nfor the flow and access of information. It takes dedicated specialists‚Äîdata engineers‚Äî \\nto maintain data so that it remains available and usable by others. In short, data \\nengineers set up and operate the organization‚Äôs data infrastructure, preparing it for \\nfurther analysis by data analysts and scientists. \\n‚ÄîFrom ‚ÄúData Engineering and Its Main Concepts‚Äù by AlexSoft1 \\nThe first type of data engineering is SQL -focused. The work and primary storage of \\nthe data is in relational databases. All of the data processing is done with SQL or a \\nSQL-based language. Sometimes, this data processing is done with an ETL tool.2 The \\nsecond type of data engineering is Big Data ‚Äìfocused. The work and primary storage \\nof the data is in Big Data technologies like Hadoop, Cassandra, and HBase. All of the \\ndata processing is done in Big Data frameworks like MapReduce, Spark, and Flink. \\nWhile SQL is used, the primary processing is done with programming languages like \\nJava, Scala, and Python. \\n‚ÄîJesse Anderson3 \\nIn relation to previously existing roles, the data engineering field could be thought of \\nas a superset of business intelligence and data warehousing that brings more elements \\nfrom software engineering. This discipline also integrates specialization around the \\noperation of so-called ‚Äúbig data‚Äù distributed systems, along with concepts around the \\nextended Hadoop ecosystem, stream processing, and in computation at scale. \\n‚ÄîMaxime Beauchemin4 \\nData engineering is all about the movement, manipulation, and management of data. \\n‚ÄîLewis Gavin5 \\nWow! It‚Äôs entirely understandable if you‚Äôve been confused about data engineering. \\nThat‚Äôs only a handful of definitions, and they contain an enormous range of opinions \\nabout the meaning of data engineering. \\nData Engineering Defined \\n \\n1 ‚ÄúData Engineering and Its Main Concepts,‚Äù AlexSoft, last updated August 26, 2021, https://oreil.ly/e94py. \\n2 ETL stands for extract, transform, load, a common pattern we cover in the book. \\n3 Jesse Anderson, ‚ÄúThe Two Types of Data Engineering,‚Äù June 27, 2018, https://oreil.ly/dxDt6. \\n4 Maxime Beauchemin, ‚ÄúThe Rise of the Data Engineer,‚Äù January 20, 2017, https://oreil.ly/kNDmd. \\n5 Lewis Gavin, What Is Data Engineering? (Sebastapol, CA: O‚ÄôReilly, 2020), https://oreil.ly/ELxLi.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 12, 'page_label': '13'}, page_content='What Is Data Engineering?  | \\n 5 \\nWhen we unpack the common threads of how various people define data engineering, \\nan obvious pattern emerges: a data engineer gets data, stores it, and prepares it for \\nconsumption by data scientists, analysts, and others. We define data engineering and \\ndata engineer as follows: \\nData engineering  is the development, implementation, and maintenance of systems \\nand processes that take in raw data and produce high -quality, consistent information \\nthat supports downstream use cases, such as analysis and machine learning. Data \\nengineering is the intersec tion of security, data management, DataOps, data \\narchitecture, orchestration, and software engineering. A data engineer  manages the \\ndata engineering lifecycle, beginning with getting data from source systems and \\nending with serving data for use cases, such as analysis or machine learning. \\n4  \\nThe Data Engineering Lifecycle \\nIt is all too easy to fixate on technology and miss the bigger picture myopically. This \\nbook centers around a big idea called the data engineering lifecycle (Figure 1-1), which \\nwe believe gives data engineers the holistic context to view their role. \\n \\nFigure 1-1. The data engineering lifecycle'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 13, 'page_label': '14'}, page_content='| Chapter 1: Data Engineering Described \\nThe data engineering lifecycle shifts the conversation away from technology and \\ntoward the data itself and the end goals that it must serve. The stages of the data \\nengineering lifecycle are as follows: \\n‚Ä¢ Generation \\n‚Ä¢ Storage \\n‚Ä¢ Ingestion \\n‚Ä¢ Transformation \\n‚Ä¢ Serving \\nThe data engineering lifecycle also has a notion of undercurrents‚Äîcritical ideas across \\nthe entire lifecycle. These include security, data management, DataOps, data \\narchitecture, orchestration, and software engineering. We cover the data engineering \\nlifecycle and its undercurrents more extensively in Chapter 2. Still, we introduce it \\nhere because it is essential to our definition of data engineering and the discussion that \\nfollows in this chapter. \\nNow that you have a working definition of data engineering and an introduction to its \\nlifecycle, let‚Äôs take a step back and look at a bit of history.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 14, 'page_label': '15'}, page_content='What Is Data Engineering?  | \\n 7 \\nEvolution of the Data Engineer \\nHistory doesn‚Äôt repeat itself, but it rhymes. \\n‚ÄîA famous adage often attributed to Mark Twain \\nUnderstanding data engineering today and tomorrow requires a context of how the \\nfield evolved. This section is not a history lesson, but looking at the past is invaluable \\nin understanding where we are today and where things are going. A common theme \\nconstantly reappears: what‚Äôs old is new again. \\nThe early days: 1980 to 2000, from data warehousing to the web \\nThe birth of the data engineer arguably has its roots in data warehousing, dating as far \\nback as the 1970s, with the business data warehouse taking shape in the 1980s and Bill \\nInmon officially coining the term data warehouse in 1989. After engineers at IBM \\ndeveloped the relational database and Structured Query Language (SQL), Oracle \\npopularized the technology. As nascent data systems grew, businesses needed \\ndedicated tools and data pipelines for reporting and business intelligence (BI). To help \\npeople correctly model their business logic in the data warehouse, Ralph Kimball and \\nInmon developed their respective eponymous data -modeling techniques and \\napproaches, which are still widely used today. \\nData warehousing ushered in the first age of scalable analytics, with new massively \\nparallel processing (MPP) databases that use multiple processors to crunch large \\namounts of data coming on the market and supporting unprecedented volumes of data. \\nRoles such as BI engineer, ETL developer, and data warehouse engineer addressed the \\nvarious needs of the data warehouse. Data warehouse and BI engineering were a \\nprecursor to today‚Äôs data engineering and still play a central role in the discipline. \\nThe internet went mainstream around the mid-1990s, creating a whole new generation \\nof web -first companies such as AOL, Yahoo, and Amazon. The dot -com boom \\nspawned a ton of activity in web applications and the backend systems to support \\nthem‚Äîservers, databa ses, and storage. Much of the infrastructure was expensive, \\nmonolithic, and heavily licensed. The vendors selling these backend systems likely \\ndidn‚Äôt foresee the sheer scale of the data that web applications would produce. \\nThe early 2000s: The birth of contemporary data engineering \\nFast-forward to the early 2000s, when the dot -com boom of the late ‚Äô90s went bust, \\nleaving behind a tiny cluster of survivors. Some of these companies, such as Yahoo, \\nGoogle, and Amazon, would grow into powerhouse tech companies. Initially, these \\ncompanies continued to rely on the traditional monolithic, relational databases and data \\nwarehouses of the 1990s, pushing these systems to the limit. As these systems buckled,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 15, 'page_label': '16'}, page_content='8  | Chapter 1: Data Engineering Described \\nupdated approaches were needed to handle data growth. The new generation of the \\nsystems must be cost-effective, scalable, available, and reliable. \\nCoinciding with the explosion of data, commodity hardware‚Äîsuch as servers, RAM, \\ndisks, and flash drives ‚Äîalso became cheap and ubiquitous. Several innovations \\nallowed distributed computation and storage on massive computing clusters at a vast \\nscale. These i nnovations started decentralizing and breaking apart traditionally \\nmonolithic services. The ‚Äúbig data‚Äù era had begun. \\nThe Oxford English Dictionary defines big data as ‚Äúextremely large data sets that may \\nbe analyzed computationally to reveal patterns, trends, and associations, especially \\nrelating to human behavior and interactions.‚Äù Another famous and succinct description \\nof big data is the three Vs of data: velocity, variety, and volume. \\nIn 2003, Google published a paper on the Google File System, and shortly after that, \\nin 2004, a paper on MapReduce, an ultra-scalable data-processing paradigm. In truth, \\nbig data has earlier antecedents in MPP data warehouses and data management for \\nexperimental physics projects, but Google‚Äôs publications constituted a ‚Äúbig bang‚Äù for \\ndata technologies and the cultural roots of data engineering as we know it today. You‚Äôll \\nlearn more about MPP systems and MapReduce in Chapters 3 and 8, respectively. \\nThe Google papers inspired engineers at Yahoo to develop and later open source \\nApache Hadoop in 2006. 6  It‚Äôs hard to overstate the impact of Hadoop. Software \\nengineers interested in large-scale data problems were drawn to the possibilities of this \\nnew open source technology ecosystem. As companies of all sizes and types saw their \\ndata grow into many terabytes and even petabytes, the era of the big data engineer was \\nborn. \\nAround the same time, Amazon had to keep up with its own exploding data needs and \\ncreated elastic computing environments (Amazon Elastic Compute Cloud, or EC2), \\ninfinitely scalable storage systems (Amazon Simple Storage Service, or S3), highly \\nscalable NoSQL databases (Amazon DynamoDB), and many other core data building \\nblocks.7 Amazon elected to offer these services for internal and external consumption \\nthrough Amazon Web Services (AWS), becoming the first popular public cloud. AWS \\ncreated an ultra -flexible pay -as-you-go resource marketplace by virtualizing and \\nreselling vast pools of commodity hardware. Instead of purchasing hardware for a data \\ncenter, developers could simply rent compute and storage from \\nAWS. \\n \\n6 Cade Metz, ‚ÄúHow Yahoo Spawned Hadoop, the Future of Big Data,‚Äù Wired, October 18, 2011, \\nhttps://oreil.ly/iaD9G. \\n7 Ron Miller, ‚ÄúHow AWS Came to Be,‚Äù TechCrunch, July 2, 2016, https://oreil.ly/VJehv.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 16, 'page_label': '17'}, page_content='What Is Data Engineering?  | \\n 9 \\nAs AWS became a highly profitable growth engine for Amazon, other public clouds \\nwould soon follow, such as Google Cloud, Microsoft Azure, and DigitalOcean. The \\npublic cloud is arguably one of the most significant innovations of the 21st century and \\nspawned a revolution in the way software and data applications are developed and \\ndeployed. \\nThe early big data tools and public cloud laid the foundation for today‚Äôs data \\necosystem. The modern data landscape‚Äîand data engineering as we know it now ‚Äî\\nwould not exist without these innovations. \\nThe 2000s and 2010s: Big data engineering \\nOpen source big data tools in the Hadoop ecosystem rapidly matured and spread from \\nSilicon Valley to tech-savvy companies worldwide. For the first time, any business had \\naccess to the same bleeding-edge data tools used by the top tech companies. Another \\nrevolution occurred with the transition from batch computing to event streaming, \\nushering in a new era of big ‚Äúreal -time‚Äù data. You‚Äôll learn about batch and event \\nstreaming throughout this book. \\nEngineers could choose the latest and greatest ‚ÄîHadoop, Apache Pig, Apache Hive, \\nDremel, Apache HBase, Apache Storm, Apache Cassandra, Apache Spark, Presto, and \\nnumerous other new technologies that came on the scene. Traditional \\nenterpriseoriented and GUI -based data tools suddenly felt outmoded, and code -first \\nengineering was in vogue with the ascendance of MapReduce. We (the authors) were \\naround during this time, and it felt like old dogmas died a sudden death upon the altar \\nof big data. \\nThe explosion of data tools in the late 2000s and 2010s ushered in the big data \\nengineer. To effectively use these tools and techniques ‚Äînamely, the Hadoop \\necosystem including Hadoop, Y ARN, Hadoop Distributed File System (HDFS), and \\nMapReduce‚Äîbig data engineers had to be proficient in software development and \\nlow-level infrastructure hacking, but with a shifted emphasis. Big data engineers \\ntypically maintained massive clusters of commodity hardware to deliver data at scale. \\nWhile they might occasionally submit pull requests to Hadoop core code, they shifted \\ntheir focus from core technology development to data delivery. \\nBig data quickly became a victim of its own success. As a buzzword, big data gained \\npopularity during the early 2000s through the mid -2010s. Big data captured the \\nimagination of companies trying to make sense of the ever -growing volumes of data \\nand the endless barrage of shameless marketing from companies selling big data tools \\nand services. Because of the immense hype, it was common to see companies using \\nbig data tools for small data problems, sometimes standing up a Hadoop cluster to \\nprocess just a few gigabytes. It seemed like everyone wanted in on the big data action.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 17, 'page_label': '18'}, page_content='10  | Chapter 1: Data Engineering Described \\nDan Ariely tweeted, ‚ÄúBig data is like teenage sex: everyone talks about it, nobody \\nreally knows how to do it, everyone thinks everyone else is doing it, so everyone claims \\nthey are doing it.‚Äù \\nFigure 1-2 shows a snapshot of Google Trends for the search term ‚Äúbig data‚Äù to get an \\nidea of the rise and fall of big data. \\n \\nFigure 1-2. Google Trends for ‚Äúbig data‚Äù (March 2022) \\nDespite the term‚Äôs popularity, big data has lost steam. What happened? One word: \\nsimplification. Despite the power and sophistication of open source big data tools, \\nmanaging them was a lot of work and required constant attention. Often, companies \\nemployed entire teams of big data engineers, costing millions of dollars a year, to \\nbabysit these platforms. Big data engineers often spent excessive time maintaining \\ncomplicated tooling and arguably not as much time delivering the business‚Äôs insights \\nand value. \\nOpen source developers, clouds, and third parties started looking for ways to abstract, \\nsimplify, and make big data available without the high administrative overhead and \\ncost of managing their clusters, and installing, configuring, and upgrading their ope n \\nsource code. The term big data is essentially a relic to describe a particular time and \\napproach to handling large amounts of data. \\nToday, data is moving faster than ever and growing ever larger, but big data processing \\nhas become so accessible that it no longer merits a separate term; every company aims \\nto solve its data problems, regardless of actual data size. Big data engineers are  now \\nsimply data engineers.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 18, 'page_label': '19'}, page_content='What Is Data Engineering?  | \\n 11 \\nThe 2020s: Engineering for the data lifecycle \\nAt the time of this writing, the data engineering role is evolving rapidly. We expect this \\nevolution to continue at a rapid clip for the foreseeable future. Whereas data engineers \\nhistorically tended to the low-level details of monolithic frameworks such as Hadoop, \\nSpark, or Informatica, the trend is moving toward decentralized, modularized, \\nmanaged, and highly abstracted tools. \\nIndeed, data tools have proliferated at an astonishing rate (see Figure 1-3). Popular \\ntrends in the early 2020s include the modern data stack, representing a collection of \\noff-the-shelf open source and third -party products assembled to make analysts‚Äô lives \\neasier. At the same time, data sources and data formats are growing both in variety and \\nsize. Data engineering is increasingly a discipline of interoperation, and connecting \\nvarious technologies like LEGO bricks, to serve ultimate business goals. \\n \\nFigure 1-3. Matt Turck‚Äô s Data Landscape in 2012 versus 2021 \\nThe data engineer we discuss in this book can be described more precisely as a data \\nlifecycle engineer. With greater abstraction and simplification, a data lifecycle engineer \\nis no longer encumbered by the gory details of yesterday‚Äôs big data frameworks. While \\ndata engineers maintain skills in low-level data programming and use these as required, \\nthey increasingly find their role focused on things higher in the value chain: security, \\ndata management, DataOps, data architecture, orchestration, and general data lifecycle \\nmanagement.8 \\nAs tools and workflows simplify, we‚Äôve seen a noticeable shift in the attitudes of data \\nengineers. Instead of focusing on who has the ‚Äúbiggest data,‚Äù open source projects and \\nservices are increasingly concerned with managing and governing data, making it \\neasier to use and discover, and improving its quality. Data engineers are now \\n \\n8 DataOps is an abbreviation for data operations. We cover this topic in Chapter 2. For more information, read \\nthe DataOps Manifesto.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 19, 'page_label': '20'}, page_content='12  | Chapter 1: Data Engineering Described \\nconversant in acronyms such as CCP A and GDPR;9 as they engineer pipelines, they \\nconcern themselves with privacy, anonymization, data garbage collection, and \\ncompliance with regulations. \\nWhat‚Äôs old is new again. While ‚Äúenterprisey‚Äù stuff like data management (including \\ndata quality and governance) was common for large enterprises in the pre-big-data era, \\nit wasn‚Äôt widely adopted in smaller companies. Now that many of the challenging \\nproblems of yesterday‚Äôs data systems are solved, neatly productized, and packaged, \\ntechnologists and entrepreneurs have shifted focus back to the ‚Äúenterprisey‚Äù stuff, but \\nwith an emphasis on decentralization and agility, which contrasts with the traditional \\nenterprise command-and-control approach. \\nWe view the present as a golden age of data lifecycle management. Data engineers \\nmanaging the data engineering lifecycle have better tools and techniques than ever \\nbefore. We discuss the data engineering lifecycle and its undercurrents in greater detail \\nin the next chapter. \\nData Engineering and Data Science \\nWhere does data engineering fit in with data science? There‚Äôs some debate, with some \\narguing data engineering is a subdiscipline of data science. We believe data \\nengineering is separate from data science and analytics. They complement each other, \\nbut they are distinctly different. Data engineering sits upstream from data science \\n(Figure 1 -4), meaning data engineers provide the inputs used by data scientists \\n(downstream from data engineering), who convert these inputs into something useful. \\n \\nFigure 1-4. Data engineering sits upstream from data science \\nConsider the Data Science Hierarchy of Needs ( Figure 1-5). In 2017, Monica Rogati \\npublished this hierarchy in an article that showed where AI and machine learning (ML) \\nsat in proximity to more ‚Äúmundane‚Äù areas such as data movement/storage, collection, \\nand infrastructure. \\n \\n9 These acronyms stand for California Consumer Privacy Act and General Data Protection Regulation, \\nrespectively.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 20, 'page_label': '21'}, page_content='What Is Data Engineering?  | \\n 13 \\n \\nFigure 1-5. The Data Science Hierarchy of Needs \\nAlthough many data scientists are eager to build and tune ML models, the reality is an \\nestimated 70% to 80% of their time is spent toiling in the bottom three parts of the \\nhierarchy‚Äîgathering data, cleaning data, processing data ‚Äîand only a tiny slice of \\ntheir time on analysis and ML. Rogati argues that companies need to build a solid data \\nfoundation (the bottom three levels of the hierarchy) before tackling areas such as AI \\nand ML. \\nData scientists aren‚Äôt typically trained to engineer production-grade data systems, and \\nthey end up doing this work haphazardly because they lack the support and resources \\nof a data engineer. In an ideal world, data scientists should spend more than 90% of  \\ntheir time focused on the top layers of the pyramid: analytics, experimentation, and \\nML. When data engineers focus on these bottom parts of the hierarchy, they build a \\nsolid foundation for data scientists to succeed. \\nWith data science driving advanced analytics and ML, data engineering straddles the \\ndivide between getting data and getting value from data (see Figure 1-6). We believe \\ndata engineering is of equal importance and visibility to data science, with data \\nengineers playing a vital role in making data science successful in production. \\n \\nFigure 1-6. A data engineer gets data and provides value from the data'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 21, 'page_label': '22'}, page_content='Data Engineering Skills and Activities  | \\n 14 \\nData Engineering Skills and Activities \\nThe skill set of a data engineer encompasses the ‚Äúundercurrents‚Äù of data engineering: \\nsecurity, data management, DataOps, data architecture, and software engineering. This \\nskill set requires an understanding of how to evaluate data tools and how they fit \\ntogether across the data engineering lifecycle. It‚Äôs also critical to know how data is \\nproduced in source systems and how analysts and data scientists will consume and \\ncreate value after processing and curating data. Finally, a data engineer juggles a lot of \\ncomplex moving parts and must constantly optimize along the axes of cost, agility, \\nscalability, simplicity, reuse, and interoperability (Figure 1-7). We cover these topics \\nin more detail in upcoming chapters. \\n \\nFigure 1-7. The balancing act of data engineering \\nAs we discussed, in the recent past, a data engineer was expected to know and \\nunderstand how to use a small handful of powerful and monolithic technologies \\n(Hadoop, Spark, Teradata, Hive, and many others) to create a data solution. Utilizing \\nthese technolo gies often requires a sophisticated understanding of software \\nengineering, networking, distributed computing, storage, or other low -level details. \\nTheir work would be devoted to cluster administration and maintenance, managing \\noverhead, and writing pipeline and transformation jobs, among other tasks. \\nNowadays, the data-tooling landscape is dramatically less complicated to manage and \\ndeploy. Modern data tools considerably abstract and simplify workflows. As a result, \\ndata engineers are now focused on balancing the simplest and most cost-effective, best-\\nof-breed services that deliver value to the business. The data engineer is also expected \\nto create agile data architectures that evolve as new trends emerge. \\nWhat are some things a data engineer does not do? A data engineer typically does not \\ndirectly build ML models, create reports or dashboards, perform data analysis, build \\nkey performance indicators (KPIs), or develop software applications. A data engineer \\nshould have a good functioning understanding of these areas to serve stakeholders \\nbest. \\nData Maturity and the Data Engineer \\nThe level of data engineering complexity within a company depends a great deal on \\nthe company‚Äôs data maturity. This significantly impacts a data engineer‚Äôs day -to-day \\njob responsibilities and career progression. What is data maturity, exactly?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 22, 'page_label': '23'}, page_content='‚Ä¢ \\n Data Engineering Skills and Activities  | \\n 15 \\nData maturity  is the progression toward higher data utilization, capabilities, and \\nintegration across the organization, but data maturity does not simply depend on the \\nage or revenue of a company. An early -stage startup can have greater data maturity \\nthan a 100-year-old company with annual revenues in the billions. What matters is the \\nway data is leveraged as a competitive advantage. \\nData maturity models have many versions, such as Data Management Maturity \\n(DMM) and others, and it‚Äôs hard to pick one that is both simple and useful for data \\nengineering. So, we‚Äôll create our own simplified data maturity model. Our data \\nmaturity model (Figure 1-8) has three stages: starting with data, scaling with data, and \\nleading with data. Let‚Äôs look at each of these stages and at what a data engineer \\ntypically does at each stage. \\n \\nFigure 1-8. Our simplified data maturity model for a company \\nStage 1: Starting with data \\nA company getting started with data is, by definition, in the very early stages of its data \\nmaturity. The company may have fuzzy, loosely defined goals or no goals. Data \\narchitecture and infrastructure are in the very early stages of planning and \\ndevelopment. Adoption and utilization are likely low or nonexistent. The data team is \\nsmall, often with a headcount in the single digits. At this stage, a data engineer is \\nusually a generalist and will typically play several other roles, such as data scientist or \\nsoftware engineer. A data engineer‚Äôs goal is to move fast, get traction, and add value. \\nThe practicalities of getting value from data are typically poorly understood, but the \\ndesire exists. Reports or analyses lack formal structure, and most requests for data are \\nad hoc. While it‚Äôs tempting to jump headfirst into ML at this stage, we don‚Äôt \\nrecommend it. We‚Äôve seen countless data teams get stuck and fall short when they try \\nto jump to ML without building a solid data foundation. \\nThat‚Äôs not to say you can‚Äôt get wins from ML at this stage ‚Äîit is rare but possible. \\nWithout a solid data foundation, you likely won‚Äôt have the data to train reliable ML \\nmodels nor the means to deploy these models to production in a scalable and repeatable \\nway. We half -jokingly call ourselves ‚Äúrecovering data scientists‚Äù , mainly from \\npersonal experience with being involved in premature data science projects without \\nadequate data maturity or data engineering support.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 23, 'page_label': '24'}, page_content='16  | Chapter 1: Data Engineering Described \\nA data engineer should focus on the following in organizations getting started with \\ndata: \\nGet buy-in from key stakeholders, including executive management. Ideally, the \\ndata engineer should have a sponsor for critical initiatives to design and build a \\ndata architecture to support the company‚Äôs goals. \\n‚Ä¢ Define the right data architecture (usually solo, since a data architect likely isn‚Äôt \\navailable). This means determining business goals and the competitive advantage \\nyou‚Äôre aiming to achieve with your data initiative. Work toward a data architecture \\nthat supports these goals. See Chapter 3 for our advice on ‚Äúgood‚Äù data architecture. \\n‚Ä¢ Identify and audit data that will support key initiatives and operate within the data \\narchitecture you designed. \\n‚Ä¢ Build a solid data foundation for future data analysts and data scientists to generate \\nreports and models that provide competitive value. In the meantime, you may also \\nhave to generate these reports and models until this team is hired. \\nThis is a delicate stage with lots of pitfalls. Here are some tips for this stage: \\n‚Ä¢ Organizational willpower may wane if a lot of visible successes don‚Äôt occur with \\ndata. Getting quick wins will establish the importance of data within the \\norganization. Just keep in mind that quick wins will likely create technical debt. \\nHave a plan to reduce this debt, as it will otherwise add friction for future delivery. \\n‚Ä¢ Get out and talk to people, and avoid working in silos. We often see the data team \\nworking in a bubble, not communicating with people outside their departments \\nand getting perspectives and feedback from business stakeholders. The danger is \\nyou‚Äôll spend a lot of time working on things of little use to people. \\n‚Ä¢ Avoid undifferentiated heavy lifting. Don‚Äôt box yourself in with unnecessary \\ntechnical complexity. Use off-the-shelf, turnkey solutions wherever possible. \\n‚Ä¢ Build custom solutions and code only where this creates a competitive advantage. \\nStage 2: Scaling with data \\nAt this point, a company has moved away from ad hoc data requests and has formal \\ndata practices. Now the challenge is creating scalable data architectures and planning \\nfor a future where the company is genuinely data-driven. Data engineering roles move \\nfrom generalists to specialists, with people focusing on particular aspects of the data \\nengineering lifecycle. \\nIn organizations that are in stage 2 of data maturity, a data engineer‚Äôs goals are to do \\nthe following: \\n‚Ä¢ Establish formal data practices'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 24, 'page_label': '25'}, page_content='‚Ä¢ \\n Data Engineering Skills and Activities  | \\n 17 \\n‚Ä¢ Create scalable and robust data architectures'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 25, 'page_label': '26'}, page_content='‚Ä¢ \\n18  | Chapter 1: Data Engineering Described \\nAdopt DevOps and DataOps practices \\n‚Ä¢ Build systems that support ML \\n‚Ä¢ Continue to avoid undifferentiated heavy lifting and customize only when a \\ncompetitive advantage results \\nWe return to each of these goals later in the book. Issues \\nto watch out for include the following: \\n‚Ä¢ As we grow more sophisticated with data, there‚Äôs a temptation to adopt \\nbleedingedge technologies based on social proof from Silicon Valley companies. \\nThis is rarely a good use of your time and energy. Any technology decisions should \\nbe driven by the value they‚Äôll deliver to your customers. \\n‚Ä¢ The main bottleneck for scaling is not cluster nodes, storage, or technology but the \\ndata engineering team. Focus on solutions that are simple to deploy and manage \\nto expand your team‚Äôs throughput. \\n‚Ä¢ You‚Äôll be tempted to frame yourself as a technologist, a data genius who can \\ndeliver magical products. Shift your focus instead to pragmatic leadership and \\nbegin transitioning to the next maturity stage; communicate with other teams about \\nthe practical utility of data. Teach the organization how to consume and leverage \\ndata. \\nStage 3: Leading with data \\nAt this stage, the company is data-driven. The automated pipelines and systems created \\nby data engineers allow people within the company to do self -service analytics and \\nML. Introducing new data sources is seamless, and tangible value is derived. Data \\nengineers implement proper controls and practices to ensure that data is always \\navailable to the people and systems. Data engineering roles continue to specialize more \\ndeeply than in stage 2. \\nIn organizations in stage 3 of data maturity, a data engineer will continue building on \\nprior stages, plus they will do the following: \\n‚Ä¢ Create automation for the seamless introduction and usage of new data \\n‚Ä¢ Focus on building custom tools and systems that leverage data as a competitive \\nadvantage \\n‚Ä¢ Focus on the ‚Äúenterprisey‚Äù aspects of data, such as data management (including \\ndata governance and quality) and DataOps \\n‚Ä¢ Deploy tools that expose and disseminate data throughout the organization, \\nincluding data catalogs, data lineage tools, and metadata management systems'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 26, 'page_label': '27'}, page_content='‚Ä¢ \\n Data Engineering Skills and Activities  | \\n 19 \\nCollaborate efficiently with software engineers, ML engineers, analysts, and \\nothers \\n‚Ä¢ Create a community and environment where people can collaborate and speak \\nopenly, no matter their role or position \\nIssues to watch out for include the following: \\n‚Ä¢ At this stage, complacency is a significant danger. Once organizations reach stage \\n3, they must constantly focus on maintenance and improvement or risk falling \\nback to a lower stage. \\n‚Ä¢ Technology distractions are a more significant danger here than in the other stages. \\nThere‚Äôs a temptation to pursue expensive hobby projects that don‚Äôt deliver value \\nto the business. Utilize custom -built technology only where it provides a \\ncompetitive advantage. \\nThe Background and Skills of a Data Engineer \\nData engineering is a fast -growing field, and a lot of questions remain about how to \\nbecome a data engineer. Because data engineering is a relatively new discipline, little \\nformal training is available to enter the field. Universities don‚Äôt have a standard  data \\nengineering path. Although a handful of data engineering boot camps and online \\ntutorials cover random topics, a common curriculum for the subject doesn‚Äôt yet exist. \\nPeople entering data engineering arrive with varying backgrounds in education, career, \\nand skill set. Everyone entering the field should expect to invest a significant amount \\nof time in self -study. Reading this book is a good starting point; one of the pri mary \\ngoals of this book is to give you a foundation for the knowledge and skills we think \\nare necessary to succeed as a data engineer. \\nIf you‚Äôre pivoting your career into data engineering, we‚Äôve found that the transition is \\neasiest when moving from an adjacent field, such as software engineering, ETL \\ndevelopment, database administration, data science, or data analysis. These disciplines \\ntend to be ‚Äúdata aware‚Äù and provide good context for data roles in an organization. \\nThey also equip folks with the relevant technical skills and context to solve data \\nengineering problems. \\nDespite the lack of a formalized path, a requisite body of knowledge exists that we \\nbelieve a data engineer should know to be successful. By definition, a data engineer \\nmust understand both data and technology. With respect to data, this entails knowing \\nabout various best practices around data management. On the technology end, a data \\nengineer must be aware of various options for tools, their interplay, and their trade -'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 27, 'page_label': '28'}, page_content='20  | Chapter 1: Data Engineering Described \\noffs. This requires a good understanding of software engineering, DataOps, and data \\narchitecture. \\nZooming out, a data engineer must also understand the requirements of data consumers \\n(data analysts and data scientists) and the broader implications of data across the \\norganization. Data engineering is a holistic practice; the best data engineers view their \\nresponsibilities through business and technical lenses. \\nBusiness Responsibilities \\nThe macro responsibilities we list in this section aren‚Äôt exclusive to data engineers but \\nare crucial for anyone working in a data or technology field. Because a simple Google \\nsearch will yield tons of resources to learn about these areas, we will simply list them \\nfor brevity: \\nKnow how to communicate with nontechnical and technical people. \\nCommunication is key, and you need to be able to establish rapport and trust with \\npeople across the organization. We suggest paying close attention to \\norganizational hierarchies, who reports to whom, how people interact, and which \\nsilos exist. These observations will be invaluable to your success. \\nUnderstand how to scope and gather business and product requirements. \\nYou need to know what to build and ensure that your stakeholders agree with your \\nassessment. In addition, develop a sense of how data and technology decisions \\nimpact the business. \\nUnderstand the cultural foundations of Agile, DevOps, and DataOps. \\nMany technologists mistakenly believe these practices are solved through \\ntechnology. We feel this is dangerously wrong. Agile, DevOps, and DataOps are \\nfundamentally cultural, requiring buy-in across the organization. \\nControl costs. \\nYou‚Äôll be successful when you can keep costs low while providing outsized value. \\nKnow how to optimize for time to value, the total cost of ownership, and \\nopportunity cost. Learn to monitor costs to avoid surprises. \\nLearn continuously. \\nThe data field feels like it‚Äôs changing at light speed. People who succeed in it are \\ngreat at picking up new things while sharpening their fundamental knowledge. \\nThey‚Äôre also good at filtering, determining which new developments are most \\nrelevant to their work, which are still immature, and which are just fads. Stay \\nabreast of the field and learn how to learn. \\nA successful data engineer always zooms out to understand the big picture and how to \\nachieve outsized value for the business. Communication is vital, both for technical and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 28, 'page_label': '29'}, page_content='‚Ä¢ \\n Data Engineering Skills and Activities  | \\n 21 \\nnontechnical people. We often see data teams succeed based on their communication \\nwith other stakeholders; success or failure is rarely a technology issue. Knowing how \\nto navigate an organization, scope and gather requirements,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 29, 'page_label': '30'}, page_content='22  | Chapter 1: Data Engineering Described \\ncontrol costs, and continuously learn will set you apart from the data engineers who \\nrely solely on their technical abilities to carry their career. \\nTechnical Responsibilities \\nYou must understand how to build architectures that optimize performance and cost at \\na high level, using prepackaged or homegrown components. Ultimately, architectures \\nand constituent technologies are building blocks to serve the data engineering lifecycle. \\nRecall the stages of the data engineering lifecycle: \\n‚Ä¢ Generation \\n‚Ä¢ Storage \\n‚Ä¢ Ingestion \\n‚Ä¢ Transformation \\n‚Ä¢ Serving \\nThe undercurrents of the data engineering lifecycle are the following: \\n‚Ä¢ Security \\n‚Ä¢ Data management \\n‚Ä¢ DataOps \\n‚Ä¢ Data architecture \\n‚Ä¢ Orchestration \\n‚Ä¢ Software engineering \\nZooming in a bit, we discuss some of the tactical data and technology skills you‚Äôll need \\nas a data engineer in this section; we discuss these in more detail in subsequent chapters. \\nPeople often ask, should a data engineer know how to code? Short answer: yes. A data \\nengineer should have production -grade software engineering chops. We note that the \\nnature of software development projects undertaken by data engineers has changed \\nfundamentally in the last few years. Fully managed services now replace a great deal \\nof low -level programming effort previously expected of engineers, who now use \\nmanaged open source, and simple plug -and-play software -as-a-service (SaaS) \\nofferings. For example, data engineers now focus on high-level abstractions or writing \\npipelines as code within an orchestration framework. \\nEven in a more abstract world, software engineering best practices provide a \\ncompetitive advantage, and data engineers who can dive into the deep architectural \\ndetails of a codebase give their companies an edge when specific technical needs arise. \\nIn short , a data engineer who can‚Äôt write production -grade code will be severely'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 30, 'page_label': '31'}, page_content='Data Engineering Skills and Activities \\n |  23 \\nhindered, and we don‚Äôt see this changing anytime soon. Data engineers remain software \\nengineers, in addition to their many other roles. \\nWhat languages should a data engineer know? We divide data engineering \\nprogramming languages into primary and secondary categories. At the time of this \\nwriting, the primary languages of data engineering are SQL, Python, a Java Virtual \\nMachine (JVM) language (usually Java or Scala), and bash: \\nSQL \\nThe most common interface for databases and data lakes. After briefly being \\nsidelined by the need to write custom MapReduce code for big data processing, \\nSQL (in various forms) has reemerged as the lingua franca of data. \\nPython \\nThe bridge language between data engineering and data science. A growing \\nnumber of data engineering tools are written in Python or have Python APIs. It‚Äôs \\nknown as ‚Äúthe second-best language at everything.‚Äù Python underlies popular data \\ntools such as pandas,  NumPy, Airflow, sci-kit learn, TensorFlow, PyTorch, and \\nPySpark. Python is the glue between underlying components and is frequently a \\nfirst-class API language for interfacing with a framework. \\nJVM languages such as Java and Scala \\nPrevalent for Apache open source projects such as Spark, Hive, and Druid. The \\nJVM is generally more performant than Python and may provide access to lower-\\nlevel features than a Python API (for example, this is the case for Apache Spark \\nand Beam). Understan ding Java or Scala will be beneficial if you‚Äôre using a \\npopular open source data framework. \\nbash \\nThe command -line interface for Linux operating systems. Knowing bash \\ncommands and being comfortable using CLIs will significantly improve your \\nproductivity and workflow when you need to script or perform OS operations. \\nEven today, data engineers frequently use command-line tools like awk or sed to \\nprocess files in a data pipeline or call bash commands from orchestration \\nframeworks. If you‚Äôre using Windows, feel free to substitute PowerShell for bash. \\nThe Unreasonable Effectiveness of SQL \\nThe advent of MapReduce and the big data era relegated SQL to pass√© status. Since then, \\nvarious developments have dramatically enhanced the utility of SQL in the data \\nengineering lifecycle. Spark SQL, Google BigQuery, Snowflake, Hive, and many other \\ndata tools can process massive amounts of data by using declarative, set -theoretic SQL \\nsemantics. SQL is also supported by many streaming frameworks, such as Apache Flink,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 31, 'page_label': '32'}, page_content='24  | Chapter 1: Data Engineering Described \\nBeam, and Kafka. We believe that competent data engineers should be highly proficient \\nin SQL. \\nAre we saying that SQL is a be-all and end-all language? Not at all. SQL is a powerful \\ntool that can quickly solve complex analytics and data transformation problems. Given \\nthat time is a primary constraint for data engineering team throughput, engineers \\nshould embrace tools that combine simplicity and high productivity. Data engineers \\nalso do well to develop expertise in composing SQL with other operations, either \\nwithin frameworks such as Spark and Flink or by using orchestration to combine \\nmultiple tools . Data engineers should also learn modern SQL semantics for dealing \\nwith JavaScript Object Notation (JSON) parsing and nested data and consider \\nleveraging a SQL management framework such as dbt (Data Build Tool). \\nA proficient data engineer also recognizes when SQL is not the right tool for the job \\nand can choose and code in a suitable alternative. A SQL expert could likely write a \\nquery to stem and tokenize raw text in a natural language processing (NLP) pipeline \\nbut would also recognize that coding in native Spark is a far superior alternative to this \\nmasochistic exercise. \\nData engineers may also need to develop proficiency in secondary programming \\nlanguages, including R, JavaScript, Go, Rust, C/C++, C#, and Julia. Developing in \\nthese languages is often necessary when popular across the company or used with \\ndomain-specific data tools. For instance, JavaScript has proven popular as a language \\nfor user -defined functions in cloud data warehouses. At the same time, C# and \\nPowerShell are essential in companies that leverage Azure and the Microsoft \\necosystem. \\nKeeping Pace in a Fast-Moving Field \\nOnce a new technology rolls over you, if you‚Äôre not part of the steamroller, you‚Äôre part of \\nthe road. \\n‚ÄîStewart Brand \\nHow do you keep your skills sharp in a rapidly changing field like data engineering? \\nShould you focus on the latest tools or deep dive into fundamentals? Here‚Äôs our advice: \\nfocus on the fundamentals to understand what‚Äôs not going to change; pay attention t o \\nongoing developments to know where the field is going. New paradigms and practices \\nare introduced all the time, and it‚Äôs incumbent on you to stay current. \\nStrive to understand how new technologies will be helpful in the lifecycle.  \\nThe Continuum of Data Engineering Roles, from A to B \\nAlthough job descriptions paint a data engineer as a ‚Äúunicorn‚Äù who must possess every \\ndata skill imaginable, data engineers don‚Äôt all do the same type of work or have the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 32, 'page_label': '33'}, page_content='Data Engineering Skills and Activities \\n |  25 \\nsame skill set. Data maturity is a helpful guide to understanding the types of data \\nchallenges a company will face as it grows its data capability. It‚Äôs beneficial to look at \\nsome critical distinctions in the kinds of work data engineers do. Though these \\ndistinctions are simplistic, they clarify what data scientists and data engineers do and \\navoid lumping either role into the unicorn bucket. \\nIn data science, there‚Äôs the notion of type A and type B data scientists. 10 Type A data \\nscientists‚Äîwhere A stands for analysis‚Äîfocus on understanding and deriving insight \\nfrom data. Type B data scientists ‚Äîwhere B stands for building‚Äîshare similar \\nbackgrounds as type A data scientists and possess strong programming skills. The type \\nB data scientist builds systems that make data science work in production. Borrowing \\nfrom this data scientist continuum, we‚Äôll create a similar distinction  for two types of \\ndata engineers: \\nType A data engineers \\nA stands for abstraction. In this case, the data engineer avoids undifferentiated \\nheavy lifting, keeping data architecture as abstract and straightforward as possible \\nand not reinventing the wheel. Type A data engineers manage the data engineering \\nlifecycle mainly by using entire ly off-the-shelf products, managed services, and \\ntools. Type A data engineers work at companies across industries and at all levels \\nof data maturity. \\nType B data engineers \\nB stands for build. Type B data engineers build data tools and systems that scale \\nand leverage a company‚Äôs core competency and competitive advantage. In the data \\nmaturity range, a type B data engineer is more commonly found at companies in \\nstage 2 and 3 (scaling and leading with data), or when an initial data use case is so \\nunique and mission-critical that custom data tools are required to get started. \\nType A and type B data engineers may work in the same company and may even be the \\nsame person! More commonly, a type A data engineer is first hired to set the foundation, \\nwith type B data engineer skill sets either learned or hired as the need arises withi n a \\ncompany. \\nData Engineers Inside an Organization \\nData engineers don‚Äôt work in a vacuum. Depending on what they‚Äôre working on, they \\nwill interact with technical and nontechnical people and face different directions \\n(internal and external). Let‚Äôs explore what data engineers do inside an organization and \\nwith whom they interact. \\n \\n10 Robert Chang, ‚ÄúDoing Data Science at Twitter,‚Äù Medium, June 20, 2015, https://oreil.ly/xqjAx.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 33, 'page_label': '34'}, page_content='26  | Chapter 1: Data Engineering Described \\nInternal-Facing Versus External-Facing Data Engineers \\nA data engineer serves several end users and faces many internal and external directions \\n(Figure 1-9). Since not all data engineering workloads and responsibilities are the same, \\nit‚Äôs essential to understand whom the data engineer serves. Depending on the end -use \\ncases, a data engineer‚Äôs primary responsibilities are external facing, internal facing, or \\na blend of the two. \\n \\nFigure 1-9. The directions a data engineer faces \\nAn external-facing data engineer typically aligns with the users of external -facing \\napplications, such as social media apps, Internet of Things (IoT) devices, and \\necommerce platforms. This data engineer architects, builds, and manages the systems \\nthat collect, store, and process transactional and event data from these applications. The \\nsystems built by these data engineers have a feedback loop from the application to the \\ndata pipeline, and then back to the application (Figure 1-10). \\n \\nFigure 1-10. External-facing data engineer systems \\nExternal-facing data engineering comes with a unique set of problems. Externalfacing \\nquery engines often handle much larger concurrency loads than internalfacing systems. \\nEngineers also need to consider putting tight limits on queries that users can run to limit \\nthe infrastructure impact of any single user. In addition, security is a much more \\ncomplex and sensitive problem for external queries, especially if the data being queried \\nis multitenant (data from many customers and housed in a single table). \\nAn internal-facing data engineer typically focuses on activities crucial to the needs of \\nthe business and internal stakeholders ( Figure 1-11). Examples include creating and \\nmaintaining data pipelines and data warehouses for BI dashboards, reports, business \\nprocesses, data science, and ML models.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 34, 'page_label': '35'}, page_content='Data Engineers Inside an Organization \\n |  27 \\n \\nFigure 1-11. Internal-facing data engineer \\nExternal-facing and internal -facing responsibilities are often blended. In practice, \\ninternal-facing data is usually a prerequisite to external-facing data. The data engineer \\nhas two sets of users with very different requirements for query concurrency, security, \\nand more. \\nData Engineers and Other Technical Roles \\nIn practice, the data engineering lifecycle cuts across many domains of responsibility. \\nData engineers sit at the nexus of various roles, directly or through managers, \\ninteracting with many organizational units. \\nLet‚Äôs look at whom a data engineer may impact. In this section, we‚Äôll discuss technical \\nroles connected to data engineering (Figure 1-12). \\n \\nFigure 1-12. Key technical stakeholders of data engineering \\nThe data engineer is a hub between data producers, such as software engineers, data \\narchitects, and DevOps or site-reliability engineers (SREs), and data consumers, such \\nas data analysts, data scientists, and ML engineers. In addition, data engineers will \\ninteract with those in operational roles, such as DevOps engineers. \\nGiven the pace at which new data roles come into vogue (analytics and ML engineers \\ncome to mind), this is by no means an exhaustive list. \\nUpstream stakeholders \\nTo be successful as a data engineer, you need to understand the data architecture you‚Äôre \\nusing or designing and the source systems producing the data you‚Äôll need. Next, we'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 35, 'page_label': '36'}, page_content='28  | Chapter 1: Data Engineering Described \\ndiscuss a few familiar upstream stakeholders: data architects, software engineers, and \\nDevOps engineers. \\nData architects.    Data architects function at a level of abstraction one step removed \\nfrom data engineers. Data architects design the blueprint for organizational data \\nmanagement, mapping out processes and overall data architecture and systems.11They \\nalso serve as a bridge between an organization‚Äôs technical and nontechnical sides. \\nSuccessful data architects generally have ‚Äúbattle scars‚Äù from extensive engineering \\nexperience, allowing them to guide and assist engineers while successfully \\ncommunicating engineering challenges to nontechnical business stakeholders. Data \\narchitects implement policies for managing data across silos and business units, steer \\nglobal strategies such as data management and data governance, and guide significant \\ninitiatives. Data architects often play a central role in cloud migrations and greenfield \\ncloud design. \\nThe advent of the cloud has shifted the boundary between data architecture and data \\nengineering. Cloud data architectures are much more fluid than on -premises systems, \\nso architecture decisions that traditionally involved extensive study, long lead times, \\npurchase contracts, and hardware installation are now often made during the \\nimplementation process, just one step in a larger strategy. Nevertheless, data architects \\nwill remain influential visionaries in enterprises, working hand in hand with data \\nengineers to determine the big picture of architecture practices and data strategies. \\nDepending on the company‚Äôs data maturity and size, a data engineer may overlap with \\nor assume the responsibilities of a data architect. Therefore, a data engineer should have \\na good understanding of architecture best practices and approaches. \\nNote that we have placed data architects in the upstream stakeholders section. Data \\narchitects often help design application data layers that are source systems for data \\nengineers. Architects may also interact with data engineers at various other stages of \\nthe data engineering lifecycle. We cover ‚Äúgood‚Äù data architecture in Chapter 3. \\nSoftware engineers.    Software engineers build the software and systems that run a \\nbusiness; they are largely responsible for generating the internal data  that data \\nengineers will consume and process. The systems built by software engineers typically \\ngenerate application event data and logs, which are significant assets in their own right. \\nThis internal data contrasts with external data pulled from SaaS platforms or partner \\nbusinesses. In well-run technical organizations, software engineers and data engineers \\n \\n11 Paramita (Guha) Ghosh, ‚ÄúData Architect vs. Data Engineer,‚Äù Dataversity, November 12, 2021, \\nhttps://oreil.ly/TlyZY.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 36, 'page_label': '37'}, page_content='Data Engineers Inside an Organization \\n |  29 \\ncoordinate from the inception of a new project to design application data for \\nconsumption by analytics and ML applications. \\nA data engineer should work together with software engineers to understand the \\napplications that generate data, the volume, frequency, and format of the generated data, \\nand anything else that will impact the data engineering lifecycle, such as data security \\nand regulatory compliance. For example, this might mean setting upstream \\nexpectations on what the data software engineers need to do their jobs. Data engineers \\nmust work closely with the software engineers. \\nDevOps engineers and site -reliability engineers.    DevOps and SREs often \\nproduce data through operational monitoring. We classify them as upstream of data \\nengineers, but they may also be downstream, consuming data through dashboards or \\ninteracting with data engineers directly in coordinating operations of data systems. \\nDownstream stakeholders \\nData engineering exists to serve downstream data consumers and use cases. This \\nsection discusses how data engineers interact with various downstream roles. We‚Äôll also \\nintroduce a few service models, including centralized data engineering teams and cross-\\nfunctional teams. \\nData scientists.    Data scientists build forward-looking models to make predictions \\nand recommendations. These models are then evaluated on live data to provide value \\nin various ways. For example, model scoring might determine automated actions in \\nresponse to real -time condi tions, recommend products to customers based on the \\nbrowsing history in their current session, or make live economic predictions used by \\ntraders. \\nAccording to common industry folklore, data scientists spend 70% to 80% of their time \\ncollecting, cleaning, and preparing data. 12  In our experience, these numbers often \\nreflect immature data science and data engineering practices. In particular, many \\npopular data science frameworks can become bottlenecks if they are not scaled up \\nappropriately. Data scientists who work exclusively o n a single workstation force \\nthemselves to downsample data, making data preparation significantly more \\ncomplicated and potentially compromising the quality of the models they produce. \\nFurthermore, locally developed code and environments are often difficult to deploy in \\n \\n12 A variety of references exist for this notion. Although this cliche is widely known, a healthy debate has arisen \\naround its validity in different practical settings. For more details, see Leigh Dodds, ‚ÄúDo Data Scientists \\nSpend 80% of Their Time Cleaning Data? Turns Out, No?‚Äù Lost Boy blog, January 31, 2020, \\nhttps://oreil.ly/szFww; and Alex Woodie, ‚ÄúData Prep Still Dominates Data Scientists‚Äô Time, Survey Finds,‚Äù \\nDatanami, July 6, 2020, https://oreil.ly/jDVWF.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 37, 'page_label': '38'}, page_content='30  | Chapter 1: Data Engineering Described \\nproduction, and a lack of automation significantly hampers data science workflows. If \\ndata engineers do their job and collaborate successfully, data scientists shouldn‚Äôt spend \\ntheir time collecting, cleaning, and preparing data after initial exploratory wo rk. Data \\nengineers should automate this work as much as possible. \\nThe need for production-ready data science is a significant driver behind the emergence \\nof the data engineering profession. Data engineers should help data scientists to enable \\na path to production. In fact, we (the authors) moved from data science to data  \\nengineering after recognizing this fundamental need. Data engineers work to provide \\nthe data automation and scale that make data science more efficient. \\nData analysts.    Data analysts (or business analysts) seek to understand business \\nperformance and trends. Whereas data scientists are forward -looking, a data analyst \\ntypically focuses on the past or present. Data analysts usually run SQL queries in a data \\nwarehouse or a da ta lake. They may also utilize spreadsheets for computation and \\nanalysis and various BI tools such as Microsoft Power BI, Looker, or Tableau. Data \\nanalysts are domain experts in the data they work with frequently and become \\nintimately familiar with data definitions, characteristics, and quality problems. A data \\nanalyst‚Äôs typical downstream customers are business users, management, and \\nexecutives. \\nData engineers work with data analysts to build pipelines for new data sources required \\nby the business. Data analysts‚Äô subject-matter expertise is invaluable in improving data \\nquality, and they frequently collaborate with data engineers in this capacity. \\nMachine learning engineers and AI researchers.    Machine learning engineers \\n(ML engineers) overlap with data engineers and data scientists. ML engineers develop \\nadvanced ML techniques, train models, and design and maintain the infrastructure \\nrunning ML processes in a scaled production environment. ML eng ineers often have \\nadvanced working knowledge of ML and deep learning techniques and frameworks \\nsuch as PyTorch or TensorFlow. \\nML engineers also understand the hardware, services, and systems required to run these \\nframeworks, both for model training and model deployment at a production scale. It‚Äôs \\ncommon for ML flows to run in a cloud environment where ML engineers can spin up \\nand scale infrastructure resources on demand or rely on managed services. \\nAs we‚Äôve mentioned, the boundaries between ML engineering, data engineering, and \\ndata science are blurry. Data engineers may have some operational responsibilities over \\nML systems, and data scientists may work closely with ML engineering in designing \\nadvanced ML processes.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 38, 'page_label': '39'}, page_content='Data Engineers Inside an Organization \\n |  31 \\nThe world of ML engineering is snowballing and parallels a lot of the same \\ndevelopments occurring in data engineering. Whereas several years ago, the attention \\nof ML was focused on how to build models, ML engineering now increasingly \\nemphasizes incorporating best practices of machine learning operations (MLOps) and \\nother mature practices previously adopted in software engineering and DevOps. AI \\nresearchers work on new, advanced ML techniques. AI researchers may work inside \\nlarge technology companies, specia lized intellectual property startups (OpenAI, \\nDeepMind), or academic institutions. Some practitioners are dedicated to part -time \\nresearch in conjunction with ML engineering responsibilities inside a company. Those \\nworking inside specialized ML labs are oft en 100% dedicated to research. Research \\nproblems may target immediate practical applications or more abstract demonstrations \\nof AI. DALL -E, Gato AI, AlphaGo, and GPT -3/GPT-4 are great examples of ML \\nresearch projects. Given the pace of advancements in ML, these examples will very \\nlikely be quaint in a few years‚Äô time. We‚Äôve provided some references in ‚ÄúAdditional \\nResources‚Äù on page 32. \\nAI researchers in well -funded organizations are highly specialized and operate with \\nsupporting teams of engineers to facilitate their work. ML engineers in academia \\nusually have fewer resources but rely on teams of graduate students, postdocs, and \\nuniversity staff to provide engineering support. ML engineers who are partially \\ndedicated to research often rely on the same support teams for research and production. \\nData Engineers and Business Leadership \\nWe‚Äôve discussed technical roles with which a data engineer interacts. But data \\nengineers also operate more broadly as organizational connectors, often in a \\nnontechnical capacity. Businesses have come to rely increasingly on data as a core part \\nof many prod ucts or a product in itself. Data engineers now participate in strategic \\nplanning and lead key initiatives that extend beyond the boundaries of IT. Data \\nengineers often support data architects by acting as the glue between the business and \\ndata science/analytics. \\nData in the C-suite \\nC-level executives are increasingly involved in data and analytics, as these are \\nrecognized as significant assets for modern businesses. For example, CEOs now \\nconcern themselves with initiatives that were once the exclusive province of IT, such \\nas cloud migrations or deployment of a new customer data platform. \\nChief executive officer.    Chief executive officers (CEOs) at nontech companies \\ngenerally don‚Äôt concern themselves with the nitty -gritty of data frameworks and \\nsoftware. Instead, they define a vision in collaboration with technical C-suite roles and \\ncompany data leadership. Data engineers provide a window into what‚Äôs possible with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 39, 'page_label': '40'}, page_content='32  | Chapter 1: Data Engineering Described \\ndata. Data engineers and their managers maintain a map of what data is available to the \\norganization‚Äîboth internally and from third parties ‚Äîin what time frame. They are \\nalso tasked to study primary data architectural changes in collaboration with other \\nengineering roles. For example, data engineers are often heavily involved in cloud \\nmigrations, migrations to new data systems, or deployment of streaming technologies. \\nChief information officer.    A chief information officer (CIO) is the senior C -suite \\nexecutive responsible for information technology within an organization; it is an \\ninternal-facing role. A CIO must possess deep knowledge of information technology \\nand business processes ‚Äîeither alone is insufficient. CIOs direct the information \\ntechnology organization, setting ongoing policies while also defining and executing \\nsignificant initiatives under the direction of the CEO. \\nCIOs often collaborate with data engineering leadership in organizations with a \\nwelldeveloped data culture. If an organization is not very high in its data maturity, a \\nCIO will typically help shape its data culture. CIOs will work with engineers and \\narchitects to map out major initiatives and make strategic decisions on adopting major \\narchitectural elements, such as enterprise resource planning (ERP) and customer \\nrelationship management (CRM) systems, cloud migrations, data systems, and \\ninternal-facing IT. \\nChief technology officer.    A chief technology officer (CTO) is similar to a CIO but \\nfaces outward. A CTO owns the key technological strategy and architectures for \\nexternalfacing applications, such as mobile, web apps, and IoT ‚Äîall critical data \\nsources for data engineers. The CTO is likely a skilled technologist and has a good \\nsense of software engineering fundamentals and system architecture. In some \\norganizations without a CIO, the CTO or sometimes the chief operating officer (COO) \\nplays the role of CIO. Data engineers often report directly or indirectly through a CTO. \\nChief data officer.    The chief data officer (CDO) was created in 2002 at Capital One \\nto recognize the growing importance of data as a business asset. The CDO is \\nresponsible for a company‚Äôs data assets and strategy. CDOs are focused on data‚Äôs \\nbusiness utility but should have a strong technical grounding. CDOs oversee data \\nproducts, strategy, initiatives, and core functions such as master data management and \\nprivacy. Occasionally, CDOs manage business analytics and data engineering. \\nChief analytics officer.    The chief analytics officer (CAO) is a variant of the CDO \\nrole. Where both roles exist, the CDO focuses on the technology and organization \\nrequired to deliver data. The CAO is responsible for analytics, strategy, and decision \\nmaking for the business. A CAO may oversee data science and ML, though this largely \\ndepends on whether the company has a CDO or CTO role.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 40, 'page_label': '41'}, page_content='Data Engineers Inside an Organization \\n |  33 \\nChief algorithms officer.    A chief algorithms officer (CAO-2) is a recent innovation \\nin the C -suite, a highly technical role focused specifically on data science and ML. \\nCAO-2s typically have experience as individual contributors and team leads in data \\nscience or ML projects. Freque ntly, they have a background in ML research and a \\nrelated advanced degree. \\nCAO-2s are expected to be conversant in current ML research and have deep technical \\nknowledge of their company‚Äôs ML initiatives. In addition to creating business \\ninitiatives, they provide technical leadership, set research and development agendas, \\nand build research teams. \\nData engineers and project managers \\nData engineers often work on significant initiatives, potentially spanning many years. \\nAs we write this book, many data engineers are working on cloud migrations, migrating \\npipelines and warehouses to the next generation of data tools. Other data engineers are \\nstarting greenfield projects, assembling new data architectures from scratch by \\nselecting from an astonishing number of best-of-breed architecture and tooling options. \\nThese large initiatives often benefit from project management (in contrast to product \\nmanagement, discussed next). Whereas data engineers function in an infrastructure and \\nservice delivery capacity, project managers direct traffic and serve as gatekeepers. Most \\nproject managers operate according to some variation of  Agile and Scrum, with \\nWaterfall still appearing occasionally. Business never sleeps, and business stakeholders \\noften have a significant backlog of things they want to address and new initiatives they \\nwant to launch. Project managers must filter a long list of requests and prioritize critical \\ndeliverables to keep projects on track and better serve the company. \\nData engineers interact with project managers, often planning sprints for projects and \\nensuing standups related to the sprint. Feedback goes both ways, with data engineers \\ninforming project managers and other stakeholders about progress and blockers, and \\nproject managers balancing the cadence of technology teams against the ever-changing \\nneeds of the business. \\nData engineers and product managers \\nProduct managers oversee product development, often owning product lines. In the \\ncontext of data engineers, these products are called data products. Data products are \\neither built from the ground up or are incremental improvements upon existing \\nproducts. Data engineers interact more frequently with product managers  as the \\ncorporate world has adopted a data -centric focus . Like project managers, product \\nmanagers balance the activity of technology teams against the needs of the customer \\nand business.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 41, 'page_label': '42'}, page_content='|  34 \\nData engineers and other management roles \\nData engineers interact with various managers beyond project and product managers. \\nHowever, these interactions usually follow either the services or cross -functional \\nmodels. Data engineers either serve a variety of incoming requests as a centralized \\nteam or work as a resource assigned to a particular manager, project, or product. \\nFor more information on data teams and how to structure them, we recommend John \\nThompson‚Äôs Building Analytics Teams  (Packt) and Jesse Anderson‚Äôs Data Teams \\n(Apress). Both books provide strong frameworks and perspectives on the roles of \\nexecutives with data, who to hire, and how to construct the most effective data team \\nfor your company. \\nCompanies don‚Äôt hire engineers simply to hack on code in isolation. \\nTo be worthy of their title, engineers should develop a deep \\nunderstanding of the problems they‚Äôre tasked with solving, the \\ntechnology tools at their disposal, and the people they work with and \\nserve. \\nConclusion \\nThis chapter provided you with a brief overview of the data engineering landscape, \\nincluding the following: \\n‚Ä¢ Defining data engineering and describing what data engineers do \\n‚Ä¢ Describing the types of data maturity in a company \\n‚Ä¢ Type A and type B data engineers \\n‚Ä¢ Whom data engineers work with \\nWe hope that this first chapter has whetted your appetite, whether you are a software \\ndevelopment practitioner, data scientist, ML engineer, business stakeholder, \\nentrepreneur, or venture capitalist. Of course, a great deal still remains to elucidate in \\nsubsequent chapters. Chapter 2  covers the data engineering lifecycle, followed by \\narchitecture in Chapter 3. The following chapters get into the nitty-gritty of technology \\ndecisions for each part of the lifecycle. The entire data field is in flux, and as much as \\npossible, each chapter focuses on the immutables‚Äîperspectives that will be valid for \\nmany years amid relentless change.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 42, 'page_label': '43'}, page_content='35 \\nConclusion  \\nAdditional Resources \\n‚Ä¢ ‚ÄúThe AI Hierarchy of Needs‚Äù by Monica Rogati \\n‚Ä¢ The AlphaGo research web page \\n‚Ä¢ ‚ÄúBig Data Will Be Dead in Five Years‚Äù by Lewis Gavin \\n‚Ä¢ Building Analytics Teams by John K. Thompson (Packt) \\n‚Ä¢ Chapter 1 of What Is Data Engineering? by Lewis Gavin (O‚ÄôReilly) \\n‚Ä¢ ‚ÄúData as a Product vs. Data as a Service‚Äù by Justin Gage \\n‚Ä¢ ‚ÄúData Engineering: A Quick and Simple Definition‚Äù by James Furbush (O‚ÄôReilly) \\n‚Ä¢ Data Teams by Jesse Anderson (Apress) \\n‚Ä¢ ‚ÄúDoing Data Science at Twitter‚Äù by Robert Chang \\n‚Ä¢ ‚ÄúThe Downfall of the Data Engineer‚Äù by Maxime Beauchemin \\n‚Ä¢ ‚ÄúThe Future of Data Engineering Is the Convergence of Disciplines‚Äù  by Liam \\nHausmann \\n‚Ä¢ ‚ÄúHow CEOs Can Lead a Data-Driven Culture‚Äù by Thomas H. Davenport and Nitin \\nMittal \\n‚Ä¢ ‚ÄúHow Creating a Data-Driven Culture Can Drive Success‚Äù by Frederik Bussler \\n‚Ä¢ The Information Management Body of Knowledge website \\n‚Ä¢ ‚ÄúInformation Management Body of Knowledge‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúInformation Management‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúOn Complexity in Big Data‚Äù by Jesse Anderson (O‚ÄôReilly) \\n‚Ä¢ ‚ÄúOpenAI‚Äôs New Language Generator GPT -3 Is Shockingly Good ‚Äîand \\nCompletely Mindless‚Äù by Will Douglas Heaven \\n‚Ä¢ ‚ÄúThe Rise of the Data Engineer‚Äù by Maxime Beauchemin \\n‚Ä¢ ‚ÄúA Short History of Big Data‚Äù by Mark van Rijmenam \\n‚Ä¢ ‚ÄúSkills of the Data Architect‚Äù by Bob Lambert \\n‚Ä¢ ‚ÄúThe Three Levels of Data Analysis: A Framework for Assessing Data \\nOrganization Maturity‚Äù by Emilie Schario \\n‚Ä¢ ‚ÄúWhat Is a Data Architect? IT‚Äôs Data Framework Visionary‚Äù by Thor Olavsrud \\n‚Ä¢ ‚ÄúWhich Profession Is More Complex to Become, a Data Engineer or a Data \\nScientist?‚Äù thread on Quora \\n‚Ä¢ ‚ÄúWhy CEOs Must Lead Big Data Initiatives‚Äù by John Weathington \\nCHAPTER 2'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 43, 'page_label': '44'}, page_content='36  | Chapter 1: Data Engineering Described \\nThe Data Engineering Lifecycle \\nThe major goal of this book is to encourage you to move beyond viewing data \\nengineering as a specific collection of data technologies. The data landscape is \\nundergoing an explosion of new data technologies and practices, with ever-increasing \\nlevels of abstraction and ease of use. Because of increased technical abstraction, data \\nengineers will increasingly become data lifecycle engineers, thinking and operating in \\nterms of the principles of data lifecycle management. \\nIn this chapter, you‚Äôll learn about the data engineering lifecycle, which is the central \\ntheme of this book. The data engineering lifecycle is our framework describing ‚Äúcradle \\nto grave‚Äù data engineering . You will also learn about the undercurrents of the data \\nengineering lifecycle, which are key foundations that support all data engineering \\nefforts. \\nWhat Is the Data Engineering Lifecycle? \\nThe data engineering lifecycle comprises stages that turn raw data ingredients into a \\nuseful end product, ready for consumption by analysts, data scientists, ML engineers, \\nand others. This chapter introduces the major stages of the data engineering lifecycle, \\nfocusing on each stage‚Äôs core concepts and saving details for later chapters. We divide \\nthe data engineering lifecycle into five stages (Figure 2-1, top): \\n‚Ä¢ Generation \\n‚Ä¢ Storage \\n‚Ä¢ Ingestion \\n‚Ä¢ Transformation \\n‚Ä¢ Serving data'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 44, 'page_label': '45'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 37 \\n \\nFigure 2-1. Components and undercurrents of the data engineering lifecycle \\nWe begin the data engineering lifecycle by getting data from source systems and \\nstoring it. Next, we transform the data and then proceed to our central goal, serving \\ndata to analysts, data scientists, ML engineers, and others. In reality, storage occurs \\nthroughout the lifecycle as data flows from beginning to end ‚Äîhence, the diagram \\nshows the storage ‚Äústage‚Äù as a foundation that underpins other stages. \\nIn general, the middle stages ‚Äîstorage, ingestion, transformation ‚Äîcan get a bit \\njumbled. And that‚Äôs OK. Although we split out the distinct parts of the data engineering \\nlifecycle, it‚Äôs not always a neat, continuous flow. Various stages of the lifecycle may \\nrepeat themselves, occur out of order, overlap, or weave together in interesting and \\nunexpected ways. \\nActing as a bedrock are undercurrents (Figure 2-1, bottom) that cut across multiple \\nstages of the data engineering lifecycle: security, data management, DataOps, data \\narchitecture, orchestration, and software engineering. No part of the data engineering \\nlifecycle can adequately function without these undercurrents. \\nThe Data Lifecycle Versus the Data Engineering \\nLifecycle \\nYou may be wondering about the difference between the overall data lifecycle and the \\ndata engineering lifecycle. There‚Äôs a subtle distinction between the two. The data \\nengineering lifecycle is a subset of the whole data lifecycle (Figure 2-2). Whereas the \\nfull data lifecycle encompasses data across its entire lifespan, the data engineering \\nlifecycle focuses on the stages a data engineer controls.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 45, 'page_label': '46'}, page_content='38  | Chapter 2: The Data Engineering Lifecycle \\n \\nFigure 2-2. The data engineering lifecycle is a subset of the full data lifecycle \\nGeneration: Source Systems \\nA source system is the origin of the data used in the data engineering lifecycle. For \\nexample, a source system could be an IoT device, an application message queue, or a \\ntransactional database. A data engineer consumes data from a source system but \\ndoesn‚Äôt typically own or control the source system itself. The data engineer needs to \\nhave a working understanding of the way source systems work, the way they generate \\ndata, the frequency and velocity of the data, and the variety of data they generate. \\nEngineers also need to keep an open line of communication with source system owners \\non changes that could break pipelines and analytics. Application code might change \\nthe structure of data in a field, or the application team might even choose to migrate \\nthe backend to an entirely new database technology. \\nA major challenge in data engineering is the dizzying array of data source systems \\nengineers must work with and understand. As an illustration, let‚Äôs look at two common \\nsource systems, one very traditional (an application database) and the other a more \\nrecent example (IoT swarms). \\nFigure 2 -3 illustrates a traditional source system with several application servers \\nsupported by a database. This source system pattern became popular in the 1980s with \\nthe explosive success of relational database management systems (RDBMSs). The \\napplication + database pattern remains popular today with various modern evolutions \\nof software development practices. For example, applications often consist of many \\nsmall service/database pairs with microservices rather than a single monolith. \\n \\nFigure 2-3. Source system example: an application database'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 46, 'page_label': '47'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 39 \\nLet‚Äôs look at another example of a source system. Figure 2-4 illustrates an IoT swarm: \\na fleet of devices (circles) sends data messages (rectangles) to a central collection \\nsystem. This IoT source system is increasingly common as IoT devices such as sensors, \\nsmart devices, and much more increase in the wild. \\n \\nFigure 2-4. Source system example: an IoT swarm and message queue \\nEvaluating source systems: Key engineering considerations \\nThere are many things to consider when assessing source systems, including how the \\nsystem handles ingestion, state, and data generation. The following is a starting set of \\nevaluation questions of source systems that data engineers must consider: \\n‚Ä¢ What are the essential characteristics of the data source? Is it an application? A \\nswarm of IoT devices? \\n‚Ä¢ How is data persisted in the source system? Is data persisted long term, or is it \\ntemporary and quickly deleted? \\n‚Ä¢ At what rate is data generated? How many events per second? How many \\ngigabytes per hour? \\n‚Ä¢ What level of consistency can data engineers expect from the output data? If \\nyou‚Äôre running data -quality checks against the output data, how often do data \\ninconsistencies occur‚Äînulls where they aren‚Äôt expected, lousy formatting, etc.? \\n‚Ä¢ How often do errors occur? \\n‚Ä¢ Will the data contain duplicates? \\n‚Ä¢ Will some data values arrive late, possibly much later than other messages \\nproduced simultaneously? \\n‚Ä¢ What is the schema of the ingested data? Will data engineers need to join across \\nseveral tables or even several systems to get a complete picture of the data?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 47, 'page_label': '48'}, page_content='40  | Chapter 2: The Data Engineering Lifecycle \\n‚Ä¢ If schema changes (say, a new column is added), how is this dealt with and \\ncommunicated to downstream stakeholders? \\n‚Ä¢ How frequently should data be pulled from the source system? \\n‚Ä¢ For stateful systems (e.g., a database tracking customer account information), is \\ndata provided as periodic snapshots or update events from change data capture \\n(CDC)? What‚Äôs the logic for how changes are performed, and how are these \\ntracked in the source database? \\n‚Ä¢ Who/what is the data provider that will transmit the data for downstream \\nconsumption? \\n‚Ä¢ Will reading from a data source impact its performance? \\n‚Ä¢ Does the source system have upstream data dependencies? What are the \\ncharacteristics of these upstream systems? \\n‚Ä¢ Are data-quality checks in place to check for late or missing data? \\nSources produce data consumed by downstream systems, including humangenerated \\nspreadsheets, IoT sensors, and web and mobile applications. Each source has its unique \\nvolume and cadence of data generation. A data engineer should know how the source \\ngenerates data, including relevant quirks or nuances. Data engineers also need to \\nunderstand the limits of the source systems they interact with. For example, will \\nanalytical queries against a source application database cause resource contention and \\nperformance issues? \\nOne of the most challenging nuances of source data is the schema. The schema defines \\nthe hierarchical organization of data. Logically, we can think of data at the level of a \\nwhole source system, drilling down into individual tables, all the way to the structure \\nof respective fields. The schema of data shipped from source systems is  handled in \\nvarious ways. Two popular options are schemaless and fixed schema. \\nSchemaless doesn‚Äôt mean the absence of schema. Rather, it means that the application \\ndefines the schema as data is written, whether to a message queue, a flat file, a blob, \\nor a document database such as MongoDB. A more traditional model built on relational \\ndatabase storage uses a fixed schema enforced in the database, to which application \\nwrites must conform. \\nEither of these models presents challenges for data engineers. Schemas change over \\ntime; in fact, schema evolution is encouraged in the Agile approach to software \\ndevelopment. A key part of the data engineer‚Äôs job is taking raw data input in the source \\nsystem schema and transforming this into valuable output for analytics. This job \\nbecomes more challenging as the source schema evolves.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 48, 'page_label': '49'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 41 \\nWe dive into source systems in greater detail in Chapter 5; we also cover schemas and \\ndata modeling in Chapters 6 and 8, respectively. \\nStorage \\nYou need a place to store data. Choosing a storage solution is key to success in the rest \\nof the data lifecycle, and it‚Äôs also one of the most complicated stages of the data \\nlifecycle for a variety of reasons. First, data architectures in the cloud often l everage \\nseveral storage solutions. Second, few data storage solutions function purely as \\nstorage, with many supporting complex transformation queries; even object storage \\nsolutions may support powerful query capabilities ‚Äîe.g., Amazon S3 Select . Third, \\nwhile storage is a stage of the data engineering lifecycle, it frequently touches on other \\nstages, such as ingestion, transformation, and serving. \\nStorage runs across the entire data engineering lifecycle, often occurring in multiple \\nplaces in a data pipeline, with storage systems crossing over with source systems, \\ningestion, transformation, and serving. In many ways, the way data is stored impacts \\nhow it is used in all of the stages of the data engineering lifecycle. For example, cloud \\ndata warehouses can store data, process data in pipelines, and serve it to analysts. \\nStreaming frameworks such as Apache Kafka and Pulsar can function simultaneously \\nas ingestion, storage, and query systems for messages, with object storage being a \\nstandard layer for data transmission. \\nEvaluating storage systems: Key engineering considerations \\nHere are a few key engineering questions to ask when choosing a storage system for a \\ndata warehouse, data lakehouse, database, or object storage: \\n‚Ä¢ Is this storage solution compatible with the architecture‚Äôs required write and read \\nspeeds? \\n‚Ä¢ Will storage create a bottleneck for downstream processes? \\n‚Ä¢ Do you understand how this storage technology works? Are you utilizing the \\nstorage system optimally or committing unnatural acts? For instance, are you \\napplying a high rate of random access updates in an object storage system? (This \\nis an antipattern with significant performance overhead.) \\n‚Ä¢ Will this storage system handle anticipated future scale? You should consider all \\ncapacity limits on the storage system: total available storage, read operation rate, \\nwrite volume, etc. \\n‚Ä¢ Will downstream users and processes be able to retrieve data in the required \\nservice-level agreement (SLA)? \\n‚Ä¢ Are you capturing metadata about schema evolution, data flows, data lineage, and \\nso forth? Metadata has a significant impact on the utility of data. Metadata'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 49, 'page_label': '50'}, page_content='42  | Chapter 2: The Data Engineering Lifecycle \\nrepresents an investment in the future, dramatically enhancing discoverability and \\ninstitutional knowledge to streamline future projects and architecture changes. \\n‚Ä¢ Is this a pure storage solution (object storage), or does it support complex query \\npatterns (i.e., a cloud data warehouse)? \\n‚Ä¢ Is the storage system schema -agnostic (object storage)? Flexible schema \\n(Cassandra)? Enforced schema (a cloud data warehouse)? \\n‚Ä¢ How are you tracking master data, golden records data quality, and data lineage \\nfor data governance? (We have more to say on these in ‚ÄúData Management‚Äù on \\npage 50.) \\n‚Ä¢ How are you handling regulatory compliance and data sovereignty? For example, \\ncan you store your data in certain geographical locations but not others? \\nUnderstanding data access frequency \\nNot all data is accessed in the same way. Retrieval patterns will greatly vary based on \\nthe data being stored and queried. This brings up the notion of the ‚Äútemperatures‚Äù of \\ndata. Data access frequency will determine the temperature of your data. \\nData that is most frequently accessed is called hot data. Hot data is commonly retrieved \\nmany times per day, perhaps even several times per second‚Äîfor example, in systems \\nthat serve user requests. This data should be stored for fast retrieval, where ‚Äúfast‚Äù is \\nrelative to the use case. Lukewarm data might be accessed every so often‚Äîsay, every \\nweek or month. \\nCold data is seldom queried and is appropriate for storing in an archival system. Cold \\ndata is often retained for compliance purposes or in case of a catastrophic failure in \\nanother system. In the ‚Äúold days,‚Äù cold data would be stored on tapes and shipped to \\nremote archival facilities. In cloud environments, vendors offer specialized storage \\ntiers with very cheap monthly storage costs but high prices for data retrieval. \\nSelecting a storage system \\nWhat type of storage solution should you use? This depends on your use cases, data \\nvolumes, frequency of ingestion, format, and size of the data being ingested ‚Äî \\nessentially, the key considerations listed in the preceding bulleted questions. There is \\nno one-size-fits-all universal storage recommendation. Every storage technology has \\nits trade-offs. Countless varieties of storage technologies exist, and it‚Äôs easy to be \\noverwhelmed when deciding the best option for your data architecture. \\nChapter 6 covers storage best practices and approaches in greater detail, as well as the \\ncrossover between storage and other lifecycle stages.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 50, 'page_label': '51'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 43 \\nIngestion \\nAfter you understand the data source, the characteristics of the source system you‚Äôre \\nusing, and how data is stored, you need to gather the data. The next stage of the data \\nengineering lifecycle is data ingestion from source systems. \\nIn our experience, source systems and ingestion represent the most significant \\nbottlenecks of the data engineering lifecycle. The source systems are normally outside \\nyour direct control and might randomly become unresponsive or provide data of poor \\nquality. Or, your data ingestion service might mysteriously stop working for many \\nreasons. As a result, data flow stops or delivers insufficient data for storage, processing, \\nand serving. \\nUnreliable source and ingestion systems have a ripple effect across the data \\nengineering lifecycle. But you‚Äôre in good shape, assuming you‚Äôve answered the big \\nquestions about source systems. \\nKey engineering considerations for the ingestion phase \\nWhen preparing to architect or build a system, here are some primary questions about \\nthe ingestion stage: \\n‚Ä¢ What are the use cases for the data I‚Äôm ingesting? Can I reuse this data rather than \\ncreate multiple versions of the same dataset? \\n‚Ä¢ Are the systems generating and ingesting this data reliably, and is the data \\navailable when I need it? \\n‚Ä¢ What is the data destination after ingestion? \\n‚Ä¢ How frequently will I need to access the data? \\n‚Ä¢ In what volume will the data typically arrive? \\n‚Ä¢ What format is the data in? Can my downstream storage and transformation \\nsystems handle this format? \\n‚Ä¢ Is the source data in good shape for immediate downstream use? If so, for how \\nlong, and what may cause it to be unusable? \\n‚Ä¢ If the data is from a streaming source, does it need to be transformed before \\nreaching its destination? Would an in-flight transformation be appropriate, where \\nthe data is transformed within the stream itself? \\nThese are just a sample of the factors you‚Äôll need to think about with ingestion, and we \\ncover those questions and more in Chapter 7. Before we leave, let‚Äôs briefly turn our \\nattention to two major data ingestion concepts: batch versus streaming and push versus \\npull.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 51, 'page_label': '52'}, page_content='44  | Chapter 2: The Data Engineering Lifecycle \\nBatch versus streaming \\nVirtually all data we deal with is inherently streaming. Data is nearly always produced \\nand updated continually at its source. Batch ingestion  is simply a specialized and \\nconvenient way of processing this stream in large chunks ‚Äîfor example, handling a \\nfull day‚Äôs worth of data in a single batch. \\nStreaming ingestion allows us to provide data to downstream systems‚Äîwhether other \\napplications, databases, or analytics systems‚Äîin a continuous, real-time fashion. Here, \\nreal-time (or near real-time) means that the data is available to a downstream system \\na short time after it is produced (e.g., less than one second later). The latency required \\nto qualify as real-time varies by domain and requirements. \\nBatch data is ingested either on a predetermined time interval or as data reaches a preset \\nsize threshold. Batch ingestion is a one-way door: once data is broken into batches, the \\nlatency for downstream consumers is inherently constrained. Because of limitations of \\nlegacy systems, batch was for a long time the default way to ingest data. Batch \\nprocessing remains an extremely popular way to ingest data for downstream \\nconsumption, particularly in analytics and ML. \\nHowever, the separation of storage and compute in many systems and the ubiquity of \\nevent-streaming and processing platforms make the continuous processing of data \\nstreams much more accessible and increasingly popular. The choice largely depends \\non the use case and expectations for data timeliness. \\nKey considerations for batch versus stream ingestion \\nShould you go streaming-first? Despite the attractiveness of a streaming-first approach, \\nthere are many trade -offs to understand and think about. The following are some \\nquestions to ask yourself when determining whether streaming ingestion is an \\nappropriate choice over batch ingestion: \\n‚Ä¢ If I ingest the data in real time, can downstream storage systems handle the rate of \\ndata flow? \\n‚Ä¢ Do I need millisecond real-time data ingestion? Or would a micro-batch approach \\nwork, accumulating and ingesting data, say, every minute? \\n‚Ä¢ What are my use cases for streaming ingestion? What specific benefits do I realize \\nby implementing streaming? If I get data in real time, what actions can I take on \\nthat data that would be an improvement upon batch? \\n‚Ä¢ Will my streaming-first approach cost more in terms of time, money, maintenance, \\ndowntime, and opportunity cost than simply doing batch? \\n‚Ä¢ Are my streaming pipeline and system reliable and redundant if infrastructure \\nfails?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 52, 'page_label': '53'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 45 \\n‚Ä¢ What tools are most appropriate for the use case? Should I use a managed service \\n(Amazon Kinesis, Google Cloud Pub/Sub, Google Cloud Dataflow) or stand up \\nmy own instances of Kafka, Flink, Spark, Pulsar, etc.? If I do the latter, who will \\nmanage it? What are the costs and trade-offs? \\n‚Ä¢ If I‚Äôm deploying an ML model, what benefits do I have with online predictions \\nand possibly continuous training? \\n‚Ä¢ Am I getting data from a live production instance? If so, what‚Äôs the impact of my \\ningestion process on this source system? \\nAs you can see, streaming -first might seem like a good idea, but it‚Äôs not always \\nstraightforward; extra costs and complexities inherently occur. Many great ingestion \\nframeworks do handle both batch and micro-batch ingestion styles. We think batch is \\nan excellent approach for many common use cases, such as model training and weekly \\nreporting. Adopt true real-time streaming only after identifying a business use case that \\njustifies the trade-offs against using batch. \\nPush versus pull \\nIn the push model of data ingestion, a source system writes data out to a target, whether \\na database, object store, or filesystem. In the pull model, data is retrieved from the \\nsource system. The line between the push and pull paradigms can be quite blurry; data \\nis often pushed and pulled as it works its way through the various stages of a data \\npipeline. \\nConsider, for example, the extract, transform, load (ETL) process, commonly used in \\nbatch-oriented ingestion workflows. ETL‚Äôs extract (E) part clarifies that we‚Äôre dealing \\nwith a pull ingestion model. In traditional ETL, the ingestion system queries a current \\nsource table snapshot on a fixed schedule. You‚Äôll learn more about ETL and extract, \\nload, transform (ELT) throughout this book. \\nIn another example, consider continuous CDC, which is achieved in a few ways. One \\ncommon method triggers a message every time a row is changed in the source database. \\nThis message is pushed to a queue, where the ingestion system picks it up. Another \\ncommon CDC method uses binary logs, which record every commit to the database. \\nThe database pushes to its logs. The ingestion system reads the logs but doesn‚Äôt directly \\ninteract with the database otherwise. This adds little to no additional load to the source \\ndatabase. Some versions of batch CDC use the pull pattern. For example, in timestamp-\\nbased CDC, an ingestion system queries the source database and pulls the rows that \\nhave changed since the previous update. \\nWith streaming ingestion, data bypasses a backend database and is pushed directly to \\nan endpoint, typically with data buffered by an event-streaming platform. This pattern'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 53, 'page_label': '54'}, page_content='46  | Chapter 2: The Data Engineering Lifecycle \\nis useful with fleets of IoT sensors emitting sensor data. Rather than relying on a \\ndatabase to maintain the current state, we simply think of each recorded reading as an \\nevent. This pattern is also growing in popularity in software applications as it simplifies \\nreal-time processing, allows app developers to tailor their messages for downstream \\nanalytics, and greatly simplifies the lives of data engineers. \\nWe discuss ingestion best practices and techniques in depth in Chapter 7. Next, let‚Äôs \\nturn to the transformation stage of the data engineering lifecycle. \\nTransformation \\nAfter you‚Äôve ingested and stored data, you need to do something with it. The next stage \\nof the data engineering lifecycle is transformation, meaning data needs to be changed \\nfrom its original form into something useful for downstream use cases. Without proper \\ntransformations, data will sit inert, and not be in a useful form for reports, analysis, or \\nML. Typically, the transformation stage is where data begins to create value for \\ndownstream user consumption. \\nImmediately after ingestion, basic transformations map data into correct types \\n(changing ingested string data into numeric and date types, for example), putting \\nrecords into standard formats, and removing bad ones. Later stages of transformation \\nmay transform the data schema and apply normalization. Downstream, we can apply \\nlarge-scale aggregation for reporting or featurize data for ML processes. \\nKey considerations for the transformation phase \\nWhen considering data transformations within the data engineering lifecycle, it helps \\nto consider the following: \\n‚Ä¢ What‚Äôs the cost and return on investment (ROI) of the transformation? What is the \\nassociated business value? \\n‚Ä¢ Is the transformation as simple and self-isolated as possible? \\n‚Ä¢ What business rules do the transformations support? \\nYou can transform data in batch or while streaming in flight. As mentioned in \\n‚ÄúIngestion‚Äù on page 39, virtually all data starts life as a continuous stream; batch is just \\na specialized way of processing a data stream. Batch transformations are \\noverwhelmingly popular, but given the growing popularity of stream -processing \\nsolutions and the general increase  in the amount of streaming data, we expect the \\npopularity of streaming transformations to continue growing, perhaps entirely \\nreplacing batch processing in certain domains soon. \\nLogically, we treat transformation as a standalone area of the data engineering \\nlifecycle, but the realities of the lifecycle can be much more complicated in practice.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 54, 'page_label': '55'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 47 \\nTransformation is often entangled in other phases of the lifecycle. Typically, data is \\ntransformed in source systems or in flight during ingestion. For example, a source \\nsystem may add an event timestamp to a record before forwarding it to an ingestion \\nprocess. Or a record within a streaming pipeline may be ‚Äúenriched‚Äù with additional \\nfields and calculations before it‚Äôs sent to a data warehouse. Transformations are \\nubiquitous in various parts of the lifecycle. Data preparation, data wrangling, and \\ncleaning‚Äîthese transformative tasks add value for end consumers of data. \\nBusiness logic is a major driver of data transformation, often in data modeling. Data \\ntranslates business logic into reusable elements (e.g., a sale means ‚Äúsomebody bought \\n12 picture frames from me for $30 each, or $360 in total‚Äù). In this case, somebody \\nbought 12 picture frames for $30 each. Data modeling is critical for obtaining a clear \\nand current picture of business processes. A simple view of raw retail transactions \\nmight not be useful without adding the logic of accounting rules so that the CFO has a \\nclear picture of financial health. Ensure a standard approach for implementing business \\nlogic across your transformations. \\nData featurization for ML is another data transformation process. Featurization intends \\nto extract and enhance data features useful for training ML models. Featurization can \\nbe a dark art, combining domain expertise (to identify which features might be \\nimportant for prediction) with extensive experience in data science. For this book, the \\nmain point is that once data scientists determine how to featurize data, featurization \\nprocesses can be automated by data engineers in the transformation stage of a data \\npipeline. \\nTransformation is a profound subject, and we cannot do it justice in this brief \\nintroduction. Chapter 8 delves into queries, data modeling, and various transformation \\npractices and nuances. \\nServing Data \\nYou‚Äôve reached the last stage of the data engineering lifecycle. Now that the data has \\nbeen ingested, stored, and transformed into coherent and useful structures, it‚Äôs time to \\nget value from your data. ‚ÄúGetting value‚Äù from data means different things to different \\nusers. \\nData has value when it‚Äôs used for practical purposes. Data that is not consumed or \\nqueried is simply inert. Data vanity projects are a major risk for companies. Many \\ncompanies pursued vanity projects in the big data era, gathering massive datasets in \\ndata lakes that were never consumed in any useful way. The cloud era is triggering a \\nnew wave of vanity projects built on the latest data warehouses, object storage systems, \\nand streaming technologies. Data projects must be intentional across the lifecycle. \\nWhat is the ultimate business purpose of the data so carefully collected, cleaned, and \\nstored?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 55, 'page_label': '56'}, page_content='48  | Chapter 2: The Data Engineering Lifecycle \\nData serving is perhaps the most exciting part of the data engineering lifecycle. This is \\nwhere the magic happens. This is where ML engineers can apply the most advanced \\ntechniques. Let‚Äôs look at some of the popular uses of data: analytics, ML, and reverse \\nETL. \\nAnalytics \\nAnalytics is the core of most data endeavors. Once your data is stored and transformed, \\nyou‚Äôre ready to generate reports or dashboards and do ad hoc analysis on the data. \\nWhereas the bulk of analytics used to encompass BI, it now includes other facets such \\nas operational analytics and embedded analytics ( Figure 2-5). Let‚Äôs briefly touch on \\nthese variations of analytics. \\n \\nFigure 2-5. Types of analytics \\nBusiness intelligence.    BI marshals collected data to describe a business‚Äôs past and \\ncurrent state. BI requires using business logic to process raw data. Note that data \\nserving for analytics is yet another area where the stages of the data engineering \\nlifecycle can get tangled. As we mentioned earlier, business logic is often applied to \\ndata in the transformation stage of the data engineering lifecycle, but a logic -on-read \\napproach has become increasingly popular. Data is stored in a clean but fairly raw \\nform, with minimal postprocessing business logic. A BI system maintains a repository \\nof business logic and definitions. This business logic is used to query the data \\nwarehouse so that reports and dashboards align with business definitions. \\nAs a company grows its data maturity, it will move from ad hoc data analysis to self -\\nservice analytics, allowing democratized data access to business users without needing \\nIT to intervene. The capability to do self -service analytics assumes that data is go od \\nenough that people across the organization can simply access it themselves, slice and \\ndice it however they choose, and get immediate insights. Although self -service \\nanalytics is simple in theory, it‚Äôs tough to pull off in practice. The main reason is that \\npoor data quality, organizational silos, and a lack of adequate data skills often get in \\nthe way of allowing widespread use of analytics.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 56, 'page_label': '57'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 49 \\nOperational analytics.    Operational analytics focuses on the fine-grained details of \\noperations, promoting actions that a user of the reports can act upon immediately. \\nOperational analytics could be a live view of inventory or real -time dashboarding of \\nwebsite or application health. In this case, data is consumed in real time, either directly \\nfrom a source system or from a streaming data pipeline. The types of insights in \\noperational analytics differ from traditional BI since operational analytics is focu sed \\non the present and doesn‚Äôt necessarily concern historical trends. \\nEmbedded analytics.    You may wonder why we‚Äôve broken out embedded analytics \\n(customer-facing analytics) separately from BI. In practice, analytics provided to \\ncustomers on a SaaS platform come with a separate set of requirements and \\ncomplications. Internal BI faces a limited a udience and generally presents a limited \\nnumber of unified views. Access controls are critical but not particularly complicated. \\nAccess is managed using a handful of roles and access tiers. \\nWith embedded analytics, the request rate for reports, and the corresponding burden \\non analytics systems, goes up dramatically; access control is significantly more \\ncomplicated and critical. Businesses may be serving separate analytics and data to \\nthousands or more customers. Each customer must see their data and only their data. \\nAn internal data-access error at a company would likely lead to a procedural review. A \\ndata leak between customers would be considered a massive breach of trust, leading to \\nmedia attention and a significant loss of customers. Minimize your blast radius related \\nto data leaks and security vulnerabilities. Apply tenant - or data-level security within \\nyour storage and anywhere there‚Äôs a possibility of data leakage. \\nMultitenancy \\nMany current storage and analytics systems support multitenancy in various ways. Data \\nengineers may choose to house data for many customers in common tables to allow a \\nunified view for internal analytics and ML. This data is presented externally to individual \\ncustomers through logical views with appropriately defined controls and filters. It is \\nincumbent on data engineers to understand the minutiae of multitenancy in the systems \\nthey deploy to ensure absolute data security and isolation. \\nMachine learning \\nThe emergence and success of ML is one of the most exciting technology revolutions. \\nOnce organizations reach a high level of data maturity, they can begin to identify \\nproblems amenable to ML and start organizing a practice around it.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 57, 'page_label': '58'}, page_content='50  | Chapter 2: The Data Engineering Lifecycle \\nThe responsibilities of data engineers overlap significantly in analytics and ML, and \\nthe boundaries between data engineering, ML engineering, and analytics engineering \\ncan be fuzzy. For example, a data engineer may need to support Spark clusters that \\nfacilitate analytics pipelines and ML model training. They may also need to provide a \\nsystem that orchestrates tasks across teams and support metadata and cataloging \\nsystems that track data history and lineage. Setting these domains of responsibility and \\nthe relevant reporting structures is a critical organizational decision. \\nThe feature store is a recently developed tool that combines data engineering and ML \\nengineering. Feature stores are designed to reduce the operational burden for ML \\nengineers by maintaining feature history and versions, supporting feature sharing \\namong teams, and providing basic operational and orchestration capabilities, such as \\nbackfilling. In practice, data engineers are part of the core support team for feature \\nstores to support ML engineering. \\nShould a data engineer be familiar with ML? It certainly helps. Regardless of the \\noperational boundary between data engineering, ML engineering, business analytics, \\nand so forth, data engineers should maintain operational knowledge about their teams. \\nA good data engineer is conversant in the fundamental ML techniques and related data-\\nprocessing requirements, the use cases for models within their company, and the \\nresponsibilities of the organization‚Äôs various analytics teams. This helps maintain \\nefficient communication and facilitate collaboration. Ideally, data engineers will build \\ntools in partnership with other teams that neither team can make independently. \\nThis book cannot possibly cover ML in depth. A growing ecosystem of books, videos, \\narticles, and communities is available if you‚Äôre interested in learning more; we include \\na few suggestions in ‚ÄúAdditional Resources‚Äù on page 69. \\nThe following are some considerations for the serving data phase specific to ML: \\n‚Ä¢ Is the data of sufficient quality to perform reliable feature engineering? Quality \\nrequirements and assessments are developed in close collaboration with teams \\nconsuming the data. \\n‚Ä¢ Is the data discoverable? Can data scientists and ML engineers easily find valuable \\ndata? \\n‚Ä¢ Where are the technical and organizational boundaries between data engineering \\nand ML engineering? This organizational question has significant architectural \\nimplications. \\n‚Ä¢ Does the dataset properly represent ground truth? Is it unfairly biased? \\nWhile ML is exciting, our experience is that companies often prematurely dive into it. \\nBefore investing a ton of resources into ML, take the time to build a solid data'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 58, 'page_label': '59'}, page_content='What Is the Data Engineering Lifecycle?  | \\n 51 \\nfoundation. This means setting up the best systems and architecture across the data \\nengineering and ML lifecycle. It‚Äôs generally best to develop competence in analytics \\nbefore moving to ML. Many companies have dashed their ML dreams because they \\nundertook initiatives without appropriate foundations. \\nReverse ETL \\nReverse ETL has long been a practical reality in data, viewed as an antipattern that we \\ndidn‚Äôt like to talk about or dignify with a name. Reverse ETL takes processed data from \\nthe output side of the data engineering lifecycle and feeds it back into source systems, \\nas shown in Figure 2-6. In reality, this flow is beneficial and often necessary; reverse \\nETL allows us to take analytics, scored models, etc., and feed these back into \\nproduction systems or SaaS platforms. \\n \\nFigure 2-6. Reverse ETL \\nMarketing analysts might calculate bids in Microsoft Excel by using the data in their \\ndata warehouse, and then upload these bids to Google Ads. This process was often \\nentirely manual and primitive. \\nAs we‚Äôve written this book, several vendors have embraced the concept of reverse ETL \\nand built products around it, such as Hightouch and Census. Reverse ETL remains \\nnascent as a practice, but we suspect that it is here to stay. \\nReverse ETL has become especially important as businesses rely increasingly on SaaS \\nand external platforms. For example, companies may want to push specific metrics \\nfrom their data warehouse to a customer data platform or CRM system. Advertising \\nplatforms are another everyday use case, as in the Google Ads example. Expect to see \\nmore activity in reverse ETL, with an overlap in both data engineering and ML \\nengineering.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 59, 'page_label': '60'}, page_content='52  | Chapter 2: The Data Engineering Lifecycle \\nThe jury is out on whether the term reverse ETL will stick. And the practice may \\nevolve. Some engineers claim that we can eliminate reverse ETL by handling data \\ntransformations in an event stream and sending those events back to source systems as \\nneeded. Realizing widespread adoption of this pattern ac ross businesses is another \\nmatter. The gist is that transformed data will need to be returned to source systems in \\nsome manner, ideally with the correct lineage and business process associated with the \\nsource system. \\nMajor Undercurrents Across the Data \\nEngineering Lifecycle \\nData engineering is rapidly maturing. Whereas prior cycles of data engineering simply \\nfocused on the technology layer, the continued abstraction and simplification'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 60, 'page_label': '61'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  53 \\nof tools and practices have shifted this focus. Data engineering now encompasses far \\nmore than tools and technology. The field is now moving up the value chain, \\nincorporating traditional enterprise practices such as data management and cost \\noptimization and newer practices like DataOps. \\nWe‚Äôve termed these practices undercurrents‚Äîsecurity, data management, DataOps, \\ndata architecture, orchestration, and software engineering‚Äîthat support every aspect \\nof the data engineering lifecycle (Figure 2-7). In this section, we give a brief overview \\nof these undercurrents and their major components, which you‚Äôll see in more detail \\nthroughout the book. \\n \\nFigure 2-7. The major undercurrents of data engineering \\nSecurity \\nSecurity must be top of mind for data engineers, and those who ignore it do so at their \\nperil. That‚Äôs why security is the first undercurrent. Data engineers must understand \\nboth data and access security, exercising the principle of least privilege. The principle \\nof least privilege means giving a user or system access to only the essential data and \\nresources to perform an intended function. A common antipattern we see with data \\nengineers with little security experience is to give admin access to all users. This is a \\ncatastrophe waiting to happen! \\nGive users only the access they need to do their jobs today, nothing more. Don‚Äôt operate \\nfrom a root shell when you‚Äôre just looking for visible files with standard user access. \\nWhen querying tables with a lesser role, don‚Äôt use the superuser role in a data base. \\nImposing the principle of least privilege on ourselves can prevent accidental damage \\nand keep you in a security-first mindset. \\nPeople and organizational structure are always the biggest security vulnerabilities in \\nany company. When we hear about major security breaches in the media, it often turns \\nout that someone in the company ignored basic precautions, fell victim to a phishing  \\nattack, or otherwise acted irresponsibly. The first line of defense for data security is to \\ncreate a culture of security that permeates the organization. All individuals who have \\naccess to data must understand their responsibility in protecting the compan y‚Äôs \\nsensitive data and its customers.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 61, 'page_label': '62'}, page_content='54  | Chapter 2: The Data Engineering Lifecycle \\nData security is also about timing ‚Äîproviding data access to exactly the people and \\nsystems that need to access it and only for the duration necessary to perform their \\nwork. Data should be protected from unwanted visibility, both in flight and at rest, by \\nusing encryption, tokenization, data masking, obfuscation, and simple, robust access \\ncontrols. \\nData engineers must be competent security administrators, as security falls in their \\ndomain. A data engineer should understand security best practices for the cloud and on \\nprem. Knowledge of user and identity access management (IAM) roles, policies, \\ngroups, network security, password policies, and encryption are good places to start. \\nThroughout the book, we highlight areas where security should be top of mind in the \\ndata engineering lifecycle. You can also gain more detailed insights into security in \\nChapter 10. \\nData Management \\nYou probably think that data management sounds very‚Ä¶corporate. ‚ÄúOld school‚Äù data \\nmanagement practices make their way into data and ML engineering. What‚Äôs old is \\nnew again. Data management has been around for decades but didn‚Äôt get a lot of \\ntraction in data engineering until recently. Data tools are becoming simpler, and there \\nis less complexity for data engineers to manage. As a result, the data engineer moves \\nup the value chain toward the next rung of best practices. Data best practices once \\nreserved for huge companies‚Äîdata governance, master data management, dataquality \\nmanagement, metadata management‚Äîare now filtering down to companies of all sizes \\nand maturity levels. As we like to say, data engineering is becoming ‚Äúenterprisey.‚Äù This \\nis ultimately a great thing! \\nThe Data Management Association International (DAMA) Data Management Body of \\nKnowledge (DMBOK), which we consider to be the definitive book for enterprise data \\nmanagement, offers this definition: \\nData management is the development, execution, and supervision of plans, policies, \\nprograms, and practices that deliver, control, protect, and enhance the value of data \\nand information assets throughout their lifecycle. \\nThat‚Äôs a bit lengthy, so let‚Äôs look at how it ties to data engineering. Data engineers \\nmanage the data lifecycle, and data management encompasses the set of best practices \\nthat data engineers will use to accomplish this task, both technically and strategically. \\nWithout a framework for managing data, data engineers are simply technicians \\noperating in a vacuum. Data engineers need a broader perspective of data‚Äôs utility \\nacross the organization, from the source systems to the C -suite, and everywhere in \\nbetween.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 62, 'page_label': '63'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  55 \\nWhy is data management important? Data management demonstrates that data is vital \\nto daily operations, just as businesses view financial resources, finished goods, or real \\nestate as assets. Data management practices form a cohesive framework that everyone \\ncan adopt to ensure that the organization gets value from data and handles it \\nappropriately. \\nData management has quite a few facets, including the following: \\n‚Ä¢ Data governance, including discoverability and accountability \\n‚Ä¢ Data modeling and design \\n‚Ä¢ Data lineage \\n‚Ä¢ Storage and operations \\n‚Ä¢ Data integration and interoperability \\n‚Ä¢ Data lifecycle management \\n‚Ä¢ Data systems for advanced analytics and ML \\n‚Ä¢ Ethics and privacy \\nWhile this book is in no way an exhaustive resource on data management, let‚Äôs briefly \\ncover some salient points from each area as they relate to data engineering. \\nData governance \\nAccording to Data Governance: The Definitive Guide, ‚ÄúData governance is, first and \\nforemost, a data management function to ensure the quality, integrity, security, and \\nusability of the data collected by an organization.‚Äù13 \\nWe can expand on that definition and say that data governance engages people, \\nprocesses, and technologies to maximize data value across an organization while \\nprotecting data with appropriate security controls. Effective data governance is \\ndeveloped with intention and supported by the organization. When data governance is \\naccidental and haphazard, the side effects can range from untrusted data to security \\nbreaches and everything in between. Being intentional about data governance will \\nmaximize the organization‚Äôs data capabilities and the value generated from data. It will \\nalso (hopefully) keep a company out of the headlines for questionable or downright \\nreckless data practices. \\nThink of the typical example of data governance being done poorly. A business analyst \\ngets a request for a report but doesn‚Äôt know what data to use to answer the question. \\nThey may spend hours digging through dozens of tables in a transactional database, \\n \\n13 Evren Eryurek et al., Data Governance: The Definitive Guide (Sebastopol, CA: O‚ÄôReilly, 2021), 1, \\nhttps://oreil.ly/LFT4d.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 63, 'page_label': '64'}, page_content='56  | Chapter 2: The Data Engineering Lifecycle \\nwildly guessing at which fields might be useful. The analyst compiles a ‚Äúdirectionally \\ncorrect‚Äù report but isn‚Äôt entirely sure that the report‚Äôs underlying data is accurate or \\nsound. The recipient of the report also questions the validity of the data. The integrity \\nof the analyst‚Äîand of all data in the company‚Äôs systems‚Äîis called into question. The \\ncompany is confused about its performance, making business planning impossible. \\nData governance is a foundation for data -driven business practices and a \\nmissioncritical part of the data engineering lifecycle. When data governance is \\npracticed well, people, processes, and technologies align to treat data as a key business \\ndriver; if data issues occur, they are promptly handled. \\nThe core categories of data governance are discoverability, security, and \\naccountability.14 Within these core categories are subcategories, such as data quality, \\nmetadata, and privacy. Let‚Äôs look at each core category in turn. \\nDiscoverability.    In a data-driven company, data must be available and discoverable. \\nEnd users should have quick and reliable access to the data they need to do their jobs. \\nThey should know where the data comes from, how it relates to other data, and what \\nthe data means. \\nSome key areas of data discoverability include metadata management and master data \\nmanagement. Let‚Äôs briefly describe these areas. \\nMetadata.    Metadata is ‚Äúdata about data,‚Äù and it underpins every section of the data \\nengineering lifecycle. Metadata is exactly the data needed to make data discoverable \\nand governable. \\nWe divide metadata into two major categories: autogenerated and human generated. \\nModern data engineering revolves around automation, but metadata collection is often \\nmanual and error prone. \\nTechnology can assist with this process, removing much of the error -prone work of \\nmanual metadata collection. We‚Äôre seeing a proliferation of data catalogs, data-lineage \\ntracking systems, and metadata management tools. Tools can crawl databases to look \\nfor relationships and monitor data pipelines to track where data comes from and where \\nit goes. A low -fidelity manual approach uses an internally led effort where various \\nstakeholders crowdsource metadata collection within the organization. These data \\nmanagement tools are covered in depth throughout the book, as they undercut much of \\nthe data engineering lifecycle. \\nMetadata becomes a byproduct of data and data processes. However, key challenges \\nremain. In particular, interoperability and standards are still lacking. Metadata tools are \\n \\n14 Eryurek, Data Governance, 5.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 64, 'page_label': '65'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  57 \\nonly as good as their connectors to data systems and their ability to share metadata. In \\naddition, automated metadata tools should not entirely take humans out of the loop. \\nData has a social element; each organization accumulates social capital and knowledge \\naround processes, datasets, and pipelines. Human-oriented metadata systems focus on \\nthe social aspect of metadata. This is something that Airbnb has emphasized in its \\nvarious blog posts on data tools, particularly its original Dataportal concept. 15 Such \\ntools should provide a place to disclose data owners, data consumers, and domain \\nexperts. Documentation and internal wiki tools provide a key foundation for metadata \\nmanagement, but these tools should also integrate with automated data cataloging. For \\nexample, data -scanning tools can generate wiki pages with links to relevant data \\nobjects. \\nOnce metadata systems and processes exist, data engineers can consume metadata in \\nuseful ways. Metadata becomes a foundation for designing pipelines and managing \\ndata throughout the lifecycle. \\nDMBOK identifies four main categories of metadata that are useful to data engineers: \\n‚Ä¢ Business metadata \\n‚Ä¢ Technical metadata \\n‚Ä¢ Operational metadata \\n‚Ä¢ Reference metadata \\nLet‚Äôs briefly describe each category of metadata. \\nBusiness metadata relates to the way data is used in the business, including business \\nand data definitions, data rules and logic, how and where data is used, and the data \\nowner(s). \\nA data engineer uses business metadata to answer nontechnical questions about who, \\nwhat, where, and how. For example, a data engineer may be tasked with creating a data \\npipeline for customer sales analysis. But what is a customer? Is it someone who‚Äôs \\npurchased in the last 90 days? Or someone who‚Äôs purchased at any time the business \\nhas been open? A data engineer would use the correct data to refer to business metadata \\n(data dictionary or data catalog) to look up how a ‚Äúcustomer‚Äù is defined. Business \\nmetadata provides a data engineer with the right context and definitions to properly \\nuse data. \\nTechnical metadata describes the data created and used by systems across the data \\nengineering lifecycle. It includes the data model and schema, data lineage, field \\n \\n15 Chris Williams et al., ‚ÄúDemocratizing Data at Airbnb,‚Äù The Airbnb Tech Blog, May 12, 2017, \\nhttps://oreil.ly/dM332.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 65, 'page_label': '66'}, page_content='58  | Chapter 2: The Data Engineering Lifecycle \\nmappings, and pipeline workflows. A data engineer uses technical metadata to create, \\nconnect, and monitor various systems across the data engineering lifecycle. \\nHere are some common types of technical metadata that a data engineer will use: \\n‚Ä¢ Pipeline metadata (often produced in orchestration systems) \\n‚Ä¢ Data lineage \\n‚Ä¢ Schema \\nOrchestration is a central hub that coordinates workflow across various systems. \\nPipeline metadata captured in orchestration systems provides details of the workflow \\nschedule, system and data dependencies, configurations, connection details, and much \\nmore. \\nData-lineage metadata tracks the origin and changes to data, and its dependencies, \\nover time. As data flows through the data engineering lifecycle, it evolves through \\ntransformations and combinations with other data. Data lineage provides an audit trail \\nof data‚Äôs evolution as it moves through various systems and workflows. \\nSchema metadata describes the structure of data stored in a system such as a database, \\na data warehouse, a data lake, or a filesystem; it is one of the key differentiators across \\ndifferent storage systems. Object stores, for example, don‚Äôt manage schema metadata; \\ninstead, this must be managed in a metastore. On the other hand, cloud data warehouses \\nmanage schema metadata internally. \\nThese are just a few examples of technical metadata that a data engineer should know \\nabout. This is not a complete list, and we cover additional aspects of technical metadata \\nthroughout the book. \\nOperational metadata describes the operational results of various systems and includes \\nstatistics about processes, job IDs, application runtime logs, data used in a process, and \\nerror logs. A data engineer uses operational metadata to determine whether a process \\nsucceeded or failed and the data involved in the process. \\nOrchestration systems can provide a limited picture of operational metadata, but the \\nlatter still tends to be scattered across many systems. A need for better -quality \\noperational metadata, and better metadata management, is a major motivation for next-\\ngeneration orchestration and metadata management systems. \\nReference metadata is data used to classify other data. This is also referred to as lookup \\ndata. Standard examples of reference data are internal codes, geographic codes, units \\nof measurement, and internal calendar standards. Note that much of reference data is \\nfully managed internally, but items such as geographic codes might come from'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 66, 'page_label': '67'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  59 \\nstandard external references. Reference data is essentially a standard for interpreting \\nother data, so if it changes, this change happens slowly over time. \\nData accountability.    Data accountability means assigning an individual to govern \\na portion of data. The responsible person then coordinates the governance activities of \\nother stakeholders. Managing data quality is tough if no one is accountable for the data \\nin question. \\nNote that people accountable for data need not be data engineers. The accountable \\nperson might be a software engineer or product manager, or serve in another role. In \\naddition, the responsible person generally doesn‚Äôt have all the resources necessary to \\nmaintain data quality. Instead, they coordinate with all people who touch the data, \\nincluding data engineers. \\nData accountability can happen at various levels; accountability can happen at the level \\nof a table or a log stream but could be as fine-grained as a single field entity that occurs \\nacross many tables. An individual may be accountable for managing a custom er ID \\nacross many systems. For enterprise data management, a data domain is the set of all \\npossible values that can occur for a given field type, such as in this ID example. This \\nmay seem excessively bureaucratic and meticulous, but it can significantly affect data \\nquality. \\nData quality.    \\nCan I trust this data? \\n‚ÄîEveryone in the business \\nData quality is the optimization of data toward the desired state and orbits the question, \\n‚ÄúWhat do you get compared with what you expect?‚Äù Data should conform to the \\nexpectations in the business metadata. Does the data match the definition agreed upon \\nby the business? \\nA data engineer ensures data quality across the entire data engineering lifecycle. This \\ninvolves performing data -quality tests, and ensuring data conformance to schema \\nexpectations, data completeness, and precision. \\nAccording to Data Governance: The Definitive Guide, data quality is defined by three \\nmain characteristics:16 \\nAccuracy \\nIs the collected data factually correct? Are there duplicate values? Are the numeric \\nvalues accurate? \\n \\n16 Eryurek, Data Governance, 113.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 67, 'page_label': '68'}, page_content='60  | Chapter 2: The Data Engineering Lifecycle \\nCompleteness \\nAre the records complete? Do all required fields contain valid values? \\nTimeliness \\nAre records available in a timely fashion? \\nEach of these characteristics is quite nuanced. For example, how do we think about \\nbots and web scrapers when dealing with web event data? If we intend to analyze the \\ncustomer journey, we must have a process that lets us separate humans from machine-\\ngenerated traffic. Any bot -generated events misclassified as human present data \\naccuracy issues, and vice versa. \\nA variety of interesting problems arise concerning completeness and timeliness. In the \\nGoogle paper introducing the Dataflow model, the authors give the example of an \\noffline video platform that displays ads.17 The platform downloads video and ads while \\na connection is present, allows the user to watch these while offline, and then uploads \\nad view data once a connection is present again. This data may arrive late, well after \\nthe ads are watched. How does the platform handle billing for the ads? \\nFundamentally, this problem can‚Äôt be solved by purely technical means. Rather, \\nengineers will need to determine their standards for late-arriving data and enforce these \\nuniformly, possibly with the help of various technology tools. \\nMaster Data Management \\nMaster data is data about business entities such as employees, customers, products, and \\nlocations. As organizations grow larger and more complex through organic growth and \\nacquisitions, and collaborate with other businesses, maintaining a consistent picture of \\nentities and identities becomes more and more challenging. \\nMaster data management  (MDM) is the practice of building consistent entity \\ndefinitions known as golden records. Golden records harmonize entity data across an \\norganization and with its partners. MDM is a business operations process facilitated by \\nbuilding and deploying technology tools. For example, an MDM team might determine \\na standard format for addresses, and then work with data engineers to build an API to \\nreturn consistent addresses and a system that uses address data to match customer \\nrecords across company divisions. \\nMDM reaches across the full data cycle into operational databases. It may fall directly \\nunder the purview of data engineering but is often the assigned responsibility of a \\n \\n17 Tyler Akidau et al., ‚ÄúThe Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost \\nin Massive-Scale, Unbounded, Out-of-Order Data Processing,‚Äù Proceedings of the VLDB Endowment 8 \\n(2015): 1792‚Äì1803, https://oreil.ly/Z6XYy.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 68, 'page_label': '69'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  61 \\ndedicated team that works across the organization. Even if they don‚Äôt own MDM, data \\nengineers must always be aware of it, as they might collaborate on MDM initiatives.  \\nData quality sits across the boundary of human and technology problems. Data \\nengineers need robust processes to collect actionable human feedback on data quality \\nand use technology tools to detect quality issues preemptively before downstream \\nusers ever see them. We cover these collection processes in the appropriate chapters \\nthroughout this book. \\nData modeling and design \\nTo derive business insights from data, through business analytics and data science, the \\ndata must be in a usable form. The process for converting data into a usable form is \\nknown as data modeling and design. Whereas we traditionally think of data modeling \\nas a problem for database administrators (DBAs) and ETL developers, data modeling \\ncan happen almost anywhere in an organization. Firmware engineers develop the data \\nformat of a record for an IoT device, or web application developers design the JSON \\nresponse to an API call or a MySQL table schema ‚Äîthese are all instances of data \\nmodeling and design. \\nData modeling has become more challenging because of the variety of new data \\nsources and use cases. For instance, strict normalization doesn‚Äôt work well with event \\ndata. Fortunately, a new generation of data tools increases the flexibility of data \\nmodels, while retaining logical separations of measures, dimensions, attributes, and \\nhierarchies. Cloud data warehouses support the ingestion of enormous quantities of \\ndenormalized and semistructured data, while still supporting common data modeling \\npatterns, such as Kimball, Inmon, and Data Vault. Data processing frameworks such \\nas Spark can ingest a whole spectrum of data, from flat structured relational records to \\nraw unstructured text. We discuss these data modeling and transformation patterns in \\ngreater detail in Chapter 8. \\nWith the wide variety of data that engineers must cope with, there is a temptation to \\nthrow up our hands and give up on data modeling. This is a terrible idea with harrowing \\nconsequences, made evident when people murmur of the write once, read never \\n(WORN) access pattern or refer to a data swamp. Data engineers need to understand \\nmodeling best practices as well as develop the flexibility to apply the appropriate level \\nand type of modeling to the data source and use case. \\nData lineage \\nAs data moves through its lifecycle, how do you know what system affected the data \\nor what the data is composed of as it gets passed around and transformed? Data lineage \\ndescribes the recording of an audit trail of data through its lifecycle, tracking both the \\nsystems that process the data and the upstream data it depends on.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 69, 'page_label': '70'}, page_content='62  | Chapter 2: The Data Engineering Lifecycle \\nData lineage helps with error tracking, accountability, and debugging of data and the \\nsystems that process it. It has the obvious benefit of giving an audit trail for the data \\nlifecycle and helps with compliance. For example, if a user would like their dat a \\ndeleted from your systems, having lineage for that data lets you know where that data \\nis stored and its dependencies. \\nData lineage has been around for a long time in larger companies with strict compliance \\nstandards. However, it‚Äôs now being more widely adopted in smaller companies as data \\nmanagement becomes mainstream. We also note that Andy Petrella‚Äôs concept of Data \\nObservability Driven Development (DODD) is closely related to data lineage. DODD \\nobserves data all along its lineage. This process is applied during development, testing, \\nand finally production to deliver quality and conformity to expectations. \\nData integration and interoperability \\nData integration and interoperability is the process of integrating data across tools and \\nprocesses. As we move away from a single -stack approach to analytics and toward a \\nheterogeneous cloud environment in which various tools process data on demand, \\nintegration and interoperability occupy an ever-widening swath of the data engineer‚Äôs \\njob. \\nIncreasingly, integration happens through general -purpose APIs rather than custom \\ndatabase connections. For example, a data pipeline might pull data from the Salesforce \\nAPI, store it to Amazon S3, call the Snowflake API to load it into a table, call the API \\nagain to run a query, and then export the results to S3 where Spark can consume them. \\nAll of this activity can be managed with relatively simple Python code that talks to data \\nsystems rather than handling data directly. While the complexity of interacting with \\ndata systems has decreased, the number of systems and the complexity of pipelines has \\ndramatically increased. Engineers starting from scratch quickly outgrow the \\ncapabilities of bespoke scripting and stumble into the need for orchestration. \\nOrchestration is one of our undercurrents, and we discuss it in detail in ‚ÄúOrchestration‚Äù \\non page 64. \\nData lifecycle management \\nThe advent of data lakes encouraged organizations to ignore data archival and \\ndestruction. Why discard data when you can simply add more storage ad infinitum? \\nTwo changes have encouraged engineers to pay more attention to what happens at the \\nend of the data engineering lifecycle. \\nFirst, data is increasingly stored in the cloud. This means we have pay -as-you-go \\nstorage costs instead of large up -front capital expenditures for an on -premises data \\nlake. When every byte shows up on a monthly AWS statement, CFOs see opportunities'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 70, 'page_label': '71'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  63 \\nfor savings. Cloud environments make data archival a relatively straightforward \\nprocess. Major cloud vendors offer archival-specific object storage classes that allow \\nlong-term data retention at an extremely low cost, assuming very infrequent access (it \\nshould be noted that data retrieval isn‚Äôt so cheap, but that‚Äôs for another conversation). \\nThese storage classes also support extra policy controls to prevent accidental or \\ndeliberate deletion of critical archives. \\nSecond, privacy and data retention laws such as the GDPR and the CCPA require data \\nengineers to actively manage data destruction to respect users‚Äô ‚Äúright to be forgotten.‚Äù \\nData engineers must know what consumer data they retain and must have procedures \\nto destroy data in response to requests and compliance requirements. \\nData destruction is straightforward in a cloud data warehouse. SQL semantics allow \\ndeletion of rows conforming to a where clause. Data destruction was more \\nchallenging in data lakes, where write-once, read-many was the default storage pattern. \\nTools such as Hive ACID and Delta Lake allow easy management of deletion \\ntransactions at scale. New generations of metadata managemen t, data lineage, and \\ncataloging tools will also streamline the end of the data engineering lifecycle. \\nEthics and privacy \\nThe last several years of data breaches, misinformation, and mishandling of data make \\none thing clear: data impacts people. Data used to live in the Wild West, freely \\ncollected and traded like baseball cards. Those days are long gone. Whereas data‚Äôs \\nethical and privacy implications were once considered nice to have, like security, \\nthey‚Äôre now central to the general data lifecycle. Data engineers need to do the right \\nthing when no one else is watching, because everyone will be watching someday.18We \\nhope that more organizations will encourage a culture of good data ethics and privacy. \\nHow do ethics and privacy impact the data engineering lifecycle? Data engineers need \\nto ensure that datasets mask personally identifiable information (PII) and other \\nsensitive information; bias can be identified and tracked in datasets as they are \\ntransformed. Regulatory requirements and compliance penalties are only growing. \\nEnsure that your data assets are compliant with a growing number of data regulations, \\nsuch as GDPR and CCPA. Please take this seriously. We offer tips throughout the book \\nto ensure that you‚Äôre baking ethics and privacy into the data engineering lifecycle. \\n \\n18 We espouse the notion that ethical behavior is doing the right thing when no one is watching, an idea that \\noccurs in the writings of C. S. Lewis, Charles Marshall, and many other authors.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 71, 'page_label': '72'}, page_content='64  | Chapter 2: The Data Engineering Lifecycle \\nDataOps \\nDataOps maps the best practices of Agile methodology, DevOps, and statistical process \\ncontrol (SPC) to data. Whereas DevOps aims to improve the release and quality of \\nsoftware products, DataOps does the same thing for data products. \\nData products differ from software products because of the way data is used. A software \\nproduct provides specific functionality and technical features for end users. By \\ncontrast, a data product is built around sound business logic and metrics, whose users \\nmake decisions or build models that perform automated actions. A data engineer must \\nunderstand both the technical aspects of building software products and the business \\nlogic, quality, and metrics that will create excellent data products. \\nLike DevOps, DataOps borrows much from lean manufacturing and supply chain \\nmanagement, mixing people, processes, and technology to reduce time to value. As \\nData Kitchen (experts in DataOps) describes it:19 \\nDataOps is a collection of technical practices, workflows, cultural norms, and \\narchitectural patterns that enable: \\n‚Ä¢ Rapid innovation and experimentation delivering new insights to customers with \\nincreasing velocity \\n‚Ä¢ Extremely high data quality and very low error rates \\n‚Ä¢ Collaboration across complex arrays of people, technology, and environments \\n‚Ä¢ Clear measurement, monitoring, and transparency of results \\nLean practices (such as lead time reduction and minimizing defects) and the resulting \\nimprovements to quality and productivity are things we are glad to see gaining \\nmomentum both in software and data operations. \\nFirst and foremost, DataOps is a set of cultural habits; the data engineering team needs \\nto adopt a cycle of communicating and collaborating with the business, breaking down \\nsilos, continuously learning from successes and mistakes, and rapid iteration. Onl y \\nwhen these cultural habits are set in place can the team get the best results from \\ntechnology and tools. \\nDepending on a company‚Äôs data maturity, a data engineer has some options to build \\nDataOps into the fabric of the overall data engineering lifecycle. If the company has \\nno preexisting data infrastructure or practices, DataOps is very much a greenfield \\nopportunity that can be baked in from day one. With an existing project or \\ninfrastructure that lacks DataOps, a data engineer can begin adding DataOps into \\nworkflows. We suggest first starting with observability and monitoring to get a window \\n \\n19 ‚ÄúWhat Is DataOps,‚Äù DataKitchen FAQ page, accessed May 5, 2022, https://oreil.ly/Ns06w.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 72, 'page_label': '73'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  65 \\ninto the performance of a system, then adding in automation and incident response. A \\ndata engineer may work alongside an existing DataOps team to improve the data \\nengineering lifecycle in a data-mature company. In all cases, a data engineer must be \\naware of the philosophy and technical aspects of DataOps. \\nDataOps has three core technical elements: automation, monitoring and observability, \\nand incident response ( Figure 2-8). Let‚Äôs look at each of these pieces and how they \\nrelate to the data engineering lifecycle. \\n \\nFigure 2-8. The three pillars of DataOps \\nAutomation \\nAutomation enables reliability and consistency in the DataOps process and allows data \\nengineers to quickly deploy new product features and improvements to existing \\nworkflows. DataOps automation has a similar framework and workflow to DevOps, \\nconsisting of change management (environment, code, and data version control), \\ncontinuous integration/continuous deployment (CI/CD), and configuration as code. \\nLike DevOps, DataOps practices monitor and maintain the reliability of technology \\nand systems (data pipelines, orchestration, etc.), with the added dimension of checking \\nfor data quality, data/model drift, metadata integrity, and more. \\nLet‚Äôs briefly discuss the evolution of DataOps automation within a hypothetical \\norganization. An organization with a low level of DataOps maturity often attempts to \\nschedule multiple stages of data transformation processes using cron jobs. This works \\nwell for a while. As data pipelines become more complicated, several things are likely \\nto happen. If the cron jobs are hosted on a cloud instance, the instance may have an \\noperational problem, causing the jobs to stop running unexpectedly. As the spacing \\nbetween jobs becomes tighter, a job will eventually run long, causing a subsequent job \\nto fail or produce stale data. Engineers may not be aware of job failures until they hear \\nfrom analysts that their reports are out-of-date. \\nAs the organization‚Äôs data maturity grows, data engineers will typically adopt an \\norchestration framework, perhaps Airflow or Dagster. Data engineers are aware that \\nAirflow presents an operational burden, but the benefits of orchestration eventually \\noutweigh the complexity. Engineers will gradually migrate their cron jobs to Airflow \\njobs. Now, dependencies are checked before jobs run. More transformation jobs can \\nbe packed into a given time because each job can start as soon as upstream data is \\nready rather than at a fixed, predetermined time.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 73, 'page_label': '74'}, page_content='66  | Chapter 2: The Data Engineering Lifecycle \\nThe data engineering team still has room for operational improvements. A data scientist \\neventually deploys a broken DAG, bringing down the Airflow web server and leaving \\nthe data team operationally blind. After enough such headaches, the data engineering \\nteam members realize that they need to stop allowing manual DAG deployments. In \\ntheir next phase of operational maturity, they adopt automated DAG deployment. \\nDAGs are tested before deployment, and monitoring processes ensure that the new \\nDAGs start running properly. In addition, data engineers block the deployment of new \\nPython dependencies until installation is validated. After automation is adopted, the \\ndata team is much happier and experiences far fewer headaches. \\nOne of the tenets of the DataOps Manifesto is ‚ÄúEmbrace change.‚Äù This does not mean \\nchange for the sake of change but rather goal -oriented change. At each stage of our \\nautomation journey, opportunities exist for operational improvement. Even at the high \\nlevel of maturity that we‚Äôve described here, f urther room for improvement remains. \\nEngineers might embrace a next -generation orchestration framework that builds in \\nbetter metadata capabilities. Or they might try to develop a framework that builds \\nDAGs automatically based on data -lineage specifications . The main point is that \\nengineers constantly seek to implement improvements in automation that will reduce \\ntheir workload and increase the value that they deliver to the business. \\nObservability and monitoring \\nAs we tell our clients, ‚ÄúData is a silent killer.‚Äù We‚Äôve seen countless examples of bad \\ndata lingering in reports for months or years. Executives may make key decisions from \\nthis bad data, discovering the error only much later. The outcomes are usually bad and \\nsometimes catastrophic for the business. Initiatives are undermined and destroyed, \\nyears of work wasted. In some of the worst cases, bad data may lead companies to \\nfinancial ruin. \\nAnother horror story occurs when the systems that create the data for reports randomly \\nstop working, resulting in reports being delayed by several days. The data team doesn‚Äôt \\nknow until they‚Äôre asked by stakeholders why reports are late or producing stale \\ninformation. Eventually, various stakeholders lose trust in the capabilities of the core \\ndata team and start their own splinter teams. The result is many different unstable \\nsystems, inconsistent reports, and silos. \\nIf you‚Äôre not observing and monitoring your data and the systems that produce the data, \\nyou‚Äôre inevitably going to experience your own data horror story. Observability, \\nmonitoring, logging, alerting, and tracing are all critical to getting ahead of any \\nproblems along the data engineering lifecycle. We recommend you incorporate SPC to \\nunderstand whether events being monitored are out of line and which incidents are \\nworth responding to.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 74, 'page_label': '75'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  67 \\nPetrella‚Äôs DODD method mentioned previously in this chapter provides an excellent \\nframework for thinking about data observability. DODD is much like test -driven \\ndevelopment (TDD) in software engineering:20 \\nThe purpose of DODD is to give everyone involved in the data chain visibility into the \\ndata and data applications so that everyone involved in the data value chain has the \\nability to identify changes to the data or data applications at every step ‚Äîfrom \\ningestion to transformation to analysis ‚Äîto help troubleshoot or prevent data issues. \\nDODD focuses on making data observability a first -class consideration in the data \\nengineering lifecycle. \\nWe cover many aspects of monitoring and observability throughout the data \\nengineering lifecycle in later chapters. \\nIncident response \\nA high-functioning data team using DataOps will be able to ship new data products \\nquickly. But mistakes will inevitably happen. A system may have downtime, a new \\ndata model may break downstream reports, an ML model may become stale and \\nprovide bad predicti ons‚Äîcountless problems can interrupt the data engineering \\nlifecycle. Incident response  is about using the automation and observability \\ncapabilities mentioned previously to rapidly identify root causes of an incident and \\nresolve it as reliably and quickly as possible. \\nIncident response isn‚Äôt just about technology and tools, though these are beneficial; it‚Äôs \\nalso about open and blameless communication, both on the data engineering team and \\nacross the organization. As Werner V ogels, CTO of Amazon Web Services, is famous \\nfor saying, ‚ÄúEverything breaks all the time.‚Äù Data engineers must be prepared for a \\ndisaster and ready to respond as swiftly and efficiently as possible. \\nData engineers should proactively find issues before the business reports them. \\nFailure happens, and when the stakeholders or end users see problems, they will \\npresent them. They will be unhappy to do so. The feeling is different when they go to \\nraise those issues to a team and see that they are actively being worked on to resolve \\nalready. Which team‚Äôs state would you trust more as an end user? Trust takes a long \\ntime to build and can be lost in minutes. Incident response is as much about \\nretroactively responding to incidents as proactively addressing them before they \\nhappen. \\n \\n20 Andy Petrella, ‚ÄúData Observability Driven Development: The Perfect Analogy for Beginners,‚Äù Kensu, \\naccessed May 5, 2022, https://oreil.ly/MxvSX.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 75, 'page_label': '76'}, page_content='68  | Chapter 2: The Data Engineering Lifecycle \\nDataOps summary \\nAt this point, DataOps is still a work in progress. Practitioners have done a good job \\nof adapting DevOps principles to the data domain and mapping out an initial vision \\nthrough the DataOps Manifesto and other resources. Data engineers would do well to \\nmake DataOps practices a high priority in all of their work. The up-front effort will see \\na significant long-term payoff through faster delivery of products, better reliability and \\naccuracy of data, and greater overall value for the business. \\nThe state of operations in data engineering is still quite immature compared with \\nsoftware engineering. Many data engineering tools, especially legacy monoliths, are \\nnot automation-first. A recent movement has arisen to adopt automation best practices \\nacross the data engineering lifecycle. Tools like Airflow have paved the way for a new \\ngeneration of automation and data management tools. The general practices we \\ndescribe for DataOps are aspirational, and we suggest companies try to adopt them to \\nthe fullest extent possible, given the tools and knowledge available today. \\nData Architecture \\nA data architecture reflects the current and future state of data systems that support an \\norganization‚Äôs long -term data needs and strategy. Because an organization‚Äôs data \\nrequirements will likely change rapidly, and new tools and practices seem to arrive on \\na near-daily basis, data engineers must understand good data architecture. Chapter 3 \\ncovers data architecture in depth, but we want to highlight here that data architecture \\nis an undercurrent of the data engineering lifecycle. \\nA data engineer should first understand the needs of the business and gather \\nrequirements for new use cases. Next, a data engineer needs to translate those \\nrequirements to design new ways to capture and serve data, balanced for cost and \\noperational simplic ity. This means knowing the trade -offs with design patterns, \\ntechnologies, and tools in source systems, ingestion, storage, transformation, and \\nserving data. \\nThis doesn‚Äôt imply that a data engineer is a data architect, as these are typically two \\nseparate roles. If a data engineer works alongside a data architect, the data engineer \\nshould be able to deliver on the data architect‚Äôs designs and provide architectur al \\nfeedback. \\nOrchestration \\nWe think that orchestration matters because we view it as really the center of gravity \\nof both the data platform as well as the data lifecycle, the software development \\nlifecycle as it comes to data.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 76, 'page_label': '77'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  69 \\n‚ÄîNick Schrock, founder of Elementl21 \\nOrchestration is not only a central DataOps process, but also a critical part of the \\nengineering and deployment flow for data jobs. So, what is orchestration? \\nOrchestration is the process of coordinating many jobs to run as quickly and efficiently \\nas possible on a scheduled cadence. For instance, people often refer to orchestration \\ntools like Apache Airflow as schedulers. This isn‚Äôt quite accurate. A pure scheduler, \\nsuch as cron, is aware only of time; an orchestration engine builds in metadata on job \\ndependencies, generally in the form of a directed acyclic graph (DAG). The DAG can \\nbe run once or scheduled to run at a fixed interval of daily, weekly, every hour, every \\nfive minutes, etc. \\nAs we discuss orchestration throughout this book, we assume that an orchestration \\nsystem stays online with high availability. This allows the orchestration system to sense \\nand monitor constantly without human intervention and run new jobs anytime they are \\ndeployed. An orchestration system monitors jobs that it manages and kicks off new \\ntasks as internal DAG dependencies are completed. It can also monitor external \\nsystems and tools to watch for data to arrive and criteria to be met. When certain \\nconditions go out of bounds, the system also sets error conditions and sends alerts \\nthrough email or other channels. You might set an expected completion time of 10 a.m. \\nfor overnight daily data pipelines. If jobs are not done by this time, alerts go out to data \\nengineers and consumers. \\nOrchestration systems also build job history capabilities, visualization, and alerting. \\nAdvanced orchestration engines can backfill new DAGs or individual tasks as they are \\nadded to a DAG. They also support dependencies over a time range. For example, a \\nmonthly reporting job might check that an ETL job has been completed for the full \\nmonth before starting. \\nOrchestration has long been a key capability for data processing but was not often top \\nof mind nor accessible to anyone except the largest companies. Enterprises used \\nvarious tools to manage job flows, but these were expensive, out of reach of small \\nstartups, and generally not extensible. Apache Oozie was extremely popular in the \\n2010s, but it was designed to work within a Hadoop cluster and was difficult to use in \\na more heterogeneous environment. Facebook developed Dataswarm for internal use \\nin the late 2000s; this inspired popular tools such as Airflow, introduced by Airbnb in \\n2014. \\nAirflow was open source from its inception and was widely adopted. It was written in \\nPython, making it highly extensible to almost any use case imaginable. While many \\n \\n21 Ternary Data, ‚ÄúAn Introduction to Dagster: The Orchestrator for the Full Data Lifecycle - UDEM June \\n2021,‚Äù YouTube video, 1:09:40, https://oreil.ly/HyGMh.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 77, 'page_label': '78'}, page_content='70  | Chapter 2: The Data Engineering Lifecycle \\nother interesting open source orchestration projects exist, such as Luigi and Conductor, \\nAirflow is arguably the mindshare leader for the time being. Airflow arrived just as \\ndata processing was becoming more abstract and accessible, and engineers were \\nincreasingly interested in coordinating complex flows across multiple processors and \\nstorage systems, especially in cloud environments. \\nAt this writing, several nascent open source projects aim to mimic the best elements of \\nAirflow‚Äôs core design while improving on it in key areas. Some of the most interesting \\nexamples are Prefect and Dagster, which aim to improve the portability and testability \\nof DAGs to allow engineers to move from local development to production more \\neasily. Argo is an orchestration engine built around Kubernetes primitives; Metaflow \\nis an open source project out of Netflix that aims to improve data science orchestration. \\nWe must point out that orchestration is strictly a batch concept. The streaming \\nalternative to orchestrated task DAGs is the streaming DAG. Streaming DAGs remain \\nchallenging to build and maintain, but next -generation streaming platforms such as \\nPulsar aim to dramatically reduce the engineering and operational burden. We talk \\nmore about these developments in Chapter 8. \\nSoftware Engineering \\nSoftware engineering has always been a central skill for data engineers. In the early \\ndays of contemporary data engineering (2000 ‚Äì2010), data engineers worked on \\nlowlevel frameworks and wrote MapReduce jobs in C, C++, and Java. At the peak of \\nthe big data era (the mid-2010s), engineers started using frameworks that abstracted \\naway these low-level details. \\nThis abstraction continues today. Cloud data warehouses support powerful \\ntransformations using SQL semantics; tools like Spark have become more user -\\nfriendly, transitioning away from low -level coding details and toward easy -to-use \\ndataframes. Despite this abstraction, software engineering is still critical to data \\nengineering. We want to briefly discuss a few common areas of software engineering \\nthat apply to the data engineering lifecycle. \\nCore data processing code \\nThough it has become more abstract and easier to manage, core data processing code \\nstill needs to be written, and it appears throughout the data engineering lifecycle. \\nWhether in ingestion, transformation, or data serving, data engineers need to be highly \\nproficient and productive in frameworks and languages such as Spark, SQL, or Beam; \\nwe reject the notion that SQL is not code. \\nIt‚Äôs also imperative that a data engineer understand proper code-testing methodologies, \\nsuch as unit, regression, integration, end-to-end, and smoke.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 78, 'page_label': '79'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  71 \\nDevelopment of open source frameworks \\nMany data engineers are heavily involved in developing open source frameworks. \\nThey adopt these frameworks to solve specific problems in the data engineering \\nlifecycle, and then continue developing the framework code to improve the tools for \\ntheir use cases and contribute back to the community. \\nIn the big data era, we saw a Cambrian explosion of data-processing frameworks inside \\nthe Hadoop ecosystem. These tools primarily focused on transforming and serving \\nparts of the data engineering lifecycle. Data engineering tool speciation has not ceased \\nor slowed down, but the emphasis has shifted up the ladder of abstraction, away from \\ndirect data processing. This new generation of open source tools assists engineers in \\nmanaging, enhancing, connecting, optimizing, and monitoring data. \\nFor example, Airflow dominated the orchestration space from 2015 until the early \\n2020s. Now, a new batch of open source competitors (including Prefect, Dagster, and \\nMetaflow) has sprung up to fix perceived limitations of Airflow, providing better \\nmetadata handling, portability, and dependency management. Where the future of \\norchestration goes is anyone‚Äôs guess. \\nBefore data engineers begin engineering new internal tools, they would do well to \\nsurvey the landscape of publicly available tools. Keep an eye on the total cost of \\nownership (TCO) and opportunity cost associated with implementing a tool. There is \\na good chance that an open source project already exists to address the problem they‚Äôre \\nlooking to solve, and they would do well to collaborate rather than reinventing the \\nwheel. \\nStreaming \\nStreaming data processing is inherently more complicated than batch, and the tools and \\nparadigms are arguably less mature. As streaming data becomes more pervasive in \\nevery stage of the data engineering lifecycle, data engineers face interesting software \\nengineering problems. \\nFor instance, data processing tasks such as joins that we take for granted in the batch \\nprocessing world often become more complicated in real time, requiring more complex \\nsoftware engineering. Engineers must also write code to apply a variety of windowing \\nmethods. Windowing allows real-time systems to calculate valuable metrics such as \\ntrailing statistics. Engineers have many frameworks to choose from, including various \\nfunction platforms (OpenFaaS, AWS Lambda, Google Cloud Functions) for handling \\nindividual events or dedicated stream processors (Spark, Beam, Flink, or Pulsar) for \\nanalyzing streams to support reporting and real-time actions.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 79, 'page_label': '80'}, page_content='72  | Chapter 2: The Data Engineering Lifecycle \\nInfrastructure as code \\nInfrastructure as code (IaC) applies software engineering practices to the configuration \\nand management of infrastructure. The infrastructure management burden of the big \\ndata era has decreased as companies have migrated to managed big data systems ‚Äî\\nsuch as Databricks and Amazon  Elastic MapReduce (EMR) ‚Äîand cloud data \\nwarehouses. When data engineers have to manage their infrastructure in a cloud \\nenvironment, they increasingly do this through IaC frameworks rather than manually \\nspinning up instances and instal ling software. Several general -purpose and \\ncloudplatform-specific frameworks allow automated infrastructure deployment based \\non a set of specifications. Many of these frameworks can manage cloud services as \\nwell as infrastructure. There is also a notion of  IaC with containers and Kubernetes, \\nusing tools like Helm. \\nThese practices are a vital part of DevOps, allowing version control and repeatability \\nof deployments. Naturally, these capabilities are vital throughout the data engineering \\nlifecycle, especially as we adopt DataOps practices. \\nPipelines as code \\nPipelines as code is the core concept of present-day orchestration systems, which touch \\nevery stage of the data engineering lifecycle. Data engineers use code (typically \\nPython) to declare data tasks and dependencies among them. The orchestration engine \\ninterprets these instructions to run steps using available resources. \\nGeneral-purpose problem solving \\nIn practice, regardless of which high-level tools they adopt, data engineers will run into \\ncorner cases throughout the data engineering lifecycle that require them to solve \\nproblems outside the boundaries of their chosen tools and to write custom code. When \\nusing frameworks like Fivetran, Airbyte, or Matillion, data engineers will encounter \\ndata sources without existing connectors and need to write something custom. They \\nshould be proficient in software engineering to understand APIs, pull and transform \\ndata, handle exceptions, and so forth. \\nConclusion \\nMost discussions we‚Äôve seen in the past about data engineering involve technologies \\nbut miss the bigger picture of data lifecycle management. As technologies become \\nmore abstract and do more heavy lifting, a data engineer has the opportunity to think \\nand a ct on a higher level. The data engineering lifecycle, supported by its \\nundercurrents, is an extremely useful mental model for organizing the work of data \\nengineering.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 80, 'page_label': '81'}, page_content='Major Undercurrents Across the Data Engineering Lifecycle \\n |  73 \\nWe break the data engineering lifecycle into the following stages: \\n‚Ä¢ Generation \\n‚Ä¢ Storage \\n‚Ä¢ Ingestion \\n‚Ä¢ Transformation \\n‚Ä¢ Serving data \\nSeveral themes cut across the data engineering lifecycle as well. These are the \\nundercurrents of the data engineering lifecycle. At a high level, the undercurrents are \\nas follows:'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 81, 'page_label': '82'}, page_content='|  74 \\n‚Ä¢ Security \\n‚Ä¢ Data management \\n‚Ä¢ DataOps \\n‚Ä¢ Data architecture \\n‚Ä¢ Orchestration \\n‚Ä¢ Software engineering \\nA data engineer has several top-level goals across the data lifecycle: produce optimum \\nROI and reduce costs (financial and opportunity), reduce risk (security, data quality), \\nand maximize data value and utility. \\nThe next two chapters discuss how these elements impact good architecture design, \\nalong with choosing the right technologies. If you feel comfortable with these two \\ntopics, feel free to skip ahead to Part II, where we cover each of the stages of the data \\nengineering lifecycle. \\nAdditional Resources \\n‚Ä¢ ‚ÄúA Comparison of Data Processing Frameworks‚Äù by Ludovic Santos \\n‚Ä¢ DAMA International website \\n‚Ä¢ ‚ÄúThe Dataflow Model: A Practical Approach to Balancing Correctness, Latency, \\nand Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing‚Äù by Tyler \\nAkidau et al. \\n‚Ä¢ ‚ÄúData Processing‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúData Transformation‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúDemocratizing Data at Airbnb‚Äù by Chris Williams et al. \\n‚Ä¢ ‚ÄúFive Steps to Begin Collecting the Value of Your Data‚Äù Lean-Data web page \\n‚Ä¢ ‚ÄúGetting Started with DevOps Automation‚Äù by Jared Murrell \\n‚Ä¢ ‚ÄúIncident Management in the Age of DevOps‚Äù Atlassian web page \\n‚Ä¢ ‚ÄúAn Introduction to Dagster: The Orchestrator for the Full Data Lifecycle‚Äù video \\nby Nick Schrock \\n‚Ä¢ ‚ÄúIs DevOps Related to DataOps?‚Äù by Carol Jang and Jove Kuang \\n‚Ä¢ ‚ÄúThe Seven Stages of Effective Incident Response‚Äù Atlassian web page \\n‚Ä¢ ‚ÄúStaying Ahead of Debt‚Äù by Etai Mizrahi \\n‚Ä¢ ‚ÄúWhat Is Metadata‚Äù by Michelle Knight'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 82, 'page_label': '83'}, page_content='|  75 \\nAdditional Resources'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 83, 'page_label': '84'}, page_content=''),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 84, 'page_label': '85'}, page_content='77 \\nCHAPTE\\nR 3 \\nDesig\\nning Good Data \\nArchitecture \\nGood data architecture provides seamless capabilities across every step of the data \\nlifecycle and undercurrent. We‚Äôll begin by defining data architecture and then discuss \\ncomponents and considerations. We‚Äôll then touch on specific batch patterns (data \\nwarehouses, data lakes), streaming patterns, and patterns that unify batch and \\nstreaming. Throughout, we‚Äôll emphasize leveraging the capabilities of the clou d to \\ndeliver scalability, availability, and reliability. \\nWhat Is Data Architecture? \\nSuccessful data engineering is built upon rock -solid data architecture. This chapter \\naims to review a few popular architecture approaches and frameworks, and then craft \\nour opinionated definition of what makes ‚Äúgood‚Äù data architecture. Indeed, we won‚Äôt \\nmake everyone happy. Still, we will lay out a pragmatic, domain -specific, working \\ndefinition for data architecture that we think will work for companies of vastly \\ndifferent scales, business processes, and needs.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 85, 'page_label': '86'}, page_content='78  | Chapter 3: Designing Good Data Architecture \\nWhat is data architecture? When you stop to unpack it, the topic becomes a bit murky; \\nresearching data architecture yields many inconsistent and often outdated definitions. \\nIt‚Äôs a lot like when we defined data engineering in Chapter 1‚Äîthere‚Äôs no consensus. \\nIn a field that is constantly changing, this is to be expected. So what do we mean by \\ndata architecture for the purposes of this book? Before defining the term, it‚Äôs essential \\nto understand the context in which it sits. Let‚Äôs briefly cover enterprise arch itecture, \\nwhich will frame our definition of data architecture. \\nEnterprise Architecture Defined \\nEnterprise architecture has many subsets, including business, technical, application, \\nand data ( Figure 3 -1). As such, many frameworks and resources are devoted to \\nenterprise architecture. In truth, architecture is a surprisingly controversial topic. \\n \\nFigure 3-1. Data architecture is a subset of enterprise architecture \\nThe term enterprise gets mixed reactions. It brings to mind sterile corporate offices, \\ncommand-and-control/waterfall planning, stagnant business cultures, and empty \\ncatchphrases. Even so, we can learn some things here. \\nBefore we define and describe enterprise architecture, let‚Äôs unpack this term. Let‚Äôs \\nlook at how enterprise architecture is defined by some significant thought leaders: \\nTOGAF, Gartner, and EABOK. \\nTOGAF‚Äôs definition \\nTOGAF is The Open Group Architecture Framework, a standard of The Open Group. \\nIt‚Äôs touted as the most widely used architecture framework today. Here‚Äôs the TOGAF \\ndefinition:22 \\nThe term ‚Äúenterprise‚Äù in the context of ‚Äúenterprise architecture‚Äù can denote an entire \\nenterprise‚Äîencompassing all of its information and technology services, processes, \\nand infrastructure ‚Äîor a specific domain within the enterprise. In both cases, the \\narchitecture crosses multiple systems, and multiple functional groups within the \\nenterprise. \\n \\n22 The Open Group, TOGAF Version 9.1, https://oreil.ly/A1H67.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 86, 'page_label': '87'}, page_content='|  79 \\nGartner‚Äôs definition \\nGartner is a global research and advisory company that produces research articles and \\nreports on trends related to enterprises. Among other things, it is responsible for the \\n(in)famous Gartner Hype Cycle. Gartner‚Äôs definition is as follows:23 \\nEnterprise architecture (EA) is a discipline for proactively and holistically leading \\nenterprise responses to disruptive forces by identifying and analyzing the execution of \\nchange toward desired business vision and outcomes. EA delivers value by presenting \\nbusiness and IT leaders with signature -ready recommendations for adjusting policies \\nand projects to achieve targeted business outcomes that capitalize on relevant business \\ndisruptions. \\nEABOK‚Äôs definition \\nEABOK is the Enterprise Architecture Book of Knowledge, an enterprise architecture \\nreference produced by the MITRE Corporation. EABOK was released as an \\nincomplete draft in 2004 and has not been updated since. Though seemingly obsolete, \\nEABOK is frequently referenced in descriptions of enterprise architecture ; we found \\nmany of its ideas helpful while writing this book. Here‚Äôs the EABOK definition:24 \\nEnterprise Architecture (EA) is an organizational model; an abstract representation of \\nan Enterprise that aligns strategy, operations, and technology to create a roadmap for \\nsuccess. \\nOur definition \\nWe extract a few common threads in these definitions of enterprise architecture: \\nchange, alignment, organization, opportunities, problem-solving, and migration. Here \\nis our definition of enterprise architecture, one that we feel is more relevant to today‚Äôs \\nfast-moving data landscape: \\nEnterprise architecture is the design of systems to support change in the enterprise , \\nachieved by flexible and reversible decisions  reached through careful evaluation of \\ntrade-offs. \\nHere, we touch on some key areas we‚Äôll return to throughout the book: flexible and \\nreversible decisions, change management, and evaluation of trade -offs. We discuss \\neach theme at length in this section and then make the definition more concrete in the \\nlatter part of the chapter by giving various examples of data architecture. Flexible and \\nreversible decisions are essential for two reasons. First, the world is constantly \\nchanging, and predicting the future is impossible. Reversible decisions allow you to \\nadjust course as the world changes and you gather new information. Second, there is a \\n \\n23 Gartner Glossary, s.v. ‚ÄúEnterprise Architecture (EA),‚Äù https://oreil.ly/SWwQF. \\n24 EABOK Consortium website, https://eabok.org.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 87, 'page_label': '88'}, page_content='80  | Chapter 3: Designing Good Data Architecture \\nnatural tendency toward enterprise ossification as organizations grow. Adopting a \\nculture of reversible decisions helps overcome this tendency by reducing the risk \\nattached to a decision. \\nJeff Bezos is credited with the idea of one-way and two-way doors.25 A one-way door \\nis a decision that is almost impossible to reverse. For example, Amazon could have \\ndecided to sell AWS or shut it down. It would be nearly impossible for Amazon to \\nrebuild a public cloud with the same market position after such an action. \\nOn the other hand, a two-way door is an easily reversible decision: you walk through \\nand proceed if you like what you see in the room or step back through the door if you \\ndon‚Äôt. Amazon might decide to require the use of DynamoDB for a new microservices \\ndatabase. If this policy doesn‚Äôt wor k, Amazon has the option of reversing it and \\nrefactoring some services to use other databases. Since the stakes attached to each \\nreversible decision (two-way door) are low, organizations can make more decisions, \\niterating, improving, and collecting data rapidly. \\nChange management is closely related to reversible decisions and is a central theme of \\nenterprise architecture frameworks. Even with an emphasis on reversible decisions, \\nenterprises often need to undertake large initiatives. These are ideally broken into \\nsmaller changes, each one a reversible decision in itself. Returning to Amazon, we note \\na five-year gap (2007 to 2012) from the publication of a paper on the DynamoDB \\nconcept to Werner V ogels‚Äôs announcement of the DynamoDB service on AWS. Behind \\nthe scenes, teams took numerous small actions to make DynamoDB a concrete reality \\nfor AWS customers. Managing such small actions is at the heart of change \\nmanagement. \\nArchitects are not simply mapping out IT processes and vaguely looking toward a \\ndistant, utopian future; they actively solve business problems and create new \\nopportunities. Technical solutions exist not for their own sake but in support of \\nbusiness goals. Architects identify problems in the current state (poor data quality, \\nscalability limits, money-losing lines of business), define desired future states (agile \\ndata-quality improvement, scalable cloud data solutions, improved business \\nprocesses), and realize initiatives through execution of small, concrete steps. It bears \\nrepeating: \\nTechnical solutions exist not for their own sake but in support of business goals. \\nWe found significant inspiration in Fundamentals of Software Architecture by Mark \\nRichards and Neal Ford (O‚ÄôReilly). They emphasize that trade-offs are inevitable and \\n \\n25 Jeff Haden, ‚ÄúAmazon Founder Jeff Bezos: This Is How Successful People Make Such Smart Decisions,‚Äù Inc., \\nDecember 3, 2018, https://oreil.ly/QwIm0. \\nWhat Is Data Architecture?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 88, 'page_label': '89'}, page_content='|  81 \\nubiquitous in the engineering space. Sometimes the relatively fluid nature of software \\nand data leads us to believe that we are freed from the constraints that engineers face \\nin the hard, cold physical world. Indeed, this is partially true; patching a software bug \\nis much easier than redesigning and replacing an airplane wing. However, digital \\nsystems are ultimately constrained by physical limits such as latency, reliability, \\ndensity, and energy consumption. Engineers also confront various nonphysical limits, \\nsuch as characteristics of programming languages and frameworks, and practical \\nconstraints in managing complexity, budgets, etc. Magical thinking culminates in poor \\nengineering. Data engineers must account for trade -offs at every step to design an \\noptimal system while minimizing high-interest technical debt. \\nLet‚Äôs reiterate one central point in our enterprise architecture definition: enterprise \\narchitecture balances flexibility and trade-offs. This isn‚Äôt always an easy balance, and \\narchitects must constantly assess and reevaluate with the recognition that the world is \\ndynamic. Given the pace of change that enterprises are faced with, organizations ‚Äî \\nand their architecture‚Äîcannot afford to stand still. \\nData Architecture Defined \\nNow that you understand enterprise architecture, let‚Äôs dive into data architecture by \\nestablishing a working definition that will set the stage for the rest of the book. Data \\narchitecture is a subset of enterprise architecture, inheriting its properties: processes, \\nstrategy, change management, and evaluating trade -offs. Here are a couple of \\ndefinitions of data architecture that influence our definition. \\nTOGAF‚Äôs definition \\nTOGAF defines data architecture as follows:26 \\nA description of the structure and interaction of the enterprise‚Äôs major types and \\nsources of data, logical data assets, physical data assets, and data management \\nresources. \\nDAMA‚Äôs definition \\nThe DAMA DMBOK defines data architecture as follows:27 \\nIdentifying the data needs of the enterprise (regardless of structure) and designing and \\nmaintaining the master blueprints to meet those needs. Using master blueprints to \\n \\n26 The Open Group, TOGAF Version 9.1, https://oreil.ly/A1H67. \\n27 DAMA - DMBOK: Data Management Body of Knowledge, 2nd ed. (Technics Publications, 2017). \\nWhat Is Data Architecture?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 89, 'page_label': '90'}, page_content='82  | Chapter 3: Designing Good Data Architecture \\nguide data integration, control data assets, and align data investments with business \\nstrategy. \\nOur definition \\nConsidering the preceding two definitions and our experience, we have crafted our \\ndefinition of data architecture: \\nData architecture is the design of systems to support the evolving data needs of an \\nenterprise, achieved by flexible and reversible decisions reached through a careful \\nevaluation of trade-offs. \\nHow does data architecture fit into data engineering? Just as the data engineering \\nlifecycle is a subset of the data lifecycle, data engineering architecture is a subset of \\ngeneral data architecture. Data engineering architecture  is the systems and \\nframeworks that make up the key sections of the data engineering lifecycle. We‚Äôll use \\ndata architecture interchangeably with data engineering architecture throughout this \\nbook. \\nOther aspects of data architecture that you should be aware of are operational and \\ntechnical ( Figure 3 -2). Operational architecture  encompasses the functional \\nrequirements of what needs to happen related to people, processes, and technology. \\nFor example, what business processes does the data serve? How does the organization \\nmanage data quality? What is the latency requirement from when the data is produced \\nto when it becomes available to query? Technical architecture outlines how data is \\ningested, stored, transformed, and served along the data engineering lifecycle. For \\ninstance, how will you move 10 TB of data every hour from a source database to your \\ndata lake? In short, operational architecture describes what needs to be done, and \\ntechnical architecture details how it will happen. \\n \\nFigure 3-2. Operational and technical data architecture \\nNow that we have a working definition of data architecture, let‚Äôs cover the elements of \\n‚Äúgood‚Äù data architecture. \\n‚ÄúGood‚Äù Data Architecture \\nNever shoot for the best architecture, but rather the least worst architecture.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 90, 'page_label': '91'}, page_content='|  83 \\n‚ÄîMark Richards and Neal Ford28 \\nAccording to Grady Booch, ‚ÄúArchitecture represents the significant design decisions \\nthat shape a system, where significant is measured by cost of change.‚Äù Data architects \\naim to make significant decisions that will lead to good architecture at a basic level. \\nWhat do we mean by ‚Äúgood‚Äù data architecture? To paraphrase an old cliche, you know \\ngood when you see it. Good data architecture  serves business requirements with a \\ncommon, widely reusable set of building blocks while maintaining flexibility and \\nmaking appropriate trade -offs. Bad architecture is authoritarian and tries to cram a \\nbunch of one-size-fits-all decisions into a big ball of mud. \\n \\n28 Mark Richards and Neal Ford, Fundamentals of Software Architecture (Sebastopol, CA: O‚ÄôReilly, 2020), \\nhttps://oreil.ly/hpCp0.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 91, 'page_label': '92'}, page_content='84  | Chapter 3: Designing Good Data Architecture \\nAgility is the foundation for good data architecture; it acknowledges that the world is \\nfluid. Good data architecture is flexible and easily maintainable. It evolves in response \\nto changes within the business and new technologies and practices that may unlock \\neven more value in the future. Businesses and their use cases for data are always \\nevolving. The world is dynamic, and the pace of change in the data space is \\naccelerating. Last year‚Äôs data architecture that served you well might not be sufficient \\nfor today, let alone next year. \\nBad data architecture is tightly coupled, rigid, overly centralized, or uses the wrong \\ntools for the job, hampering development and change management. Ideally, by \\ndesigning architecture with reversibility in mind, changes will be less costly. \\nThe undercurrents of the data engineering lifecycle form the foundation of good data \\narchitecture for companies at any stage of data maturity. Again, these undercurrents are \\nsecurity, data management, DataOps, data architecture, orchestration, and software  \\nengineering. \\nGood data architecture is a living, breathing thing. It‚Äôs never finished. In fact, per our \\ndefinition, change and evolution are central to the meaning and purpose of data \\narchitecture. Let‚Äôs now look at the principles of good data architecture. \\nPrinciples of Good Data Architecture \\nThis section takes a 10,000-foot view of good architecture by focusing on principles‚Äî \\nkey ideas useful in evaluating major architectural decisions and practices. We borrow \\ninspiration for our architecture principles from several sources, especially the AWS  \\nWell-Architected Framework and Google Cloud‚Äôs Five Principles for Cloud -Native \\nArchitecture. \\nThe AWS Well-Architected Framework consists of six pillars: \\n‚Ä¢ Operational excellence \\n‚Ä¢ Security \\n‚Ä¢ Reliability \\n‚Ä¢ Performance efficiency \\n‚Ä¢ Cost optimization \\n‚Ä¢ Sustainability \\nGoogle Cloud‚Äôs Five Principles for Cloud-Native Architecture are as follows: \\n‚Ä¢ Design for automation. \\n‚Ä¢ Be smart with state.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 92, 'page_label': '93'}, page_content='Principles of Good Data Architecture \\n |  85 \\n‚Ä¢ Favor managed services. \\n‚Ä¢ Practice defense in depth. \\n‚Ä¢ Always be architecting. \\nWe advise you to carefully study both frameworks, identify valuable ideas, and \\ndetermine points of disagreement. We‚Äôd like to expand or elaborate on these pillars with \\nthese principles of data engineering architecture: \\n1. Choose common components wisely. \\n2. Plan for failure. \\n3. Architect for scalability. \\n4. Architecture is leadership. \\n5. Always be architecting. \\n6. Build loosely coupled systems. \\n7. Make reversible decisions. \\n8. Prioritize security. \\n9. Embrace FinOps. \\nPrinciple 1: Choose Common Components Wisely \\nOne of the primary jobs of a data engineer is to choose common components and \\npractices that can be used widely across an organization. When architects choose well \\nand lead effectively, common components become a fabric facilitating team \\ncollaboration and breaking down silos. Common components enable agility within and \\nacross teams in conjunction with shared knowledge and skills. \\nCommon components can be anything that has broad applicability within an \\norganization. Common components include object storage, version -control systems, \\nobservability, monitoring and orchestration systems, and processing engines. Common \\ncomponents should be accessible to everyone with an appropriate use case, and teams \\nare encouraged to rely on common components already in use rather than reinventing \\nthe wheel. Common components must support robust permissions and security to \\nenable sharing of assets among teams while preventing unauthorized access. \\nCloud platforms are an ideal place to adopt common components. For example, \\ncompute and storage separation in cloud data systems allows users to access a shared \\nstorage layer (most commonly object storage) using specialized tools to access and \\nquery the data needed for specific use cases.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 93, 'page_label': '94'}, page_content='86  | Chapter 3: Designing Good Data Architecture \\nChoosing common components is a balancing act. On the one hand, you need to focus \\non needs across the data engineering lifecycle and teams, utilize common components \\nthat will be useful for individual projects, and simultaneously facilitate interoperation \\nand collaboration. On the other hand, architects should avoid decisions that will hamper \\nthe productivity of engineers working on domain -specific problems by forcing them \\ninto one-size-fits-all technology solutions. Chapter 4 provides additional details. \\nPrinciple 2: Plan for Failure \\nEverything fails, all the time. \\n‚ÄîWerner V ogels, CTO of Amazon Web Services29 \\nModern hardware is highly robust and durable. Even so, any hardware component will \\nfail, given enough time. To build highly robust data systems, you must consider failures \\nin your designs. Here are a few key terms for evaluating failure scenarios; we describe \\nthese in greater detail in this chapter and throughout the book: \\nAvailability \\nThe percentage of time an IT service or component is in an operable state. \\nReliability \\nThe system‚Äôs probability of meeting defined standards in performing its intended \\nfunction during a specified interval. \\nRecovery time objective \\nThe maximum acceptable time for a service or system outage. The recovery time \\nobjective (RTO) is generally set by determining the business impact of an outage. \\nAn RTO of one day might be fine for an internal reporting system. A website \\noutage of just five minutes could have a significant adverse business impact on an \\nonline retailer. \\nRecovery point objective \\nThe acceptable state after recovery. In data systems, data is often lost during an \\noutage. In this setting, the recovery point objective (RPO) refers to the maximum \\nacceptable data loss. \\nEngineers need to consider acceptable reliability, availability, RTO, and RPO in \\ndesigning for failure. These will guide their architecture decisions as they assess \\npossible failure scenarios. \\n \\n29 UberPulse, ‚ÄúAmazon.com CTO: Everything Fails,‚Äù YouTube video, 3:03, https://oreil.ly/vDVlX.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 94, 'page_label': '95'}, page_content='Principles of Good Data Architecture \\n |  87 \\nPrinciple 3: Architect for Scalability \\nScalability in data systems encompasses two main capabilities. First, scalable systems \\ncan scale up to handle significant quantities of data. We might need to spin up a large \\ncluster to train a model on a petabyte of customer data or scale out a streaming ingestion \\nsystem to handle a transient load spike. Our ability to scale up allows us to handle \\nextreme loads temporarily. Second, scalable systems can scale down. Once the load \\nspike ebbs, we should automatically remove capacity to cut costs. (This is rela ted to \\nprinciple 9.) An elastic system can scale dynamically in response to load, ideally in an \\nautomated fashion. \\nSome scalable systems can also scale to zero: they shut down completely when not in \\nuse. Once the large model -training job completes, we can delete the cluster. Many \\nserverless systems (e.g., serverless functions and serverless online analytical \\nprocessing, or OLAP , databases) can automatically scale to zero. \\nNote that deploying inappropriate scaling strategies can result in overcomplicated \\nsystems and high costs. A straightforward relational database with one failover node \\nmay be appropriate for an application instead of a complex cluster arrangement. \\nMeasure your current load, approximate load spikes, and estimate load over the next \\nseveral years to determine if your database architecture is appropriate. If your startup \\ngrows much faster than anticipated, this growth should also lead to more available \\nresources to rearchitect for scalability. \\nPrinciple 4: Architecture Is Leadership \\nData architects are responsible for technology decisions and architecture descriptions \\nand disseminating these choices through effective leadership and training. Data \\narchitects should be highly technically competent but delegate most individual \\ncontributor work to others. Strong leadership skills combined with high technical \\ncompetence are rare and extremely valuable. The best data architects take this duality \\nseriously. \\nNote that leadership does not imply a command-and-control approach to technology. \\nIt was not uncommon in the past for architects to choose one proprietary database \\ntechnology and force every team to house their data there. We oppose this approach \\nbecause it can significantly hinder current data projects. Cloud environments allow \\narchitects to balance common component choices with flexibility that enables \\ninnovation within projects.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 95, 'page_label': '96'}, page_content='88  | Chapter 3: Designing Good Data Architecture \\nReturning to the notion of technical leadership, Martin Fowler describes a specific \\narchetype of an ideal software architect, well embodied in his colleague Dave Rice:30 \\nIn many ways, the most important activity of Architectus Oryzus  is to mentor the \\ndevelopment team, to raise their level so they can take on more complex issues. \\nImproving the development team‚Äôs ability gives an architect much greater leverage than \\nbeing the sole decision -maker and thus running the risk of being an arc hitectural \\nbottleneck. \\nAn ideal data architect manifests similar characteristics. They possess the technical \\nskills of a data engineer but no longer practice data engineering day to day; they mentor \\ncurrent data engineers, make careful technology choices in consultation with the ir \\norganization, and disseminate expertise through training and leadership. They train \\nengineers in best practices and bring the company‚Äôs engineering resources together to \\npursue common goals in both technology and business. \\nAs a data engineer, you should practice architecture leadership and seek mentorship \\nfrom architects. Eventually, you may well occupy the architect role yourself. \\nPrinciple 5: Always Be Architecting \\nWe borrow this principle directly from Google Cloud‚Äôs Five Principles for CloudNative \\nArchitecture. Data architects don‚Äôt serve in their role simply to maintain the existing \\nstate; instead, they constantly design new and exciting things in response to changes in \\nbusiness and technology. Per the EABOK, an architect‚Äôs job is to develop deep \\nknowledge of the baseline architecture (current state), develop a target architecture, \\nand map out a sequencing plan to determine priorities and the order of architecture \\nchanges. \\nWe add that modern architecture should not be command-and-control or waterfall but \\ncollaborative and agile. The data architect maintains a target architecture and \\nsequencing plans that change over time. The target architecture becomes a moving \\ntarget, adju sted in response to business and technology changes internally and \\nworldwide. The sequencing plan determines immediate priorities for delivery. \\nPrinciple 6: Build Loosely Coupled Systems \\nWhen the architecture of the system is designed to enable teams to test, deploy, and \\nchange systems without dependencies on other teams, teams require little \\ncommunication to get work done. In other words, both the architecture and the teams \\nare loosely coupled. \\n‚ÄîGoogle DevOps tech architecture guide31 \\n \\n30 Martin Fowler, ‚ÄúWho Needs an Architect?‚Äù IEEE Software, July/August 2003, https://oreil.ly/wAMmZ. \\n31 Google Cloud, ‚ÄúDevOps Tech: Architecture,‚Äù Cloud Architecture Center, https://oreil.ly/j4MT1.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 96, 'page_label': '97'}, page_content='Principles of Good Data Architecture \\n |  89 \\nIn 2002, Bezos wrote an email to Amazon employees that became known as the Bezos \\nAPI Mandate:32 \\n1. All teams will henceforth expose their data and functionality through service \\ninterfaces. \\n2. Teams must communicate with each other through these interfaces. \\n3. There will be no other form of interprocess communication allowed: no direct \\nlinking, no direct reads of another team‚Äôs data store, no shared-memory model, no \\nback-doors whatsoever. The only communication allowed is via service interface \\ncalls over the network. \\n4. It doesn‚Äôt matter what technology they use. HTTP, Corba, Pubsub, custom \\nprotocols‚Äîdoesn‚Äôt matter. \\n5. All service interfaces, without exception, must be designed from the ground up to \\nbe externalizable. That is to say, the team must plan and design to be able to expose \\nthe interface to developers in the outside world. No exceptions. \\nThe advent of Bezos‚Äôs API Mandate is widely viewed as a watershed moment for \\nAmazon. Putting data and services behind APIs enabled the loose coupling and \\neventually resulted in AWS as we know it now. Google‚Äôs pursuit of loose coupling \\nallowed it to grow its systems to an extraordinary scale. \\nFor software architecture, a loosely coupled system has the following properties: \\n1. Systems are broken into many small components. \\n2. These systems interface with other services through abstraction layers, such as a \\nmessaging bus or an API. These abstraction layers hide and protect internal details \\nof the service, such as a database backend or internal classes and method \\ncalls. \\n3. As a consequence of property 2, internal changes to a system component don‚Äôt \\nrequire changes in other parts. Details of code updates are hidden behind stable \\nAPIs. Each piece can evolve and improve separately. \\n4. As a consequence of property 3, there is no waterfall, global release cycle for the \\nwhole system. Instead, each component is updated separately as changes and \\nimprovements are made. \\nNotice that we are talking about technical systems. We need to think bigger. Let‚Äôs \\ntranslate these technical characteristics into organizational characteristics: \\n \\n32 ‚ÄúThe Bezos API Mandate: Amazon‚Äôs Manifesto for Externalization,‚Äù Nordic APIs, January 19, 2021, \\nhttps://oreil.ly/vIs8m.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 97, 'page_label': '98'}, page_content='90  | Chapter 3: Designing Good Data Architecture \\n1. Many small teams engineer a large, complex system. Each team is tasked with \\nengineering, maintaining, and improving some system components. \\n2. These teams publish the abstract details of their components to other teams via \\nAPI definitions, message schemas, etc. Teams need not concern themselves with \\nother teams‚Äô components; they simply use the published API or message \\nspecifications to call these  components. They iterate their part to improve their \\nperformance and capabilities over time. They might also publish new capabilities \\nas they are added or request new stuff from other teams. Again, the latter happens \\nwithout teams needing to worry about the internal technical details of the requested \\nfeatures. Teams work together through loosely coupled communication. \\n3. As a consequence of characteristic 2, each team can rapidly evolve and improve \\nits component independently of the work of other teams. \\n4. Specifically, characteristic 3 implies that teams can release updates to their \\ncomponents with minimal downtime. Teams release continuously during regular \\nworking hours to make code changes and test them. \\nLoose coupling of both technology and human systems will allow your data \\nengineering teams to more efficiently collaborate with one another and with other \\nparts of the company. This principle also directly facilitates principle 7. \\nPrinciple 7: Make Reversible Decisions \\nThe data landscape is changing rapidly. Today‚Äôs hot technology or stack is tomorrow‚Äôs \\nafterthought. Popular opinion shifts quickly. You should aim for reversible decisions, \\nas these tend to simplify your architecture and keep it agile. \\nAs Fowler wrote, ‚ÄúOne of an architect‚Äôs most important tasks is to remove architecture \\nby finding ways to eliminate irreversibility in software designs.‚Äù33 What was true when \\nFowler wrote this in 2003 is just as accurate today. \\nAs we said previously, Bezos refers to reversible decisions as ‚Äútwo-way doors.‚Äù As he \\nsays, ‚ÄúIf you walk through and don‚Äôt like what you see on the other side, you can‚Äôt get \\nback to before. We can call these Type 1 decisions. But most decisions aren‚Äôt like that‚Äî\\nthey are changeable, reversible ‚Äîthey‚Äôre two -way doors.‚Äù Aim for two -way doors \\nwhenever possible. \\nGiven the pace of change‚Äîand the decoupling/modularization of technologies across \\nyour data architecture‚Äîalways strive to pick the best-of-breed solutions that work for \\ntoday. Also, be prepared to upgrade or adopt better practices as the landscape evolves. \\n \\n33 Fowler, ‚ÄúWho Needs an Architect?‚Äù'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 98, 'page_label': '99'}, page_content='Principles of Good Data Architecture \\n |  91 \\nPrinciple 8: Prioritize Security \\nEvery data engineer must assume responsibility for the security of the systems they \\nbuild and maintain. We focus now on two main ideas: zero-trust security and the shared \\nresponsibility security model. These align closely to a cloud-native architecture. \\nHardened-perimeter and zero-trust security models \\nTo define zero-trust security, it‚Äôs helpful to start by understanding the traditional hard-\\nperimeter security model and its limitations, as detailed in Google Cloud‚Äôs Five \\nPrinciples:34 \\nTraditional architectures place a lot of faith in perimeter security, crudely a hardened \\nnetwork perimeter with ‚Äútrusted things‚Äù inside and ‚Äúuntrusted things‚Äù outside. \\nUnfortunately, this approach has always been vulnerable to insider attacks, as well as \\nexternal threats such as spear phishing. \\nThe 1996 film Mission Impossible presents a perfect example of the hard -perimeter \\nsecurity model and its limitations. In the movie, the CIA hosts highly sensitive data on \\na storage system inside a room with extremely tight physical security. Ethan Hunt \\ninfiltrates CIA headquarters and exploits a human target to gain physical access to the \\nstorage system. Once inside the secure room, he can exfiltrate data with relative ease. \\nFor at least a decade, alarming media reports have made us aware of the growing \\nmenace of security breaches that exploit human targets inside hardened organizational \\nsecurity perimeters. Even as employees work on highly secure corporate networks, they \\nremain connected to the outside world through email and mobile devices. External \\nthreats effectively become internal threats. \\nIn a cloud-native environment, the notion of a hardened perimeter erodes further. All \\nassets are connected to the outside world to some degree. While virtual private cloud \\n(VPC) networks can be defined with no external connectivity, the API control plane \\nthat engineers use to define these networks still faces the internet. \\nThe shared responsibility model \\nAmazon emphasizes the shared responsibility model, which divides security into the \\nsecurity of the cloud and security in the cloud. AWS is responsible for the security of \\nthe cloud:35 \\n \\n34 Tom Grey, ‚Äú5 Principles for Cloud-Native Architecture‚ÄîWhat It Is and How to Master It,‚Äù Google Cloud \\nblog, June 19, 2019, https://oreil.ly/4NkGf. \\n35 Amazon Web Services, ‚ÄúSecurity in AWS WAF,‚Äù AWS WAF documentation, https://oreil.ly/rEFoU.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 99, 'page_label': '100'}, page_content='92  | Chapter 3: Designing Good Data Architecture \\nAWS is responsible for protecting the infrastructure that runs AWS services in the AWS \\nCloud. AWS also provides you with services that you can use securely. \\nAWS users are responsible for security in the cloud: \\nYour responsibility is determined by the AWS service that you use. You are also \\nresponsible for other factors including the sensitivity of your data, your organization‚Äôs \\nrequirements, and applicable laws and regulations. \\nIn general, all cloud providers operate on some form of this shared responsibility model. \\nThey secure their services according to published specifications. Still, it is ultimately \\nthe user‚Äôs responsibility to design a security model for their applications and data and \\nleverage cloud capabilities to realize this model. \\nData engineers as security engineers \\nIn the corporate world today, a command -and-control approach to security is quite \\ncommon, wherein security and networking teams manage perimeters and general \\nsecurity practices. The cloud pushes this responsibility out to engineers who are not \\nexplicitly in security roles. Because of this responsibility, in conjunction with more \\ngeneral erosion of the hard security perimeter, all data engineers should consider \\nthemselves security engineers. \\nFailure to assume these new implicit responsibilities can lead to dire consequences. \\nNumerous data breaches have resulted from the simple error of configuring Amazon \\nS3 buckets with public access. 36  Those who handle data must assume that they are \\nultimately responsible for securing it. \\nPrinciple 9: Embrace FinOps \\nLet‚Äôs start by considering a couple of definitions of FinOps. First, the FinOps \\nFoundation offers this:37 \\nFinOps is an evolving cloud financial management discipline and cultural practice that \\nenables organizations to get maximum business value by helping engineering, finance, \\ntechnology, and business teams to collaborate on data-driven spending decisions. In addition, \\nJ. R. Sorment and Mike Fuller provide the following definition in Cloud \\nFinOps:38 \\nThe term ‚ÄúFinOps‚Äù typically refers to the emerging professional movement that \\nadvocates a collaborative working relationship between DevOps and Finance, resulting \\n \\n36 Ericka Chickowski, ‚ÄúLeaky Buckets: 10 Worst Amazon S3 Breaches,‚Äù Bitdefender Business Insights blog, \\nJan 24, 2018, https://oreil.ly/pFEFO. \\n37 FinOps Foundation, ‚ÄúWhat Is FinOps,‚Äù https://oreil.ly/wJFVn. \\n38 J. R. Storment and Mike Fuller, Cloud FinOps (Sebastapol, CA: O‚ÄôReilly, 2019), https://oreil.ly/QV6vF.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 100, 'page_label': '101'}, page_content='Principles of Good Data Architecture \\n |  93 \\nin an iterative, data -driven management of infrastructure spending (i.e., lowering the \\nunit economics of cloud) while simultaneously increasing the cost efficiency and, \\nultimately, the profitability of the cloud environment. \\nThe cost structure of data has evolved dramatically during the cloud era. In an on -\\npremises setting, data systems are generally acquired with a capital expenditure \\n(described more in Chapter 4) for a new system every few years in an on -premises \\nsetting. Responsible parties have to balance their budget against desired compute and \\nstorage capacity. Overbuying entails wasted money, while underbuying means \\nhampering future data projects and driving significant personnel time to control system \\nload and data size ; underbuying may require faster technology refresh cycles, with \\nassociated extra costs. \\nIn the cloud era, most data systems are pay -as-you-go and readily scalable. Systems \\ncan run on a cost -per-query model, cost -per-processing-capacity model, or another \\nvariant of a pay -as-you-go model. This approach can be far more efficient than the \\ncapital expenditure approach. It is now possible to scale up for high performance, and \\nthen scale down to save money. However, the pay-as-you-go approach makes spending \\nfar more dynamic. The new challenge for data leaders is to manage budgets, priorities, \\nand efficiency. \\nCloud tooling necessitates a set of processes for managing spending and resources. In \\nthe past, data engineers thought in terms of performance engineering‚Äîmaximizing the \\nperformance for data processes on a fixed set of resources and buying adequate \\nresources for future needs. With FinOps, engineers need to learn to think about the cost \\nstructures of cloud systems. For example, what is the appropriate mix of AWS spot \\ninstances when running a distributed cluster? What is the most appropriate approach \\nfor running a sizable daily job in terms of cost -effectiveness and performance? When \\nshould the company switch from a pay-per-query model to reserved capacity? \\nFinOps evolves the operational monitoring model to monitor spending on an ongoing \\nbasis. Rather than simply monitor requests and CPU utilization for a web server, \\nFinOps might monitor the ongoing cost of serverless functions handling traffic, as well \\nas spikes in spending trigger alerts. Just as systems are designed to fail gracefully in \\nexcessive traffic, companies may consider adopting hard limits for spending, with \\ngraceful failure modes in response to spending spikes. \\nOps teams should also think in terms of cost attacks. Just as a distributed denial -\\nofservice (DDoS) attack can block access to a web server, many companies have \\ndiscovered to their chagrin that excessive downloads from S3 buckets can drive \\nspending through the roof and threaten a small startup with bankruptcy. When sharing \\ndata publicly, data teams can address these issues by setting requester-pays policies, or'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 101, 'page_label': '102'}, page_content='94  | Chapter 3: Designing Good Data Architecture \\nsimply monitoring for excessive data access spending and quickly removing access if \\nspending begins to rise to unacceptable levels.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 102, 'page_label': '103'}, page_content='Major Architecture Concepts  | \\n 95 \\nAs of this writing, FinOps is a recently formalized practice. The FinOps Foundation \\nwas started only in 2019. 39 However, we highly recommend you start thinking about \\nFinOps early, before you encounter high cloud bills. Start your journey with the FinOps \\nFoundation and O‚ÄôReilly‚Äôs Cloud FinOps. We also suggest that data engineers involve \\nthemselves in the community process of creating FinOps practices for data \\nengineering‚Äî in such a new practice area, a good deal of territory is yet to be mapped \\nout. \\nNow that you have a high-level understanding of good data architecture principles, let‚Äôs \\ndive a bit deeper into the major concepts you‚Äôll need to design and build good data \\narchitecture. \\nMajor Architecture Concepts \\nIf you follow the current trends in data, it seems like new types of data tools and \\narchitectures are arriving on the scene every week. Amidst this flurry of activity, we \\nmust not lose sight of the main goal of all of these architectures: to take data and \\ntransform it into something useful for downstream consumption. \\nDomains and Services \\nDomain: A sphere of knowledge, influence, or activity. The subject area to which the \\nuser applies a program is the domain of the software. \\n‚ÄîEric Evans40 \\nBefore diving into the components of the architecture, let‚Äôs briefly cover two terms \\nyou‚Äôll see come up very often: domain and services. A domain is the real-world subject \\narea for which you‚Äôre architecting. A service is a set of functionality whose goal is to \\naccomplish a task. For example, you might have a sales order-processing service whose \\ntask is to process orders as they are created. The sales order -processing service‚Äôs only \\njob is to process orders; it doesn‚Äôt provide other functionality, such as in ventory \\nmanagement or updating user profiles. \\nA domain can contain multiple services. For example, you might have a sales domain \\nwith three services: orders, invoicing, and products. Each service has particular tasks \\nthat support the sales domain. Other domains may also share services ( Figure 3-3). In \\nthis case, the accounting domain is responsible for basic accounting functions: \\ninvoicing, payroll, and accounts receivable (AR). Notice the accounting domain shares \\n \\n39 ‚ÄúFinOps Foundation Soars to 300 Members and Introduces New Partner Tiers for Cloud Service Providers and \\nVendors,‚Äù Business Wire, June 17, 2019, https://oreil.ly/XcwYO. \\n40 Eric Evans, Domain-Driven Design Reference: Definitions and Pattern Summaries (March 2015), \\nhttps://oreil.ly/pQ9oq.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 103, 'page_label': '104'}, page_content='96  | Chapter 3: Designing Good Data Architecture \\nthe invoice service with the sales domain since a sale generates an invoice, and \\naccounting must keep track of invoices to ensure that payment is received. Sales and \\naccounting own their respective domains. \\n \\nFigure 3-3. Two domains (sales and accounting) share a common service (invoices), \\nand sales and accounting own their respective domains \\nWhen thinking about what constitutes a domain, focus on what the domain represents \\nin the real world and work backward. In the preceding example, the sales domain \\nshould represent what happens with the sales function in your company. When \\narchitecting the sales domain, avoid cookie-cutter copying and pasting from what other \\ncompanies do. Your company‚Äôs sales function likely has unique aspects that require \\nspecific services to make it work the way your sales team expects. Identify what should \\ngo in the domai n. When determining what the domain should encompass and what \\nservices to include, the best advice is to simply go and talk with users and stakeholders, \\nlisten to what they‚Äôre saying, and build the services that will help them do their job. \\nAvoid the classic trap of architecting in a vacuum. \\nDistributed Systems, Scalability, and Designing for \\nFailure \\nThe discussion in this section is related to our second and third principles of data \\nengineering architecture discussed previously: plan for failure and architect for \\nscalability. As data engineers, we‚Äôre interested in four closely related characteristics of \\ndata systems (availability and reliability were mentioned previously, but we reiterate \\nthem here for completeness): \\nScalability \\nAllows us to increase the capacity of a system to improve performance and handle \\nthe demand. For example, we might want to scale a system to handle a high rate \\nof queries or process a huge data set. \\nElasticity \\nThe ability of a scalable system to scale dynamically; a highly elastic system can \\nautomatically scale up and down based on the current workload. Scaling up is \\ncritical as demand increases, while scaling down saves money in a cloud'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 104, 'page_label': '105'}, page_content='Major Architecture Concepts  | \\n 97 \\nenvironment. Modern systems sometimes scale to zero, meaning they can \\nautomatically shut down when idle. \\nAvailability \\nThe percentage of time an IT service or component is in an operable state. \\nReliability \\nThe system‚Äôs probability of meeting defined standards in performing its intended \\nfunction during a specified interval. \\nSee PagerDuty‚Äôs ‚ÄúWhy Are Availability and Reliability Crucial?‚Äù \\nweb page  for definitions and background on availability and \\nreliability. \\nHow are these characteristics related? If a system fails to meet performance \\nrequirements during a specified interval, it may become unresponsive. Thus low \\nreliability can lead to low availability. On the other hand, dynamic scaling helps ensure \\nadequate p erformance without manual intervention from engineers ‚Äîelasticity \\nimproves reliability. \\nScalability can be realized in a variety of ways. For your services and domains, does a \\nsingle machine handle everything? A single machine can be scaled vertically; you can \\nincrease resources (CPU, disk, memory, I/O). But there are hard limits to possible \\nresources on a single machine. Also, what happens if this machine dies? Given enough \\ntime, some components will eventually fail. What‚Äôs your plan for backup and failover? \\nSingle machines generally can‚Äôt offer high availability and reliability. We utilize a  \\ndistributed system to realize higher overall scaling capacity and increased availability \\nand reliability. Horizontal scaling allows you to add more machines to satisfy load and \\nresource requirements (Figure 3-4). Common horizontally scaled systems have a leader \\nnode that acts as the main point of contact for the instantiation, progress, and \\ncompletion of workloads. When a workload is started, the leader node distributes tasks \\nto the worker nodes within its system, completing the tasks and returning the results to \\nthe leader node. Typical modern distributed architectures also build in redundancy. Data \\nis replicated so that if a machine dies, the other machines can pick up where the missing \\nserver left off; the cluster may add more machines to restore capacity. \\nDistributed systems are widespread in the various data technologies you‚Äôll use across \\nyour architecture. Almost every cloud data warehouse object storage system you use \\nhas some notion of distribution under the hood. Management details of the distributed \\nsystem are typically abstracted away, allowing you to focus on high-level architecture \\ninstead of low -level plumbing. However, we highly recommend that you learn more \\nabout distributed systems because these details can be extremely helpful in'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 105, 'page_label': '106'}, page_content='98  | Chapter 3: Designing Good Data Architecture \\nunderstanding and improving the performance of your pipelines; Martin Kleppmann‚Äôs \\nDesigning Data-Intensive Applications (O‚ÄôReilly) is an excellent resource. \\n \\nFigure 3-4. A simple horizontal distributed system utilizing a leader-follower \\narchitecture, with one leader node and three worker nodes \\nTight Versus Loose Coupling: Tiers, Monoliths, and \\nMicroservices \\nWhen designing a data architecture, you choose how much interdependence you want \\nto include within your various domains, services, and resources. On one end of the \\nspectrum, you can choose to have extremely centralized dependencies and workflows. \\nEvery part of a domain and service is vitally dependent upon every other domain and \\nservice. This pattern is known as tightly coupled. \\nOn the other end of the spectrum, you have decentralized domains and services that do \\nnot have strict dependence on each other, in a pattern known as loose coupling. In a \\nloosely coupled scenario, it‚Äôs easy for decentralized teams to build systems whose data \\nmay not be usable by their peers. Be sure to assign common standards, ownership, \\nresponsibility, and accountability to the teams owning their respective domains and \\nservices. Designing ‚Äúgood‚Äù data architecture relies on trade-offs between the tight and \\nloose coupling of domains and services. \\nIt‚Äôs worth noting that many of the ideas in this section originate in software \\ndevelopment. We‚Äôll try to retain the context of these big ideas‚Äô original intent and \\nspirit‚Äî keeping them agnostic of data ‚Äîwhile later explaining some differences you \\nshould be aware of when applying these concepts to data specifically. \\nArchitecture tiers \\nAs you develop your architecture, it helps to be aware of architecture tiers. Your \\narchitecture has layers‚Äîdata, application, business logic, presentation, and so forth ‚Äî\\nand you need to know how to decouple these layers. Because tight coupling of \\nmodalities presents obvious vulnerabilities, keep in mind how you structure the layers'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 106, 'page_label': '107'}, page_content='Major Architecture Concepts  | \\n 99 \\nof your architecture to achieve maximum reliability and flexibility. Let‚Äôs look at single-\\ntier and multitier architecture. \\nSingle tier.    In a single-tier architecture, your database and application are tightly \\ncoupled, residing on a single server (Figure 3-5). This server could be your laptop or a \\nsingle virtual machine (VM) in the cloud. The tightly coupled nature means if the \\nserver, the database, or the application fails, the entire architecture fails. While single-\\ntier architectures are good for prototypi ng and development, they are not advised for \\nproduction environments because of the obvious failure risks. \\n \\nFigure 3-5. Single-tier architecture \\nEven when single -tier architectures build in redundancy (for example, a failover \\nreplica), they present significant limitations in other ways. For instance, it is often \\nimpractical (and not advisable) to run analytics queries against production application \\ndatabases. Doing so risks overwhelming the database and causing the application to \\nbecome unavailable. A single -tier architecture is fine for testing systems on a local \\nmachine but is not advised for production uses. \\nMultitier.    The challenges of a tightly coupled single-tier architecture are solved by \\ndecoupling the data and application. A multitier (also known as n-tier) architecture is \\ncomposed of separate layers: data, application, business logic, presentation, etc. These \\nlayers are bottom -up and hierarchical, meaning the lower layer isn‚Äôt necessarily \\ndependent on the upper layers; the upper layers depend on the lower layers. The notion \\nis to separate data from the application, and application from the presentation. \\nA common multitier architecture is a three-tier architecture, a widely used clientserver \\ndesign. A three-tier architecture consists of data, application logic, and presentation \\ntiers ( Figure 3 -6). Each tier is isolated from the other, allowing for separation of \\nconcerns. With a three-tier architecture, you‚Äôre free to use whatever technologies you \\nprefer within each tier without the need to be monolithically focused.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 107, 'page_label': '108'}, page_content='100  | Chapter 3: Designing Good Data Architecture \\n \\nFigure 3-6. A three-tier architecture \\nWe‚Äôve seen many single-tier architectures in production. Single-tier architectures offer \\nsimplicity but also severe limitations. Eventually, an organization or application \\noutgrows this arrangement; it works well until it doesn‚Äôt. For instance, in a single -tier \\narchitecture, the data and logic layers share and compete for resources (disk, CPU, and \\nmemory) in ways that are simply avoided in a multitier architecture. Resources are \\nspread across various tiers. Data engineers should use tiers to evaluate their layered \\narchitecture and the way dependencies are handled. Again, start simple and bake in \\nevolution to additional tiers as your architecture becomes more complex. \\nIn a multitier architecture, you need to consider separating your layers and the way \\nresources are shared within layers when working with a distributed system. Distributed \\nsystems under the hood power many technologies you‚Äôll encounter across the data \\nengineering lifecycle. First, think about whether you want resource contention with \\nyour nodes. If not, exercise a shared-nothing architecture: a single node handles each \\nrequest, meaning other nodes do not share resources such as memory, disk, or CPU \\nwith thi s node or with each other. Data and resources are isolated to the node. \\nAlternatively, various nodes can handle multiple requests and share resources but at the \\nrisk of resource contention. Another consideration is whether nodes should share the \\nsame disk and memory accessible by all nodes. This is called a shared disk architecture \\nand is common when you want shared resources if a random node failure \\noccurs. \\nMonoliths \\nThe general notion of a monolith includes as much as possible under one roof; in its \\nmost extreme version, a monolith consists of a single codebase running on a single \\nmachine that provides both the application logic and user interface. \\nCoupling within monoliths can be viewed in two ways: technical coupling and domain \\ncoupling. Technical coupling refers to architectural tiers, while domain coupling refers \\nto the way domains are coupled together. A monolith has varying degrees of coupling'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 108, 'page_label': '109'}, page_content='Major Architecture Concepts  | \\n 101 \\namong technologies and domains. You could have an application with various layers \\ndecoupled in a multitier architecture but still share multiple domains. Or, you could \\nhave a single-tier architecture serving a single domain. \\nThe tight coupling of a monolith implies a lack of modularity of its components. \\nSwapping out or upgrading components in a monolith is often an exercise in trading \\none pain for another. Because of the tightly coupled nature, reusing components across \\nthe a rchitecture is difficult or impossible. When evaluating how to improve a \\nmonolithic architecture, it‚Äôs often a game of whack -a-mole: one component is \\nimproved, often at the expense of unknown consequences with other areas of the \\nmonolith. \\nData teams will often ignore solving the growing complexity of their monolith, letting \\nit devolve into a big ball of mud. \\nChapter 4 provides a more extensive discussion comparing monoliths to distributed \\ntechnologies. We also discuss the distributed monolith, a strange hybrid that emerges \\nwhen engineers build distributed systems with excessive tight coupling. \\nMicroservices \\nCompared with the attributes of a monolith ‚Äîinterwoven services, centralization, and \\ntight coupling among services ‚Äîmicroservices are the polar opposite. Microservices \\narchitecture comprises separate, decentralized, and loosely coupled services. Each \\nservice has a specific function and is decoupled from other services operating within \\nits domain. If one service temporarily goes down, it won‚Äôt affect the ability of other \\nservices to continue functioning. \\nA question that comes up often is how to convert your monolith into many \\nmicroservices (Figure 3-7). This completely depends on how complex your monolith \\nis and how much effort it will be to start extracting services out of it. It‚Äôs entirely \\npossible that your monolith cannot be broken apart, in which case, you‚Äôll want to start \\ncreating a new parallel architecture that has the services decoupled in a microservices-\\nfriendly manner. We don‚Äôt suggest an entire refactor but instead break out services. The \\nmonolith didn‚Äôt arrive overnight and is a technology issue as an organizational one. Be \\nsure you get buy-in from stakeholders of the monolith if you plan to break it apart.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 109, 'page_label': '110'}, page_content='102  | Chapter 3: Designing Good Data Architecture \\n \\nFigure 3-7. An extremely monolithic architecture runs all functionality inside a single \\ncodebase, potentially colocating a database on the same host server \\nIf you‚Äôd like to learn more about breaking apart a monolith, we suggest reading the \\nfantastic, pragmatic guide Software Architecture: The Hard Parts by Neal Ford et al. \\n(O‚ÄôReilly). \\nConsiderations for data architecture \\nAs we mentioned at the start of this section, the concepts of tight versus loose coupling \\nstem from software development, with some of these concepts dating back over 20 \\nyears. Though architectural practices in data are now adopting those from software \\ndevelopment, it‚Äôs still common to see very monolithic, tightly coupled data \\narchitectures. Some of this is due to the nature of existing data technologies and the \\nway they integrate. \\nFor example, data pipelines might consume data from many sources ingested into a \\ncentral data warehouse. The central data warehouse is inherently monolithic. A move \\ntoward a microservices equivalent with a data warehouse is to decouple the workflow \\nwith domain-specific data pipelines connecting to corresponding domain-specific data \\nwarehouses. For example, the sales data pipeline connects to the sales -specific data \\nwarehouse, and the inventory and product domains follow a similar pattern. \\nRather than dogmatically preach microservices over monoliths (among other \\narguments), we suggest you pragmatically use loose coupling as an ideal, while \\nrecognizing the state and limitations of the data technologies you‚Äôre using within your \\ndata architecture. Incorporate reversible technology choices that allow for modularity \\nand loose coupling whenever possible. \\nAs you can see in Figure 3-7, you separate the components of your architecture into \\ndifferent layers of concern in a vertical fashion. While a multitier architecture solves \\nthe technical challenges of decoupling shared resources, it does not address the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 110, 'page_label': '111'}, page_content='Major Architecture Concepts  | \\n 103 \\ncomplexity of sharing domains. Along the lines of single versus multitiered \\narchitecture, you should also consider how you separate the domains of your data \\narchitecture. For example, your analyst team might rely on data from sales and \\ninventory. The sales  and inventory domains are different and should be viewed as \\nseparate. \\nOne approach to this problem is centralization: a single team is responsible for \\ngathering data from all domains and reconciling it for consumption across the \\norganization. (This is a common approach in traditional data warehousing.) Another \\napproach is the data mesh. With the data mesh, each software team is responsible for \\npreparing its data for consumption across the rest of the organization. We‚Äôll say more \\nabout the data mesh later in this chapter. \\nOur advice: monoliths aren‚Äôt necessarily bad, and it might make sense to start with one \\nunder certain conditions. Sometimes you need to move fast, and it‚Äôs much simpler to \\nstart with a monolith. Just be prepared to break it into smaller pieces eventually; don‚Äôt \\nget too comfortable. \\nUser Access: Single Versus Multitenant \\nAs a data engineer, you have to make decisions about sharing systems across multiple \\nteams, organizations, and customers. In some sense, all cloud services are multitenant, \\nalthough this multitenancy occurs at various grains. For example, a cloud compute \\ninstance is usually on a shared server, but the VM itself provides some degree of \\nisolation. Object storage is a multitenant system, but cloud vendors guarantee security \\nand isolation so long as customers configure their permissions correctly. \\nEngineers frequently need to make decisions about multitenancy at a much smaller \\nscale. For example, do multiple departments in a large company share the same data \\nwarehouse? Does the organization share data for multiple large customers within the \\nsame table? \\nWe have two factors to consider in multitenancy: performance and security. With \\nmultiple large tenants within a cloud system, will the system support consistent \\nperformance for all tenants, or will there be a noisy neighbor problem? (That is, will \\nhigh usage from one tenant degrade performance for other tenants?) Regarding security, \\ndata from different tenants must be properly isolated. When a company has multiple \\nexternal customer tenants, these tenants should not be aware of one another, and \\nengineers must prevent data leakage. Strategies for data isolation vary by system. For \\ninstance, it is often perfectly acceptable to use multitenant tables and isolate data \\nthrough views. However, you must make certain that these views cannot leak data. Read \\nvendor or project documentation to understand appropriate strategies and risks.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 111, 'page_label': '112'}, page_content='104  | Chapter 3: Designing Good Data Architecture \\nEvent-Driven Architecture \\nYour business is rarely static. Things often happen in your business, such as getting a \\nnew customer, a new order from a customer, or an order for a product or service. These \\nare all examples of events that are broadly defined as something that happened, \\ntypically a change in the state of something. For example, a new order might be created \\nby a customer, or a customer might later make an update to this order. An event-driven \\nworkflow (Figure 3-8) encompasses the ability to create, update, and asynchronousl y \\nmove events across various parts of the data engineering lifecycle. This workflow boils \\ndown to three main areas: event production, routing, and consumption. An event must \\nbe produced and routed to something that consumes it without tightly coupled \\ndependencies among the producer, event router, and \\nconsumer. \\n \\nFigure 3-8. In an event-driven workflow, an event is produced, routed, and then \\nconsumed \\nAn event -driven architecture ( Figure 3 -9) embraces the event -driven workflow and \\nuses this to communicate across various services. The advantage of an event -driven \\narchitecture is that it distributes the state of an event across multiple services. This is \\nhelpful if a service goes offline, a nod e fails in a distributed system, or you‚Äôd like \\nmultiple consumers or services to access the same events. Anytime you have loosely \\ncoupled services, this is a candidate for event -driven architecture. Many of the \\nexamples we describe later in this chapter incorporate some form of event -driven \\narchitecture. \\nYou‚Äôll learn more about event-driven streaming and messaging systems in Chapter 5. \\n \\nFigure 3-9. In an event-driven architecture, events are passed between loosely \\ncoupled services \\nBrownfield Versus Greenfield Projects \\nBefore you design your data architecture project, you need to know whether you‚Äôre \\nstarting with a clean slate or redesigning an existing architecture. Each type of project'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 112, 'page_label': '113'}, page_content='Major Architecture Concepts  | \\n 105 \\nrequires assessing trade -offs, albeit with different considerations and approaches. \\nProjects roughly fall into two buckets: brownfield and greenfield. \\nBrownfield projects \\nBrownfield projects often involve refactoring and reorganizing an existing architecture \\nand are constrained by the choices of the present and past. Because a key part of \\narchitecture is change management, you must figure out a way around these limitations \\nand design a path f orward to achieve your new business and technical objectives. \\nBrownfield projects require a thorough understanding of the legacy architecture and \\nthe interplay of various old and new technologies. All too often, it‚Äôs easy to criticize a \\nprior team‚Äôs work and decisions, but it is far better to dig deep, ask questions, and \\nunderstand why decisions were made. Empathy and context go a long way in helping \\nyou diagnose problems with the existing architecture, identify opportunities, and \\nrecognize pitfalls. \\nYou‚Äôll need to introduce your new architecture and technologies and deprecate the old \\nstuff at some point. Let‚Äôs look at a couple of popular approaches. Many teams jump \\nheadfirst into an all-at-once or big-bang overhaul of the old architecture, often figuring \\nout deprecation as they go. Though popular, we don‚Äôt advise this approach because of \\nthe associated risks and lack of a plan. This path often leads to disaster, with many \\nirreversible and costly decisions. Your job is to make reversible, high-ROI \\ndecisions. \\nA popular alternative to a direct rewrite is the strangler pattern: new systems slowly and \\nincrementally replace a legacy architecture‚Äôs components. 41  Eventually, the legacy \\narchitecture is completely replaced. The attraction to the strangler pattern is its targeted \\nand surgical approach of deprecating one piece of a system at a time. This allows for \\nflexible and reversible decisions while assessing the  impact of the deprecation on \\ndependent systems. \\nIt‚Äôs important to note that deprecation might be ‚Äúivory tower‚Äù advice and not practical \\nor achievable. Eradicating legacy technology or architecture might be impossible if \\nyou‚Äôre at a large organization. Someone, somewhere, is using these legacy components. \\nAs someone once said, ‚ÄúLegacy is a condescending way to describe something that \\nmakes money.‚Äù \\nIf you can deprecate, understand there are numerous ways to deprecate your old \\narchitecture. It is critical to demonstrate value on the new platform by gradually \\n \\n41 Martin Fowler, ‚ÄúStranglerFigApplication,‚Äù June 29, 2004, https://oreil.ly/PmqxB.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 113, 'page_label': '114'}, page_content='106  | Chapter 3: Designing Good Data Architecture \\nincreasing its maturity to show evidence of success and then follow an exit plan to shut \\ndown old systems. \\nGreenfield projects \\nOn the opposite end of the spectrum, a greenfield project allows you to pioneer a fresh \\nstart, unconstrained by the history or legacy of a prior architecture. Greenfield projects \\ntend to be easier than brownfield projects, and many data architects and engineers find \\nthem more fun! You have the opportunity to try  the newest and coolest tools and \\narchitectural patterns. What could be more exciting? \\nYou should watch out for some things before getting too carried away. We see teams \\nget overly exuberant with shiny object syndrome. They feel compelled to reach for the \\nlatest and greatest technology fad without understanding how it will impact the value \\nof the project. There‚Äôs also a temptation to do resume-driven development, stacking up \\nimpressive new technologies without prioritizing the project‚Äôs ultimate goals.42Always \\nprioritize requirements over building something cool. \\nWhether you‚Äôre working on a brownfield or greenfield project, always focus on the \\ntenets of ‚Äúgood‚Äù data architecture. Assess trade -offs, make flexible and reversible \\ndecisions, and strive for positive ROI. \\nNow, we‚Äôll look at examples and types of architectures‚Äîsome established for decades \\n(the data warehouse), some brand -new (the data lakehouse), and some that quickly \\ncame and went but still influence current architecture patterns (Lambda architecture). \\nExamples and Types of Data Architecture \\nBecause data architecture is an abstract discipline, it helps to reason by example. In this \\nsection, we outline prominent examples and types of data architecture that are popular \\ntoday. Though this set of examples is by no means exhaustive, the intention is to expose \\nyou to some of the most common data architecture patterns and to get you thinking \\nabout the requisite flexibility and trade -off analysis needed when designing a good \\narchitecture for your use case. \\nData Warehouse \\nA data warehouse is a central data hub used for reporting and analysis. Data in a data \\nwarehouse is typically highly formatted and structured for analytics use cases. It‚Äôs \\namong the oldest and most well-established data architectures. \\n \\n42 Mike Loukides, ‚ÄúResume Driven Development,‚Äù O‚ÄôReilly Radar, October 13, 2004, https://oreil.ly/BUHa8.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 114, 'page_label': '115'}, page_content='Major Architecture Concepts  | \\n 107 \\nIn 1989, Bill Inmon originated the notion of the data warehouse, which he described as \\n‚Äúa subject -oriented, integrated, nonvolatile, and time -variant collection of data in \\nsupport of management‚Äôs decisions.‚Äù43 Though technical aspects of the data warehouse \\nhave evolved significantly, we feel this original definition still holds its weight today. \\nIn the past, data warehouses were widely used at enterprises with significant budgets \\n(often in the millions of dollars) to acquire data systems and pay internal teams to \\nprovide ongoing support to maintain the data warehouse. This was expensive and labor-\\nintensive. Since then, the scalable, pay -as-you-go model has made cloud data \\nwarehouses accessible even to tiny companies. Because a third-party provider manages \\nthe data warehouse infrastructure, companies can do a lot more with fewer people, even \\nas the complexity of their data grows. \\nIt‚Äôs worth noting two types of data warehouse architecture: organizational and \\ntechnical. The organizational data warehouse architecture organizes data associated \\nwith certain business team structures and processes. The technical data warehouse \\narchitecture reflects the technical nature of the data warehouse, such as MPP. A \\ncompany can have a data warehouse without an MPP system or run an MPP system \\nthat is not organized as a data warehouse. However, the technical and organizational \\narchitectures have existed in a virtuous cycle and are frequently identified with each \\nother. \\n \\n43 H. W. Inmon, Building the Data Warehouse (Hoboken: Wiley, 2005).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 115, 'page_label': '116'}, page_content='108  | Chapter 3: Designing Good Data Architecture \\nThe organizational data warehouse architecture has two main characteristics: \\nSeparates online analytical processing (OLAP) from production databases (online \\ntransaction processing) \\nThis separation is critical as businesses grow. Moving data into a separate physical \\nsystem directs load away from production systems and improves analytics \\nperformance. \\nCentralizes and organizes data \\nTraditionally, a data warehouse pulls data from application systems by using ETL. \\nThe extract phase pulls data from source systems. The transformation phase cleans \\nand standardizes data, organizing and imposing business logic in a highly modeled \\nform. (Chapter 8 covers transformations and data models.) The load phase pushes \\ndata into the data warehouse target database system. Data is loaded into multiple \\ndata marts that serve the analytical needs for specific lines or business and \\ndepartments. Figure 3-10 shows the general workflow. The data warehouse and \\nETL go hand in hand with specific business structures, including DBA and ETL \\ndeveloper teams that implement the direction of business leaders to ensure that \\ndata for reporting and analytics corresponds to business processes. \\n \\nFigure 3-10. Basic data warehouse with ETL \\nRegarding the technical data warehouse architecture, the first MPP systems in the late \\n1970s became popular in the 1980s. MPPs support essentially the same SQL semantics \\nused in relational application databases. Still, they are optimized to scan massive \\namounts of data in parallel and thus allow high-performance aggregation and statistical \\ncalculations. In recent years, MPP systems have increasingly shifted from a row-based \\nto a columnar architecture to facilitate even larger data and queries, especially in cloud \\ndata warehouses. MPPs are indispensable for running performant queries for large \\nenterprises as data and reporting needs grow. \\nOne variation on ETL is ELT. With the ELT data warehouse architecture, data gets \\nmoved more or less directly from production systems into a staging area in the data \\nwarehouse. Staging in this setting indicates that the data is in a raw form. Rather than \\nusing an external system, transformations are handled directly in the data warehouse.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 116, 'page_label': '117'}, page_content='Examples and Types of Data Architecture \\n |  109 \\nThe intention is to take advantage of the massive computational power of cloud data \\nwarehouses and data processing tools. Data is processed in batches, and transformed \\noutput is written into tables and views for analytics. Figure 3-11 shows the general \\nprocess. ELT is also popular in a streaming arrangement, as events are streamed from \\na CDC process, stored in a staging area, and then subsequently transformed within the \\ndata warehouse. \\n \\nFigure 3-11. ELT‚Äîextract, load, and transform \\nA second version of ELT was popularized during big data growth in the Hadoop \\necosystem. This is transform-on-read ELT, which we discuss in ‚ÄúData Lake‚Äù on page \\n101. \\nThe cloud data warehouse \\nCloud data warehouses  represent a significant evolution of the on -premises data \\nwarehouse architecture and have thus led to significant changes to the organizational \\narchitecture. Amazon Redshift kicked off the cloud data warehouse revolution. Instead \\nof needing to appropriately size an MPP system for the next several years and sign a \\nmultimillion-dollar contract to procure the system, companies had the option of \\nspinning up a Redshift cluster on demand, scaling it up over time as data and analytics \\ndemand grew. They could even spin up new Redshift clusters on demand to serve \\nspecific workloads and quickly delete clusters when they were no longer needed. \\nGoogle BigQuery, Snowflake, and other competitors popularized the idea of separating \\ncompute from storage. In this architecture, data is housed in object storage, allowing \\nvirtually limitless storage. This also gives users the option to spin up computing power \\non demand, providing ad hoc big data capabilities without the long -term cost of \\nthousands of nodes. \\nCloud data warehouses expand the capabilities of MPP systems to cover many big data \\nuse cases that required a Hadoop cluster in the very recent past. They can readily \\nprocess petabytes of data in a single query. They typically support data structures that \\nallow the storage of tens of megabytes of raw text data per row or extremely rich and \\ncomplex JSON documents. As cloud data warehouses (and data lakes) mature, the line \\nbetween the data warehouse and the data lake will continue to blur.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 117, 'page_label': '118'}, page_content='110  | Chapter 3: Designing Good Data Architecture \\nSo significant is the impact of the new capabilities offered by cloud data warehouses \\nthat we might consider jettisoning the term data warehouse altogether. Instead, these \\nservices are evolving into a new data platform with much broader capabilities than \\nthose offered by a traditional MPP system. \\nData marts \\nA data mart is a more refined subset of a warehouse designed to serve analytics and \\nreporting, focused on a single suborganization, department, or line of business; every \\ndepartment has its own data mart, specific to its needs. This is in contrast to the full \\ndata warehouse that serves the broader organization or business. \\nData marts exist for two reasons. First, a data mart makes data more easily accessible \\nto analysts and report developers. Second, data marts provide an additional stage of \\ntransformation beyond that provided by the initial ETL or ELT pipelines. This can \\nsignificantly improve performance if reports or analytics queries require complex joins \\nand aggregations of data, especially when the raw data is large. Transform processes \\ncan populate the data mart with joined and aggregated data to improve performance for \\nlive queries. Figure 3 -12 shows the general workflow. We discuss data marts, and \\nmodeling data for data marts, in Chapter 8. \\n \\nAmong the most popular architectures that appeared during the big data era is the data \\nlake. Instead of imposing tight structural limitations on data, why not simply dump all \\nof your data ‚Äîstructured and unstructured ‚Äîinto a central location? The data lake \\npromised to be a democratizing force, liberating the business to drink from a fountain \\nof li mitless data. The first -generation data lake, ‚Äúdata lake 1.0,‚Äù made solid \\ncontributions but generally failed to deliver on its promise. \\nData lake 1.0 started with HDFS. As the cloud grew in popularity, these data lakes \\nmoved to cloud-based object storage, with extremely cheap storage costs and virtually \\nlimitless storage capacity. Instead of relying on a monolithic data warehouse where \\nstorage and compute are tightly coupled, the data lake allows an immense amount of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 118, 'page_label': '119'}, page_content='Examples and Types of Data Architecture \\n |  111 \\ndata of any size and type to be stored. When this data needs to be queried or \\ntransformed, you have access to nearly unlimited computing power by spinning up a \\ncluster on demand, and you can pick your favorite data -processing technology for the \\ntask at hand‚ÄîMapReduce, Spark, Ray, Presto, Hive, etc. \\nDespite the promise and hype, data lake 1.0 had serious shortcomings. The data lake \\nbecame a dumping ground; terms such as data swamp, dark data, and WORN were \\ncoined as once-promising data projects failed. Data grew to unmanageable sizes, with \\nlittle in the way of schema management, data cataloging, and discovery tools. In \\naddition, the original data lake concept was essentially write -only, creating huge \\nheadaches with the arrival of regulations such as GDPR that required targeted deletion \\nof user records. \\nProcessing data was also challenging. Relatively banal data transformations such as \\njoins were a huge headache to code as MapReduce jobs. Later frameworks such as Pig \\nand Hive somewhat improved the situation for data processing but did little to address \\nthe basic problems of data management. Simple data manipulation language (DML) \\noperations common in SQL‚Äîdeleting or updating rows‚Äîwere painful to implement, \\ngenerally achieved by creating entirely new tables. While big data engineers radiated a \\nparticular disdain for their counterparts in data warehousing, the latter could point out \\nthat data warehouses provided basic data management capabilities out of the box, and \\nthat SQL was an efficient tool for writing complex, performant queries and \\ntransformations. \\nData lake 1.0 also failed to deliver on another core promise of the big data movement. \\nOpen source software in the Apache ecosystem was touted as a means to avoid \\nmultimillion-dollar contracts for proprietary MPP systems. Cheap, off -the-shelf \\nhardware would replace custom vendor solutions. In reality, big data costs ballooned \\nas the complexities of managing Hadoop clusters forced companies to hire large teams \\nof engineers at high salaries. Companies often chose to purchase licensed, customized \\nversions of Hadoop from vendors to avoid the exposed wires and sharp edges of the \\nraw Apache codebase and acquire a set of scaffolding tools to make Hadoop more user-\\nfriendly. Even companies that avoided managing Hadoop clusters using cloud storage \\nhad to spend big on talent to write MapReduce jobs. \\nWe should be careful not to understate the utility and power of first -generation data \\nlakes. Many organizations found significant value in data lakes ‚Äîespecially huge, \\nheavily data-focused Silicon Valley tech companies like Netflix and Facebook. These \\ncompanies had the resources to build successful data practices and create their custom \\nHadoop-based tools and enhancements. But for many organizations, data lakes turned \\ninto an internal superfund site of waste, disappointment, and spiraling costs.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 119, 'page_label': '120'}, page_content='112  | Chapter 3: Designing Good Data Architecture \\nConvergence, Next-Generation Data Lakes, and the \\nData Platform \\nIn response to the limitations of first-generation data lakes, various players have sought \\nto enhance the concept to fully realize its promise. For example, Databricks introduced \\nthe notion of a data lakehouse . The lakehouse incorporates the controls, data \\nmanagement, and data structures found in a data warehouse while still housing data in \\nobject storage and supporting a variety of query and transformation engines. In \\nparticular, the data lakehouse supports atomicity, consistency, isolation, and durability \\n(ACID) transactions, a big departure from the original data lake, where you simply pour \\nin data and never update or delete it. The term data lakehouse suggests a convergence \\nbetween data lakes and data warehouses. \\nThe technical architecture of cloud data warehouses has evolved to be very similar to a \\ndata lake architecture. Cloud data warehouses separate compute from storage, support \\npetabyte-scale queries, store a variety of unstructured data and semistructured objects, \\nand integrate with advanced processing technologies such as Spark or Beam. \\nWe believe that the trend of convergence will only continue. The data lake and the data \\nwarehouse will still exist as different architectures. In practice, their capabilities will \\nconverge so that few users will notice a boundary between them in their day -today \\nwork. We now see several vendors offering data platforms that combine data lake and \\ndata warehouse capabilities. From our perspective, AWS, Azure, Google Cloud, \\nSnowflake, and Databricks are class leaders, each offering a constellation of tightly \\nintegrated tools for working with data, running the gamut from relational to completely \\nunstructured. Instead of choosing between a data lake or data warehouse architecture, \\nfuture data engineers will have the option to choose a converged data platform based \\non a variety of factors, including vendor, ecosystem, and relative openness. \\nModern Data Stack \\nThe modern data stack  (Figure 3-13) is currently a trendy analytics architecture that \\nhighlights the type of abstraction we expect to see more widely used over the next \\nseveral years. Whereas past data stacks relied on expensive, monolithic toolsets, the \\nmain objective of the modern data stack is to use cloud-based, plug-and-play, easy-to-\\nuse, off-the-shelf components to create a modular and cost-effective data architecture. \\nThese components include data pipelines, storage, transformation, data \\nmanagement/governance, monitoring, visualization, and exploration. The domain is \\nstill in flux, and the specific tools are changing and evolving rapidly, but the core aim \\nwill remain the same: to reduce complexity and increase modularization. Note that the \\nnotion of a modern data stack integrates nicely with the converged data platform idea \\nfrom the previous section.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 120, 'page_label': '121'}, page_content='Examples and Types of Data Architecture \\n |  113 \\n \\nFigure 3-13. Basic components of the modern data stack \\nKey outcomes of the modern data stack are self-service (analytics and pipelines), agile \\ndata management, and using open source tools or simple proprietary tools with clear \\npricing structures. Community is a central aspect of the modern data stack as well. \\nUnlike products of the past that had releases and roadmaps largely hidden from users, \\nprojects and companies operating in the modern data stack space typically have strong \\nuser bases and active communities that participate in the development by using the \\nproduct early, suggesting features, and submitting pull requests to improve the code. \\nRegardless of where ‚Äúmodern‚Äù goes (we share our ideas in Chapter 11), we think the \\nkey concept of plug -and-play modularity with easy -to-understand pricing and \\nimplementation is the way of the future. Especially in analytics engineering, the modern \\ndata stack is and will continue to be the default choice of data architecture. Throughout \\nthe book, the architecture we reference contains pieces of the modern data stack, such \\nas cloud-based and plug-and-play modular components. \\nLambda Architecture \\nIn the ‚Äúold days‚Äù (the early to mid -2010s), the popularity of working with streaming \\ndata exploded with the emergence of Kafka as a highly scalable message queue and \\nframeworks such as Apache Storm and Samza for streaming/real-time analytics. These \\ntechnologies allowed companies to perform new types of analytics and modeling on \\nlarge amounts of data, user aggregation and ranking, and product recommendations. \\nData engineers needed to figure out how to reconcile batch and streaming data into a \\nsingle architecture. The Lambda architecture was one of the early popular responses to \\nthis problem. \\nIn a Lambda architecture (Figure 3-14), you have systems operating independently of \\neach other‚Äîbatch, streaming, and serving. The source system is ideally immutable and \\nappend-only, sending data to two destinations for processing: stream and batch. In -\\nstream processing intends to serve the data with the lowest possible latency in a ‚Äúspeed‚Äù \\nlayer, usually a NoSQL database. In the batch layer, data is processed and transformed \\nin a system such as a data warehouse, creating precomputed and aggregated views of \\nthe data. The serving layer provides a combined view by aggregating query results from \\nthe two layers.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 121, 'page_label': '122'}, page_content='114  | Chapter 3: Designing Good Data Architecture \\n \\nFigure 3-14. Lambda architecture \\nLambda architecture has its share of challenges and criticisms. Managing multiple \\nsystems with different codebases is as difficult as it sounds, creating error -prone \\nsystems with code and data that are extremely difficult to reconcile. \\nWe mention Lambda architecture because it still gets attention and is popular in search-\\nengine results for data architecture. Lambda isn‚Äôt our first recommendation if you‚Äôre \\ntrying to combine streaming and batch data for analytics. Technology and practices  \\nhave moved on. \\nNext, let‚Äôs look at a reaction to Lambda architecture, the Kappa architecture. \\nKappa Architecture \\nAs a response to the shortcomings of Lambda architecture, Jay Kreps proposed an \\nalternative called Kappa architecture (Figure 3-15).44 The central thesis is this: why not \\njust use a stream-processing platform as the backbone for all data handling‚Äîingestion, \\nstorage, and serving? This facilitates a true event -based architecture. Real -time and \\nbatch processing can be applied seamlessly to the same data by reading the live event \\nstream directly and replaying large chunks of data for batch processing. \\n \\nFigure 3-15. Kappa architecture \\nThough the original Kappa architecture article came out in 2014, we haven‚Äôt seen it \\nwidely adopted. There may be a couple of reasons for this. First, streaming itself is still \\na bit of a mystery for many companies; it‚Äôs easy to talk about, but harder than expected \\nto execute. Second, Kappa architecture turns out to be complicated and expensive in \\npractice. While some streaming systems can scale to huge data volumes, they are \\n \\n44 Jay Kreps, ‚ÄúQuestioning the Lambda Architecture,‚Äù O‚ÄôReilly Radar, July 2, 2014, https://oreil.ly/wWR3n.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 122, 'page_label': '123'}, page_content='Examples and Types of Data Architecture \\n |  115 \\ncomplex and expensive; batch storage and processing remain much more efficient and \\ncost-effective for enormous historical datasets. \\nThe Dataflow Model and Unified Batch and Streaming \\nBoth Lambda and Kappa sought to address limitations of the Hadoop ecosystem of the \\n2010s by trying to duct-tape together complicated tools that were likely not natural fits \\nin the first place. The central challenge of unifying batch and streaming data remained, \\nand Lambda and Kappa both provided inspiration and groundwork for continued \\nprogress in this pursuit. \\nOne of the central problems of managing batch and stream processing is unifying \\nmultiple code paths. While the Kappa architecture relies on a unified queuing and \\nstorage layer, one still has to confront using different tools for collecting real -time \\nstatistics or running batch aggregation jobs. Today, engineers seek to solve this in \\nseveral ways. Google made its mark by developing the Dataflow model and the Apache \\nBeam framework that implements this model. \\nThe core idea in the Dataflow model is to view all data as events, as the aggregation is \\nperformed over various types of windows. Ongoing real -time event streams are \\nunbounded data. Data batches are simply bounded event streams, and the boundaries \\nprovide a natural window. Engineers can choose from various windows for real -time \\naggregation, such as sliding or tumbling. Real -time and batch processing happens in \\nthe same system using nearly identical code. \\nThe philosophy of ‚Äúbatch as a special case of streaming‚Äù is now more pervasive. \\nVarious frameworks such as Flink and Spark have adopted a similar approach. \\nArchitecture for IoT \\nThe Internet of Things  (IoT) is the distributed collection of devices, aka things‚Äî \\ncomputers, sensors, mobile devices, smart home devices, and anything else with an \\ninternet connection. Rather than generating data from direct human input (think data \\nentry from a keyboard), IoT data is generated from devices that collect data periodically \\nor continuously from the surrounding environment and transmit it to a destination. IoT \\ndevices are often low -powered and operate in low -resource/low bandwidth \\nenvironments. \\nWhile the concept of IoT devices dates back at least a few decades, the smartphone \\nrevolution created a massive IoT swarm virtually overnight. Since then, numerous new \\nIoT categories have emerged, such as smart thermostats, car entertainment systems, \\nsmart TVs, and smart speakers. The IoT has evolved from a futurist fantasy to a massive \\ndata engineering domain. We expect IoT to become one of the dominant ways data is \\ngenerated and consumed, and this section goes a bit deeper than the others you‚Äôve read.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 123, 'page_label': '124'}, page_content='116  | Chapter 3: Designing Good Data Architecture \\nHaving a cursory understanding of IoT architecture will help you understand broader \\ndata architecture trends. Let‚Äôs briefly look at some IoT architecture concepts. \\nDevices \\nDevices (also known as things) are the physical hardware connected to the internet, \\nsensing the environment around them and collecting and transmitting data to a \\ndownstream destination. These devices might be used in consumer applications like a \\ndoorbell camera, smartwatch, or thermos tat. The device might be an AI -powered \\ncamera that monitors an assembly line for defective components, a GPS tracker to \\nrecord vehicle locations, or a Raspberry Pi programmed to download the latest tweets \\nand brew your coffee. Any device capable of collecting data from its environment is an \\nIoT device. \\nDevices should be minimally capable of collecting and transmitting data. However, the \\ndevice might also crunch data or run ML on the data it collects before sending it \\ndownstream‚Äîedge computing and edge machine learning, respectively. \\nA data engineer doesn‚Äôt necessarily need to know the inner details of IoT devices but \\nshould know what the device does, the data it collects, any edge computations or ML \\nit runs before transmitting the data, and how often it sends data. It also helps to kn ow \\nthe consequences of a device or internet outage, environmental or other external factors \\naffecting data collection, and how these may impact the downstream collection of data \\nfrom the device. \\nInterfacing with devices \\nA device isn‚Äôt beneficial unless you can get its data. This section covers some of the \\nkey components necessary to interface with IoT devices in the wild. \\nIoT gateway.    An IoT gateway is a hub for connecting devices and securely routing \\ndevices to the appropriate destinations on the internet. While you can connect a device \\ndirectly to the internet without an IoT gateway, the gateway allows devices to connect \\nusing extremely little power. It acts as a way station for data retention and manages an \\ninternet connection to the final data destination. \\nNew low-power WiFi standards are designed to make IoT gateways less critical in the \\nfuture, but these are just rolling out now. Typically, a swarm of devices will utilize \\nmany IoT gateways, one at each physical location where devices are present (Figure 3-\\n16).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 124, 'page_label': '125'}, page_content='Examples and Types of Data Architecture \\n |  117 \\n \\nFigure 3-16. A device swarm (circles), IoT gateways, and message queue with \\nmessages (rectangles within the queue) \\nIngestion.    Ingestion begins with an IoT gateway, as discussed previously. From \\nthere, events and measurements can flow into an event ingestion architecture. \\nOf course, other patterns are possible. For instance, the gateway may accumulate data \\nand upload it in batches for later analytics processing. In remote physical environments, \\ngateways may not have connectivity to a network much of the time. They may upload \\nall data only when they are brought into the range of a cellular or WiFi network. The \\npoint is that the diversity of IoT systems and environments presents complications ‚Äî\\ne.g., late -arriving data, data structure and schema disparities, data corruption, and  \\nconnection disruption ‚Äîthat engineers must account for in their architectures and \\ndownstream analytics. \\nStorage.    Storage requirements will depend a great deal on the latency requirement \\nfor the IoT devices in the system. For example, for remote sensors collecting scientific \\ndata for analysis at a later time, batch object storage may be perfectly acceptable. \\nHowever, near real -time responses may be expected from a system backend that \\nconstantly analyzes data in a home monitoring and automation solution. In this case, a \\nmessage queue or time-series database is more appropriate. We discuss storage systems \\nin more detail in Chapter 6. \\nServing.    Serving patterns are incredibly diverse. In a batch scientific application, \\ndata might be analyzed using a cloud data warehouse and then served in a report. Data \\nwill be presented and served in numerous ways in a home-monitoring application. Data \\nwill be analyzed in the near time using a stream-processing engine or queries in a time-\\nseries database to look for critical events such as a fire, electrical outage, or break -in. \\nDetection of an anomaly will trigger alerts to the homeowner, the fire de partment, or \\nother entity. A batch analytics component also exists‚Äîfor example, a monthly report \\non the state of the home.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 125, 'page_label': '126'}, page_content='118  | Chapter 3: Designing Good Data Architecture \\nOne significant serving pattern for IoT looks like reverse ETL (Figure 3-17), although \\nwe tend not to use this term in the IoT context. Think of this scenario: data from sensors \\non manufacturing devices is collected and analyzed. The results of these measurements \\nare processed to look for optimizations that will allow equipment t o operate more \\nefficiently. Data is sent back to reconfigure the devices and optimize them. \\n \\nFigure 3-17. IoT serving pattern for downstream use cases \\nScratching the surface of the IoT \\nIoT scenarios are incredibly complex, and IoT architecture and systems are also less \\nfamiliar to data engineers who may have spent their careers working with business data. \\nWe hope that this introduction will encourage interested data engineers to learn mo re \\nabout this fascinating and rapidly evolving specialization. \\nData Mesh \\nThe data mesh is a recent response to sprawling monolithic data platforms, such as \\ncentralized data lakes and data warehouses, and ‚Äúthe great divide of data,‚Äù wherein the \\nlandscape is divided between operational data and analytical data. 45  The data mesh \\nattempts to invert the challenges of centralized data architecture, taking the concepts of \\ndomain-driven design (commonly used in software architectures) and applying them to \\ndata architecture. Because the data mesh has captured much recent attention, you \\nshould be aware of it. \\nA big part of the data mesh is decentralization, as Zhamak Dehghani noted in her \\ngroundbreaking article on the topic:46 \\nIn order to decentralize the monolithic data platform, we need to reverse how we think \\nabout data, its locality, and ownership. Instead of flowing the data from domains into \\na centrally owned data lake or platform, domains need to host and serve their doma in \\ndatasets in an easily consumable way. \\n \\n45 Zhamak Dehghani, ‚ÄúData Mesh Principles and Logical Architecture,‚Äù MartinFowler.com, December 3, 2020, \\nhttps://oreil.ly/ezWE7. \\n46 Zhamak Dehghani, ‚ÄúHow to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh,‚Äù \\nMartinFowler.com, May 20, 2019, https://oreil.ly/SqMe8.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 126, 'page_label': '127'}, page_content='Examples and Types of Data Architecture \\n |  119 \\nDehghani later identified four key components of the data mesh:47 \\n‚Ä¢ Domain-oriented decentralized data ownership and architecture \\n‚Ä¢ Data as a product \\n‚Ä¢ Self-serve data infrastructure as a platform \\n‚Ä¢ Federated computational governance \\nFigure 3-18 shows a simplified version of a data mesh architecture. You can learn more \\nabout data mesh in Dehghani‚Äôs book Data Mesh (O‚ÄôReilly). \\n \\nFigure 3-18. Simplified example of a data mesh architecture. Source: From Data \\nMesh, by Zhamak Dehghani. Copyright ¬© 2022 Zhamak Dehghani. Published by \\nO‚ÄôReilly Media, Inc. Used with permission. \\n \\n47 Zhamak Dehghani, ‚ÄúData Mesh Principles and Logical Architecture.‚Äù'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 127, 'page_label': '128'}, page_content='120  | Chapter 3: Designing Good Data Architecture \\nOther Data Architecture Examples \\nData architectures have countless other variations, such as data fabric, data hub, scaled \\narchitecture, metadata-first architecture , event-driven architecture, live data stack \\n(Chapter 11 ), and many more. And new architectures will continue to emerge as \\npractices consolidate and mature, and tooling simplifies and improves. We‚Äôve focused \\non a handful of the most critical data architecture patterns that are extremely well \\nestablished, evolving rapidly, or both. \\nAs a data engineer, pay attention to how new architectures may help your organization. \\nStay abreast of new developments by cultivating a high -level awareness of the data \\nengineering ecosystem developments. Be open -minded and don‚Äôt get emotionally \\nattached to one approach. Once you‚Äôve identified potential value, deepen your learning \\nand make concrete decisions. When done right, minor tweaks ‚Äîor major overhauls‚Äî\\nin your data architecture can positively impact the business.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 128, 'page_label': '129'}, page_content='| \\n 121 \\nWho‚Äôs Involved with Designing a Data \\nArchitecture? \\nData architecture isn‚Äôt designed in a vacuum. Bigger companies may still employ data \\narchitects, but those architects will need to be heavily in tune and current with the state \\nof technology and data. Gone are the days of ivory tower data architecture. In the past, \\narchitecture was largely orthogonal to engineering. We expect this distinction will \\ndisappear as data engineering, and engineering in general, quickly evolves, becoming \\nmore agile, with less separation between engineering and architecture. \\nIdeally, a data engineer will work alongside a dedicated data architect. However, if a \\ncompany is small or low in its level of data maturity, a data engineer might work double \\nduty as an architect. Because data architecture is an undercurrent of the data \\nengineering lifecycle, a data engineer should understand ‚Äúgood‚Äù architecture and the \\nvarious types of data architecture. \\nWhen designing architecture, you‚Äôll work alongside business stakeholders to evaluate \\ntrade-offs. What are the trade-offs inherent in adopting a cloud data warehouse versus \\na data lake? What are the trade-offs of various cloud platforms? When might a unified \\nbatch/streaming framework (Beam, Flink) be an appropriate choice? Studying these \\nchoices in the abstract will prepare you to make concrete, valuable decisions. \\nConclusion \\nYou‚Äôve learned how data architecture fits into the data engineering lifecycle and what \\nmakes for ‚Äúgood‚Äù data architecture, and you‚Äôve seen several examples of data \\narchitectures. Because architecture is such a key foundation for success, we encourage \\nyou to invest the time to study it deeply and understand the trade-offs inherent in any \\narchitecture. You will be prepared to map out architecture that corresponds to your \\norganization‚Äôs unique requirements. \\nNext up, let‚Äôs look at some approaches to choosing the right technologies to be used in \\ndata architecture and across the data engineering lifecycle. \\nAdditional Resources \\n‚Ä¢ ‚ÄúAnemicDomainModel‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúBig Data Architectures‚Äù Azure documentation \\n‚Ä¢ ‚ÄúBoundedContext‚Äù by Martin Fowler'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 129, 'page_label': '130'}, page_content='‚Ä¢ \\n122 | Chapter 3: Designing Good Data Architecture \\n‚Ä¢ ‚ÄúA Brief Introduction to Two Data Processing Architectures‚ÄîLambda and Kappa \\nfor Big Data‚Äù by Iman Samizadeh \\n‚Ä¢ ‚ÄúThe Building Blocks of a Modern Data Platform‚Äù by Prukalpa ‚Ä¢ ‚ÄúChoosing Open \\nWisely‚Äù by Benoit Dageville et al. \\nWho‚Äôs Involved with Designing a Data Architecture?  \\n‚ÄúChoosing the Right Architecture for Global Data Distribution‚Äù  Google Cloud \\nArchitecture web page \\n‚Ä¢ ‚ÄúColumn-Oriented DBMS‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúA Comparison of Data Processing Frameworks‚Äù by Ludovik Santos \\n‚Ä¢ ‚ÄúThe Cost of Cloud, a Trillion Dollar Paradox‚Äù by Sarah Wang and Martin Casado \\n‚Ä¢ ‚ÄúThe Curse of the Data Lake Monster‚Äù by Kiran Prakash and Lucy Chambers \\n‚Ä¢ Data Architecture: A Primer for the Data Scientist  by W. H. Inmon et al. \\n(Academic Press) \\n‚Ä¢ ‚ÄúData Architecture: Complex vs. Complicated‚Äù by Dave Wells \\n‚Ä¢ ‚ÄúData as a Product vs. Data as a Service‚Äù by Justin Gage \\n‚Ä¢ ‚ÄúThe Data Dichotomy: Rethinking the Way We Treat Data and Services‚Äù by Ben \\nStopford \\n‚Ä¢ ‚ÄúData Fabric Architecture Is Key to Modernizing Data Management and \\nIntegration‚Äù by Ashutosh Gupta \\n‚Ä¢ ‚ÄúData Fabric Defined‚Äù by James Serra \\n‚Ä¢ ‚ÄúData Team Platform‚Äù by GitLab Data \\n‚Ä¢ ‚ÄúData Warehouse Architecture: Overview‚Äù by Roelant V os \\n‚Ä¢ ‚ÄúData Warehouse Architecture‚Äù tutorial at Javatpoint \\n‚Ä¢ ‚ÄúDefining Architecture‚Äù ISO/IEC/IEEE 42010 web page \\n‚Ä¢ ‚ÄúThe Design and Implementation of Modern Column-Oriented Database Systems‚Äù \\nby Daniel Abadi et al. \\n‚Ä¢ ‚ÄúDisasters I‚Äôve Seen in a Microservices World‚Äù by Joao Alves \\n‚Ä¢ ‚ÄúDomainDrivenDesign‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúDown with Pipeline Debt: Introducing Great Expectations‚Äù  by the Great \\nExpectations project \\n‚Ä¢ EABOK draft, edited by Paula Hagan \\n‚Ä¢ EABOK website \\n‚Ä¢ ‚ÄúEagerReadDerivation‚Äù by Martin Fowler'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 130, 'page_label': '131'}, page_content='‚Ä¢ \\n | \\n 123 \\n‚Ä¢ ‚ÄúEnd-to-End Serverless ETL Orchestration in AWS: A Guide‚Äù by Rittika Jindal \\n‚Ä¢ ‚ÄúEnterprise Architecture‚Äù Gartner Glossary definition \\n‚Ä¢ ‚ÄúEnterprise Architecture‚Äôs Role in Building a Data -Driven Organization‚Äù  by \\nAshutosh Gupta \\n‚Ä¢ ‚ÄúEvent Sourcing‚Äù by Martin Fowler \\n‚ÄúFalling Back in Love with Data Pipelines‚Äù by Sean Knapp \\n‚Ä¢ ‚ÄúFive Principles for Cloud-Native Architecture: What It Is and How to Master It‚Äù \\nby Tom Grey \\n‚Ä¢ ‚ÄúFocusing on Events‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúFunctional Data Engineering: A Modern Paradigm for Batch Data Processing‚Äù by \\nMaxime Beauchemin \\n‚Ä¢ ‚ÄúGoogle Cloud Architecture Framework‚Äù Google Cloud Architecture web page \\n‚Ä¢ ‚ÄúHow to Beat the Cap Theorem‚Äù by Nathan Marz \\n‚Ä¢ ‚ÄúHow to Build a Data Architecture to Drive Innovation ‚ÄîToday and Tomorrow‚Äù \\nby Antonio Castro et al. \\n‚Ä¢ ‚ÄúHow TOGAF Defines Enterprise Architecture (EA)‚Äù by Avancier Limited \\n‚Ä¢ The Information Management Body of Knowledge website \\n‚Ä¢ ‚ÄúIntroducing Dagster: An Open Source Python Library for Building Data \\nApplications‚Äù by Nick Schrock \\n‚Ä¢ ‚ÄúThe Log: What Every Software Engineer Should Know About Real-Time Data‚Äôs \\nUnifying Abstraction‚Äù by Jay Kreps \\n‚Ä¢ ‚ÄúMicrosoft Azure IoT Reference Architecture‚Äù documentation \\n‚Ä¢ Microsoft‚Äôs ‚ÄúAzure Architecture Center‚Äù \\n‚Ä¢ ‚ÄúModern CI Is Too Complex and Misdirected‚Äù by Gregory Szorc \\n‚Ä¢ ‚ÄúThe Modern Data Stack: Past, Present, and Future‚Äù by Tristan Handy \\n‚Ä¢ ‚ÄúMoving Beyond Batch vs. Streaming‚Äù by David Yaffe \\n‚Ä¢ ‚ÄúA Personal Implementation of Modern Data Architecture: Getting Strava Data \\ninto Google Cloud Platform‚Äù by Matthew Reeve \\n‚Ä¢ ‚ÄúPolyglot Persistence‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúPotemkin Data Science‚Äù by Michael Correll \\n‚Ä¢ ‚ÄúPrincipled Data Engineering, Part I: Architectural Overview‚Äù by Hussein Danish \\n‚Ä¢ ‚ÄúQuestioning the Lambda Architecture‚Äù by Jay Kreps \\n‚Ä¢ ‚ÄúReliable Microservices Data Exchange with the Outbox Pattern‚Äù  by Gunnar \\nMorling \\n‚Ä¢ ‚ÄúReportingDatabase‚Äù by Martin Fowler'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 131, 'page_label': '132'}, page_content='‚Ä¢ \\n124 | Chapter 3: Designing Good Data Architecture \\n‚Ä¢ ‚ÄúThe Rise of the Metadata Lake‚Äù by Prukalpa \\n‚Ä¢ ‚ÄúRun Your Data Team Like a Product Team‚Äù  by Emilie Schario and Taylor A. \\nMurphy \\nAdditional Resources  \\n‚ÄúSeparating Utility from Value Add‚Äù by Ross Pettit \\n‚Ä¢ ‚ÄúThe Six Principles of Modern Data Architecture‚Äù by Joshua Klahr \\n‚Ä¢ Snowflake‚Äôs ‚ÄúWhat Is Data Warehouse Architecture‚Äù web page \\n‚Ä¢ ‚ÄúSoftware Infrastructure 2.0: A Wishlist‚Äù by Erik Bernhardsson \\n‚Ä¢ ‚ÄúStaying Ahead of Data Debt‚Äù by Etai Mizrahi \\n‚Ä¢ ‚ÄúTactics vs. Strategy: SOA and the Tarpit of Irrelevancy‚Äù by Neal Ford \\n‚Ä¢ ‚ÄúTest Data Quality at Scale with Deequ‚Äù by Dustin Lange et al. \\n‚Ä¢ ‚ÄúThree-Tier Architecture‚Äù by IBM Education \\n‚Ä¢ TOGAF framework website \\n‚Ä¢ ‚ÄúThe Top 5 Data Trends for CDOs to Watch Out for in 2021‚Äù by Prukalpa \\n‚Ä¢ ‚Äú240 Tables and No Documentation?‚Äù by Alexey Makhotkin \\n‚Ä¢ ‚ÄúThe Ultimate Data Observability Checklist‚Äù by Molly V orwerck \\n‚Ä¢ ‚ÄúUnified Analytics: Where Batch and Streaming Come Together; SQL and \\nBeyond‚Äù Apache Flink Roadmap \\n‚Ä¢ ‚ÄúUtilityVsStrategicDichotomy‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúWhat Is a Data Lakehouse?‚Äù by Ben Lorica et al. \\n‚Ä¢ ‚ÄúWhat Is Data Architecture? A Framework for Managing Data‚Äù by Thor Olavsrud \\n‚Ä¢ ‚ÄúWhat Is the Open Data Ecosystem and Why It‚Äôs Here to Stay‚Äù by Casber Wang \\n‚Ä¢ ‚ÄúWhat‚Äôs Wrong with MLOps?‚Äù by Laszlo Sragner \\n‚Ä¢ ‚ÄúWhat the Heck Is Data Mesh‚Äù by Chris Riccomini \\n‚Ä¢ ‚ÄúWho Needs an Architect‚Äù by Martin Fowler \\n‚Ä¢ ‚ÄúZachman Framework‚Äù Wikipedia page'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 132, 'page_label': '133'}, page_content='125 \\nCHAPTER 4 \\nChoosing Technologies Across the \\nData \\nEngineering Lifecycle \\nData engineering nowadays suffers from an embarrassment of riches. We have no \\nshortage of technologies to solve various types of data problems. Data technologies \\nare available as turnkey offerings consumable in almost every way ‚Äîopen source, \\nmanaged open source, proprietary software, proprietary service, and more. However, \\nit‚Äôs easy to get caught up in chasing bleeding-edge technology while losing sight of the \\ncore purpose of data engineering: designing robust and reliable systems to carry data \\nthrough the f ull lifecycle and serve it according to the needs of end users. Just as \\nstructural engineers carefully choose technologies and materials to realize an \\narchitect‚Äôs vision for a building, data engineers are tasked with making appropriate \\ntechnology choices to shepherd data through the lifecycle to serve data applications \\nand users. \\nChapter 3 discussed ‚Äúgood‚Äù data architecture and why it matters. We now explain how \\nto choose the right technologies to serve this architecture. Data engineers must choose \\ngood technologies to make the best possible data product. We feel the criteria to choose \\na good data technology is simple: does it add value to a data product and the broader \\nbusiness? \\nA lot of people confuse architecture and tools. Architecture is strategic; tools are \\ntactical. We sometimes hear, ‚ÄúOur data architecture are tools X, Y, and Z.‚Äù This is the \\nwrong way to think about architecture. Architecture is the high-level design, roadmap, \\nand blueprint of data systems that satisfy the strategic aims for the business. \\nArchitecture is the what, why, and when. Tools are used to make the architecture a \\nreality; tools are the how.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 133, 'page_label': '134'}, page_content='126  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nWe often see teams going ‚Äúoff the rails‚Äù and choosing technologies before mapping \\nout an architecture. The reasons vary: shiny object syndrome, resume -driven \\ndevelopment, and a lack of expertise in architecture. In practice, this prioritization of \\ntechnology often means they cobble together a kind of Dr. Seuss fantasy machine \\nrather than a true data architecture. We strongly advise against choosing technology \\nbefore getting your architecture right. Architecture first, technology second. This \\nchapter discusses our tactical plan for making technology choices once we have a \\nstrategic architecture blueprint. The following are some considerations for choosing \\ndata technologies across the data engineering lifecycle: \\n‚Ä¢ Team size and capabilities \\n‚Ä¢ Speed to market \\n‚Ä¢ Interoperability \\n‚Ä¢ Cost optimization and business value \\n‚Ä¢ Today versus the future: immutable versus transitory technologies \\n‚Ä¢ Location (cloud, on prem, hybrid cloud, multicloud) \\n‚Ä¢ Build versus buy \\n‚Ä¢ Monolith versus modular \\n‚Ä¢ Serverless versus servers \\n‚Ä¢ Optimization, performance, and the benchmark wars \\n‚Ä¢ The undercurrents of the data engineering lifecycle \\nTeam Size and Capabilities \\nThe first thing you need to assess is your team‚Äôs size and its capabilities with \\ntechnology. Are you on a small team (perhaps a team of one) of people who are \\nexpected to wear many hats, or is the team large enough that people work in specialized \\nroles? Wi ll a handful of people be responsible for multiple stages of the data \\nengineering lifecycle, or do people cover particular niches? Your team‚Äôs size will \\ninfluence the types of technologies you adopt. \\nThere is a continuum of simple to complex technologies, and a team‚Äôs size roughly \\ndetermines the amount of bandwidth your team can dedicate to complex solutions. We \\nsometimes see small data teams read blog posts about a new cutting-edge technology \\nat a gia nt tech company and then try to emulate these same extremely complex \\ntechnologies and practices. We call this cargo-cult engineering, and it‚Äôs generally a big \\nmistake that consumes a lot of valuable time and money, often with little to nothing to \\nshow in return. Especially for small teams or teams with weaker technical chops, use'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 134, 'page_label': '135'}, page_content='|  127 \\nas many managed and SaaS tools as possible, and dedicate your limited bandwidth to \\nsolving the complex problems that directly add value to the business. \\nTake an inventory of your team‚Äôs skills. Do people lean toward low-code tools, or do \\nthey favor code -first approaches? Are people strong in certain languages like Java, \\nPython, or Go? Technologies are available to cater to every preference on the low-code \\nto code-heavy spectrum. Again, we suggest sticking with technologies and workflows \\nwith which the team is familiar. We‚Äôve seen data teams invest a lot of time in learning \\nthe shiny new data technology, language, or tool, only to never use it in production.  \\nLearning new technologies, languages, and tools is a considerable time investment, so \\nmake these investments wisely. \\nSpeed to Market \\nIn technology, speed to market wins. This means choosing the right technologies that \\nhelp you deliver features and data faster while maintaining high-quality standards and \\nsecurity. It also means working in a tight feedback loop of launching, learning, \\niterating, and making improvements. \\nPerfect is the enemy of good. Some data teams will deliberate on technology choices \\nfor months or years without reaching any decisions. Slow decisions and output are the \\nkiss of death to data teams. We‚Äôve seen more than a few data teams dissolve for moving \\ntoo slow and failing to deliver the value they were hired to produce. Deliver value early \\nand often. As we‚Äôve mentioned, use what works. Your team members will likely get \\nbetter leverage with tools they already know. Avoid undifferentiated heavy lifting that \\nengages your team in unnecessarily complex work that adds little to no value. Choose \\ntools that help you move quickly, reliably, safely, and securely. \\nInteroperability \\nRarely will you use only one technology or system. When choosing a technology or \\nsystem, you‚Äôll need to ensure that it interacts and operates with other technologies. \\nInteroperability describes how various technologies or systems connect, exchange \\ninformation, and interact. \\nLet‚Äôs say you‚Äôre evaluating two technologies, A and B. How easily does technology \\nA integrate with technology B when thinking about interoperability? This is often a \\nspectrum of difficulty, ranging from seamless to time-intensive. Is seamless integration \\nalready baked into each product, making setup a breeze? Or do you need to do a lot of \\nmanual configuration to integrate these technologies?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 135, 'page_label': '136'}, page_content='128  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nOften, vendors and open source projects will target specific platforms and systems to \\ninteroperate. Most data ingestion and visualization tools have built-in integrations with \\npopular data warehouses and data lakes. Furthermore, popular data-ingestion \\nSpeed to Market  \\ntools will integrate with common APIs and services, such as CRMs, accounting \\nsoftware, and more. \\nSometimes standards are in place for interoperability. Almost all databases allow \\nconnections via Java Database Connectivity (JDBC) or Open Database Connectivity \\n(ODBC), meaning that you can easily connect to a database by using these standards. \\nIn other c ases, interoperability occurs in the absence of standards. Representational \\nstate transfer (REST) is not truly a standard for APIs; every REST API has its quirks. \\nIn these cases, it‚Äôs up to the vendor or open source software (OSS) project to ensure \\nsmooth integration with other technologies and systems. \\nAlways be aware of how simple it will be to connect your various technologies across \\nthe data engineering lifecycle. As mentioned in other chapters, we suggest designing \\nfor modularity and giving yourself the ability to easily swap out technologies as new \\npractices and alternatives become available. \\nCost Optimization and Business Value \\nIn a perfect world, you‚Äôd get to experiment with all the latest, coolest technologies \\nwithout considering cost, time investment, or value added to the business. In reality, \\nbudgets and time are finite, and the cost is a major constraint for choosing the ri ght \\ndata architectures and technologies. Your organization expects a positive ROI from \\nyour data projects, so you must understand the basic costs you can control. Technology \\nis a major cost driver, so your technology choices and management strategies will \\nsignificantly impact your budget. We look at costs through three main lenses: total cost \\nof ownership, opportunity cost, and FinOps. \\nTotal Cost of Ownership \\nTotal cost of ownership (TCO) is the total estimated cost of an initiative, including the \\ndirect and indirect costs of products and services utilized. Direct costs can be directly \\nattributed to an initiative. Examples are the salaries of a team working on the initiative \\nor the AWS bill for all services consumed. Indirect costs, also known as overhead, are \\nindependent of the initiative and must be paid regardless of where they‚Äôre attributed.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 136, 'page_label': '137'}, page_content='|  129 \\nApart from direct and indirect costs, how something is purchased impacts the way costs \\nare accounted for. Expenses fall into two big groups: capital expenses and operational \\nexpenses. \\nCapital expenses, also known as capex, require an up -front investment. Payment is \\nrequired today. Before the cloud existed, companies would typically purchase \\nhardware and software up front through large acquisition contracts. In addition, \\nsignificant investments were required to host hardware in server rooms, data centers, \\nand colocation facilities.  These up -front investments ‚Äîcommonly hundreds of \\nthousands to millions of dollars or more ‚Äîwould be treated as assets and slowly \\ndepreciate over time. From a budget perspective, capital was required to fund the entire \\npurchase. This is capex, a significant capital outlay with a long-term plan to achieve a \\npositive ROI on the effort and expense put forth. \\nOperational expenses , also known as opex, are the opposite of capex in certain \\nrespects. Opex is gradual and spread out over time. Whereas capex is long -term \\nfocused, opex is short-term. Opex can be pay-as-you-go or similar and allows a lot of \\nflexibility. Opex is closer to a direct cost, making it easier to attribute to a data project. \\nUntil recently, opex wasn‚Äôt an option for large data projects. Data systems often \\nrequired multimillion-dollar contracts. This has changed with the advent of the cloud, \\nas data platform services allow engineers to pay on a consumption -based model. In \\ngeneral, opex allows for a far greater ability for engineering teams to choose their \\nsoftware and hardware. Cloud -based services let data engineers iterate quickly with \\nvarious software and technology configurations, often inexpensively. \\nData engineers need to be pragmatic about flexibility. The data landscape is changing \\ntoo quickly to invest in long-term hardware that inevitably goes stale, can‚Äôt easily scale, \\nand potentially hampers a data engineer‚Äôs flexibility to try new things. Given the upside \\nfor flexibility and low initial costs, we urge data engineers to take an opex -first \\napproach centered on the cloud and flexible, pay-as-you-go technologies. \\nTotal Opportunity Cost of Ownership \\nAny choice inherently excludes other possibilities. Total opportunity cost of ownership \\n(TOCO) is the cost of lost opportunities that we incur in choosing a technology, an \\narchitecture, or a process. 48 Note that ownership in this setting doesn‚Äôt require long -\\nterm purchases of hardware or licenses. Even in a cloud environment, we effectively \\nown a technology, a stack, or a pipeline once it becomes a core part of our production \\n \\n48 For more details, see ‚ÄúTotal Opportunity Cost of Ownership‚Äù by Joseph Reis in 97 Things Every Data \\nEngineer Should Know (O‚ÄôReilly). \\nCost Optimization and Business Value'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 137, 'page_label': '138'}, page_content='130  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\ndata processes and is difficult to move away from. Data engineers often fail to evaluate \\nTOCO when undertaking a new project; in our opinion, this is a massive blind spot. \\nIf you choose data stack A, you‚Äôve chosen the benefits of data stack A over all other \\noptions, effectively excluding data stacks B, C, and D. You‚Äôre committed to data stack \\nA and everything it entails‚Äîthe team to support it, training, setup, and maintenanc e. \\nWhat happens if data stack A was a poor choice? What happens when data stack A \\nbecomes obsolete? Can you still move to other data stacks? \\nHow quickly and cheaply can you move to something newer and better? This is a \\ncritical question in the data space, where new technologies and products seem to appear \\nat an ever-faster rate. Does the expertise you‚Äôve built up on data stack A translate to \\nthe next wave? Or are you able to swap out components of data stack A and buy \\nyourself some time and options? \\nThe first step to minimizing opportunity cost is evaluating it with eyes wide open. \\nWe‚Äôve seen countless data teams get stuck with technologies that seemed good at the \\ntime and are either not flexible for future growth or simply obsolete. Inflexible data \\ntechnologies are a lot like bear traps. They‚Äôre easy to get into and extremely painful to \\nescape. \\nFinOps \\nWe already touched on FinOps in ‚ÄúPrinciple 9: Embrace FinOps‚Äù on page 85. As we‚Äôve \\ndiscussed, typical cloud spending is inherently opex: companies pay for services to run \\ncritical data processes rather than making up-front purchases and clawing back value \\nover time. The goal of FinOps is to fully operationalize financial accou ntability and \\nbusiness value by applying the DevOps-like practices of monitoring and dynamically \\nadjusting systems. \\nIn this chapter, we want to emphasize one thing about FinOps that is well embodied in \\nthis quote:49 \\nIf it seems that FinOps is about saving money, then think again. FinOps is about \\nmaking money. Cloud spend can drive more revenue, signal customer base growth, \\nenable more product and feature release velocity, or even help shut down a data center. \\nIn our setting of data engineering, the ability to iterate quickly and scale dynamically \\nis invaluable for creating business value. This is one of the major motivations for \\nshifting data workloads to the cloud. \\n \\n49 J. R. Storment and Mike Fuller, Cloud FinOps (Sebastopol, CA: O‚ÄôReilly, 2019), 6, https://oreil.ly/RvRvX.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 138, 'page_label': '139'}, page_content='|  131 \\nToday Versus the Future: Immutable Versus \\nTransitory Technologies \\nIn an exciting domain like data engineering, it‚Äôs all too easy to focus on a rapidly \\nevolving future while ignoring the concrete needs of the present. The intention to build \\na better future is noble but often leads to overarchitecting and overengineering. Tooling \\nchosen for the future may be stale and out-of-date when this future arrives; the future \\nfrequently looks little like what we envisioned years before. \\nAs many life coaches would tell you, focus on the present. You should choose the best \\ntechnology for the moment and near future, but in a way that supports future unknowns \\nand evolution. Ask yourself: where are you today, and what are your goals for the \\nfuture? Your answers to these questions should inform your decisions about your \\narchitecture and thus the technologies used within that architecture. This is done by \\nunderstanding what is likely to change and what tends to stay the same. \\nWe have two classes of tools to consider: immutable and transitory. Immutable \\ntechnologies might be components that underpin the cloud or languages and paradigms \\nthat have stood the test of time. In the cloud, examples of immutable technologies are \\nobject storage, networking, servers, and security. Object storage such as Amazon S3 \\nand Azure Blob Storage will be around from today until the end of the decade, and \\nprobably much longer. Storing your data in object storage is a wise choice. Object \\nstorage continues to improve in various ways and constantly offers new options, but \\nyour data will be safe and usable in object storage regardless of the rapid evolution of \\ntechnology as a whole. \\nFor languages, SQL and bash have been around for many decades, and we don‚Äôt see \\nthem disappearing anytime soon. Immutable technologies benefit from the Lindy \\neffect: the longer a technology has been established, the longer it will be used. Think \\nof the pow er grid, relational databases, the C programming language, or the x86 \\nprocessor architecture. We suggest applying the Lindy effect as a litmus test to \\ndetermine whether a technology is potentially immutable. \\nTransitory technologies are those that come and go. The typical trajectory begins with \\na lot of hype, followed by meteoric growth in popularity, then a slow descent into \\nobscurity. The JavaScript frontend landscape is a classic example. How many \\nJavaScript frontend frameworks ha ve come and gone between 2010 and 2020? \\nBackbone.js, Ember.js, and Knockout were popular in the early 2010s, and React and \\nVue.js have massive mindshare today. What‚Äôs the popular frontend JavaScript \\nframework three years from now? Who knows. \\nNew well-funded entrants and open source projects arrive on the data front every day. \\nEvery vendor will say their product will change the industry and ‚Äúmake the world a \\nbetter place‚Äù. Most of these companies and projects don‚Äôt get long -term traction and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 139, 'page_label': '140'}, page_content='132  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nfade slowly into obscurity. Top VCs are making big-money bets, knowing that most of \\ntheir data-tooling investments will fail. If VCs pouring millions (or billions) into data-\\ntooling investments can‚Äôt get it right, how can you possibly know which technologies \\nto invest in for your data architecture? It‚Äôs hard. Just consider the number of \\ntechnologies in Matt Turck‚Äôs (in)famous depictions of the ML, AI, and data (MAD) \\nlandscape that we introduced in Chapter 1 (Figure 4-1). \\nEven relatively successful technologies often fade into obscurity quickly, after a few \\nyears of rapid adoption, a victim of their success. For instance, in the early 2010s, Hive \\nwas met with rapid uptake because it allowed both analysts and engineers to qu ery \\nmassive datasets without coding complex MapReduce jobs by hand. Inspired \\nToday Versus the Future: Immutable Versus Transitory \\nTechnologies  \\nby the success of Hive but wishing to improve on its shortcomings, engineers \\ndeveloped Presto and other technologies. Hive now appears primarily in legacy \\ndeployments. Almost every technology follows this inevitable path of decline. \\n \\nFigure 4-1. Matt Turck‚Äô s 2021 MAD data landscape \\nOur Advice \\nGiven the rapid pace of tooling and best-practice changes, we suggest evaluating tools \\nevery two years ( Figure 4-2). Whenever possible, find the immutable technologies \\nalong the data engineering lifecycle, and use those as your base. Build transitory tools \\naround the immutables.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 140, 'page_label': '141'}, page_content='|  133 \\n \\nFigure 4-2. Use a two-year time horizon to reevaluate your technology choices \\nGiven the reasonable probability of failure for many data technologies, you need to \\nconsider how easy it is to transition from a chosen technology. What are the barriers to \\nleaving? As mentioned previously in our discussion about opportunity cost, avoid'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 141, 'page_label': '142'}, page_content='134  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\n‚Äúbear traps.‚Äù Go into a new technology with eyes wide open, knowing the project might \\nget abandoned, the company may not be viable, or the technology simply isn‚Äôt a good \\nfit any longer. \\nLocation \\nCompanies now have numerous options when deciding where to run their technology \\nstacks. A slow shift toward the cloud culminates in a veritable stampede of companies \\nspinning up workloads on AWS, Azure, and Google Cloud Platform (GCP). In the last \\ndecade, many CTOs have come to view their decisions around technology hosting as \\nhaving existential significance for their organizations. If they move too slowly, they \\nrisk being left behind by their more agile competition; on the other hand, a poorly \\nplanned cloud migration could lead to technological failure and catastrophic costs. \\nLet‚Äôs look at the principal places to run your technology stack: on premises, the cloud, \\nhybrid cloud, and multicloud. \\nOn Premises \\nWhile new startups are increasingly born in the cloud, on-premises systems are still the \\ndefault for established companies. Essentially, these companies own their hardware, \\nwhich may live in data centers they own or in leased colocation space. In either ca se, \\ncompanies are operationally responsible for their hardware and the software that runs \\non it. If hardware fails, they have to repair or replace it. They also have to manage \\nupgrade cycles every few years as new, updated hardware is released and older \\nhardware ages and becomes less reliable. They must ensure that they have enough \\nhardware to handle peaks; for an online retailer, this means hosting enough capacity to \\nhandle the load spikes of Black Friday. For data engineers in charge of on -premises \\nsystems, this means buying large-enough systems to allow good performance for peak \\nload and large jobs without overbuying and overspending. \\nOn the one hand, established companies have established operational practices that \\nhave served them well. Suppose a company that relies on information technology has \\nbeen in business for some time. This means it has managed to juggle the cost and \\npersonnel requirements of running its hardware, managing software environments, \\ndeploying code from dev teams, and running databases and big data systems. \\nOn the other hand, established companies see their younger, more agile competition \\nscaling rapidly and taking advantage of cloud -managed services. They also see \\nestablished competitors making forays into the cloud, allowing them to temporarily \\nscale up eno rmous computing power for massive data jobs or the Black Friday \\nshopping spike.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 142, 'page_label': '143'}, page_content='Location  |  135 \\nCompanies in competitive sectors generally don‚Äôt have the option to stand still. \\nCompetition is fierce, and there‚Äôs always the threat of being ‚Äúdisrupted‚Äù by more agile \\ncompetition, often backed by a large pile of venture capital dollars. Every company \\nmust keep its existing systems running efficiently while deciding what moves to make \\nnext. This could involve adopting newer DevOps practices, such as containers, \\nKubernetes, microservices, and continuous deployment while keeping their hardware \\nrunning on premises. It could involve a complete migration to the cloud, as discussed \\nnext. \\nCloud \\nThe cloud flips the on-premises model on its head. Instead of purchasing hardware, you \\nsimply rent hardware and managed services from a cloud provider (such as AWS, \\nAzure, or Google Cloud). These resources can often be reserved on an extremely short-\\nterm basis; VMs spin up in less than a minute, and subsequent usage is billed in per -\\nsecond increments. This allows cloud users to dynamically scale resources that were \\ninconceivable with on-premises servers. \\nIn a cloud environment, engineers can quickly launch projects and experiment without \\nworrying about long lead time hardware planning. They can begin running servers as \\nsoon as their code is ready to deploy. This makes the cloud model extremely appealing \\nto startups that are tight on budget and time. \\nThe early cloud era was dominated by infrastructure as a service (IaaS) offerings ‚Äî \\nproducts such as VMs and virtual disks that are essentially rented slices of hardware. \\nSlowly, we‚Äôve seen a shift toward platform as a service (PaaS), while SaaS products \\ncontinue to grow at a rapid clip. \\nPaaS includes IaaS products but adds more sophisticated managed services to support \\napplications. Examples are managed databases such as Amazon Relational Database \\nService (RDS) and Google Cloud SQL, managed streaming platforms such as Amazon \\nKinesis and S imple Queue Service (SQS), and managed Kubernetes such as Google \\nKubernetes Engine (GKE) and Azure Kubernetes Service (AKS). PaaS services allow \\nengineers to ignore the operational details of managing individual machines and \\ndeploying frameworks across dis tributed systems. They provide turnkey access to \\ncomplex, autoscaling systems with minimal operational overhead. \\nSaaS offerings move one additional step up the ladder of abstraction. SaaS typically \\nprovides a fully functioning enterprise software platform with little operational \\nmanagement. Examples of SaaS include Salesforce, Google Workspace, Microsoft 365, \\nZoom, a nd Fivetran. Both the major public clouds and third parties offer SaaS \\nplatforms. SaaS covers a whole spectrum of enterprise domains, including video \\nconferencing, data management, ad tech, office applications, and CRM systems.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 143, 'page_label': '144'}, page_content='136  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nThis chapter also discusses serverless, increasingly important in PaaS and SaaS \\nofferings. Serverless products generally offer automated scaling from zero to extremely \\nhigh usage rates. They are billed on a pay -as-you-go basis and allow engineers to \\noperate without operational awareness of underlying servers. Many people quibble with \\nthe term serverless; after all, the code must run somewhere. In practice, serverless \\nusually means many invisible servers. \\nCloud services have become increasingly appealing to established businesses with \\nexisting data centers and IT infrastructure. Dynamic, seamless scaling is extremely \\nvaluable to businesses that deal with seasonality (e.g., retail businesses coping with \\nBlack Friday load) and web traffic load spikes. The advent of COVID-19 in 2020 was \\na major driver of cloud adoption, as companies recognized the value of rapidly scaling \\nup data processes to gain insights in a highly uncertain business climate; businesses \\nalso had to cope with substantially increased load due to a spike in online shopping, \\nweb app usage, and remote work. \\nBefore we discuss the nuances of choosing technologies in the cloud, let‚Äôs first discuss \\nwhy migration to the cloud requires a dramatic shift in thinking, specifically on the \\npricing front; this is closely related to FinOps, introduced in ‚ÄúFinOps‚Äù on page 120 . \\nEnterprises that migrate to the cloud often make major deployment errors by not \\nappropriately adapting their practices to the cloud pricing model. \\nA Brief Detour on Cloud Economics \\nTo understand how to use cloud services efficiently through cloud native architecture , \\nyou need to know how clouds make money. This is an extremely complex concept and \\none on which cloud providers offer little transparency. Consider this sidebar a starting \\npoint for your research, discovery, and process development. \\nCloud Services and Credit Default Swaps \\nLet‚Äôs go on a little tangent about credit default swaps. Don‚Äôt worry, this will make sense \\nin a bit. Recall that credit default swaps rose to infamy after the 2007 global financial \\ncrisis. A credit default swap was a mechanism for selling different tiers of risk attached \\nto an asset (e.g., a mortgage). It is not our intention to present this idea in any detail, but \\nrather to offer an analogy wherein many cloud services are similar to financial \\nderivatives; cloud providers not only slice hardware assets into  small pieces through \\nvirtualization, but also sell these pieces with varying technical characteristics and risks \\nattached. While providers are extremely tight -lipped about details of their internal \\nsystems, there are massive opportunities for optimization  and scaling by understanding \\ncloud pricing and exchanging notes with other users.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 144, 'page_label': '145'}, page_content='Location  |  137 \\nLook at the example of archival cloud storage. At the time of this writing, GCP openly'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 145, 'page_label': '146'}, page_content='138  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nadmits that its archival class storage runs on the same clusters as standard cloud storage, \\nyet the price per gigabyte per month of archival storage is roughly 1/17 that of standard \\nstorage. How is this possible? \\nHere‚Äôs our educated guess. When purchasing cloud storage, each disk in a storage \\ncluster has three assets that cloud providers and consumers use. First, it has a certain \\nstorage capacity ‚Äîsay, 10 TB. Second, it supports a certain number of input/output \\noperations (IOPs) per second ‚Äîsay, 100. Third, disks support a certain maximum \\nbandwidth, the maximum read speed for optimally organized files. A magnetic drive \\nmight be capable of reading at 200 MB/s. \\nAny of these limits (IOPs, storage capacity, bandwidth) is a potential bottleneck for a \\ncloud provider. For instance, the cloud provider might have a disk storing 3 TB of data \\nbut hitting maximum IOPs. An alternative to leaving the remaining 7 TB empty is to \\nsell the empty space without selling IOPs. Or, more specifically, sell cheap storage \\nspace and expensive IOPs to discourage reads. \\nMuch like traders of financial derivatives, cloud vendors also deal in risk. In the case \\nof archival storage, vendors are selling a type of insurance, but one that pays out for \\nthe insurer rather than the policy buyer in the event of a catastrophe. While data storage \\ncosts per month are extremely cheap, I risk paying a high price if I ever need to retrieve \\ndata. But this is a price that I will happily pay in a true emergency. \\nSimilar considerations apply to nearly any cloud service. While on -premises servers \\nare essentially sold as commodity hardware, the cost model in the cloud is more subtle. \\nRather than just charging for CPU cores, memory, and features, cloud vendors \\nmonetize characteristics such as durability, reliability, longevity, and predictability; a \\nvariety of compute platforms discount their offerings for workloads that are ephemeral \\nor can be arbitrarily interrupted when capacity is needed elsewhere. \\nCloud ‚â† On Premises \\nThis heading may seem like a silly tautology, but the belief that cloud services are just \\nlike familiar on -premises servers is a widespread cognitive error that plagues cloud \\nmigrations and leads to horrifying bills. This demonstrates a broader issue in te ch that \\nwe refer to as the curse of familiarity. Many new technology products are intentionally \\ndesigned to look like something familiar to facilitate ease of use and accelerate \\nadoption. But, any new technology product has subtleties and wrinkles that users must \\nlearn to identify, accommodate, and optimize. \\nMoving on-premises servers one by one to VMs in the cloud‚Äîknown as simple lift and \\nshift‚Äîis a perfectly reasonable strategy for the initial phase of cloud migration, \\nespecially when a company is facing some kind of financial cliff, such as the need to \\nsign a significant new lease or hardware contract if existing hardware is not shut down. \\nHowever, companies that leave their cloud assets in this initial state are in for a rude \\nshock. On a direct comparison basis, long-running servers in the cloud are significantly \\nmore expensive than their on-premises counterparts.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 146, 'page_label': '147'}, page_content='Location  |  139 \\nThe key to finding value in the cloud is understanding and optimizing the cloud pricing \\nmodel. Rather than deploying a set of long -running servers capable of handling full \\npeak load, use autoscaling to allow workloads to scale down to minimal infrastructure \\nwhen loads are light and up to massive clusters during peak times. To realize discounts \\nthrough more ephemeral, less durable workloads, use reserved or spot instances, or use \\nserverless functions in place of servers. \\nWe often think of this optimization as leading to lower costs, but we should also strive \\nto increase business value  by exploiting the dynamic nature of the cloud. 50 Data \\nengineers can create new value in the cloud by accomplishing things that were \\nimpossible in their on -premises environment. For example, it is possible to quickly \\nspin up massive compute clusters to run complex transformations at scales that were \\nunaffordable for on-premises hardware. \\nData Gravity \\nIn addition to basic errors such as following on -premises operational practices in the \\ncloud, data engineers need to watch out for other aspects of cloud pricing and \\nincentives that frequently catch users unawares. \\nVendors want to lock you into their offerings. Getting data onto the platform is cheap \\nor free on most cloud platforms, but getting data out can be extremely expensive. Be \\naware of data egress fees and their long -term impacts on your business before gettin g \\nblindsided by a large bill. Data gravity is real: once data lands in a cloud, the cost to \\nextract it and migrate processes can be very high. \\nHybrid Cloud \\nAs more established businesses migrate into the cloud, the hybrid cloud model is \\ngrowing in importance. Virtually no business can migrate all of its workloads overnight. \\nThe hybrid cloud model assumes that an organization will indefinitely maintain some \\nworkloads outside the cloud. \\nThere are many reasons to consider a hybrid cloud model. Organizations may believe \\nthat they have achieved operational excellence in certain areas, such as their application \\nstack and associated hardware. Thus, they may migrate only to specific workloads \\nwhere they see immediate benefits in the cloud environment. For example, an on -\\npremises Spark stack is migrated to ephemeral cloud clusters, reducing the operational \\nburden of managing software and hardware for the data engineering team and allowing \\nrapid scaling for large data jobs. \\n \\n50 This is a major point of emphasis in Storment and Fuller, Cloud FinOps.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 147, 'page_label': '148'}, page_content='140  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nThis pattern of putting analytics in the cloud is beautiful because data flows primarily \\nin one direction, minimizing data egress costs ( Figure 4 -3). That is, on -premises \\napplications generate event data that can be pushed to the cloud essentially for free. \\nThe bulk of data remains in the cloud where it is analyzed, while smaller amounts of \\ndata are pushed back to on premises for deploying models to applications, reverse ETL, \\netc. \\n \\nFigure 4-3. A hybrid cloud data flow model minimizing egress costs \\nA new generation of managed hybrid cloud service offerings also allows customers to \\nlocate cloud -managed servers in their data centers. 51  This gives users the ability to \\nincorporate the best features in each cloud alongside on-premises infrastructure. \\nMulticloud \\nMulticloud simply refers to deploying workloads to multiple public clouds. Companies \\nmay have several motivations for multicloud deployments. SaaS platforms often wish \\nto offer services close to existing customer cloud workloads. Snowflake and Databricks \\nprovide their SaaS offerings across multiple clouds for this reason. This is especially \\ncritical for data -intensive applications, where network latency and bandwidth \\nlimitations hamper performance, and data egress costs can be prohibitive. \\nAnother common motivation for employing a multicloud approach is to take advantage \\nof the best services across several clouds. For example, a company might want to \\nhandle its Google Ads and Analytics data on Google Cloud and deploy Kubernetes \\nthrough GKE. And the company might also adopt Azure specifically for Microsoft \\nworkloads. Also, the company may like AWS because it has several best -in-class \\nservices (e.g., AWS Lambda) and enjoys huge mindshare, making it relatively easy to \\nhire AWS-proficient engineers. Any mix of various cloud provider services is possible. \\n \\n51 Examples include Google Cloud Anthos and AWS Outposts.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 148, 'page_label': '149'}, page_content='Location  |  141 \\nGiven the intense competition among the major cloud providers, expect them to offer \\nmore best-of-breed services, making multicloud more compelling. \\nA multicloud methodology has several disadvantages. As we just mentioned, data \\negress costs and networking bottlenecks are critical. Going multicloud can introduce \\nsignificant complexity. Companies must now manage a dizzying array of services \\nacross severa l clouds; cross -cloud integration and security present a considerable \\nchallenge; multicloud networking can be diabolically complicated. \\nA new generation of ‚Äúcloud of clouds‚Äù services aims to facilitate multicloud with \\nreduced complexity by offering services across clouds and seamlessly replicating data \\nbetween clouds or managing workloads on several clouds through a single pane of \\nglass. To cite one example, a Snowflake account runs in a single cloud region, but \\ncustomers can readily spin up other accounts in GCP, AWS, or Azure. Snowflake \\nprovides simple scheduled data replication between these various cloud accounts. The \\nSnowflake interfac e is essentially the same in all of these accounts, removing the \\ntraining burden of switching between cloud-native data services. \\nThe ‚Äúcloud of clouds‚Äù space is evolving quickly; within a few years of this book‚Äôs \\npublication, many more of these services will be available. Data engineers and \\narchitects would do well to maintain awareness of this quickly changing cloud \\nlandscape. \\nDecentralized: Blockchain and the Edge \\nThough not widely used now, it‚Äôs worth briefly mentioning a new trend that might \\nbecome popular over the next decade: decentralized computing. Whereas today‚Äôs \\napplications mainly run on premises and in the cloud, the rise of blockchain, Web 3.0, \\nand edge computing may invert this paradigm. For the moment, decentralized platforms \\nhave proven extremely popular but have not had a significant impact in the data space; \\neven so, keeping an eye on these platforms is worthwhile as you assess technology \\ndecisions. \\nOur Advice \\nFrom our perspective, we are still at the beginning of the transition to the cloud. Thus \\nthe evidence and arguments around workload placement and migration are in flux. The \\ncloud itself is changing, with a shift from the IaaS model built around Amazon EC2 \\nthat drove the early growth of AWS and more generally toward more managed service \\nofferings such as AWS Glue, Google BigQuery, and Snowflake. \\nWe‚Äôve also seen the emergence of new workload placement abstractions. On-premises \\nservices are becoming more cloud -like and abstracted. Hybrid cloud services allow \\ncustomers to run fully managed services within their walls while facilitating tight'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 149, 'page_label': '150'}, page_content='142  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nintegration between local and remote environments. Further, the ‚Äúcloud of clouds‚Äù is \\nbeginning to take shape, fueled by third-party services and public cloud vendors. \\nChoose technologies for the present, but look toward the future \\nAs we mentioned in ‚ÄúToday Versus the Future: Immutable Versus Transitory \\nTechnologies‚Äù on page 120, you need to keep one eye on the present while planning \\nfor unknowns. Right now is a tough time to plan workload placements and migrations. \\nBecause of the fast pace of competition and change in the cloud industry, the decision \\nspace will look very differe nt in five to ten years. It is tempting to take into account \\nevery possible future architecture permutation. \\nWe believe that it is critical to avoid this endless trap of analysis. Instead, plan for the \\npresent. Choose the best technologies for your current needs and concrete plans for the \\nnear future. Choose your deployment platform based on real business needs w hile \\nfocusing on simplicity and flexibility. \\nIn particular, don‚Äôt choose a complex multicloud or hybrid-cloud strategy unless there‚Äôs \\na compelling reason. Do you need to serve data near customers on multiple clouds? Do \\nindustry regulations require you to house certain data in your data centers? Do you have \\na compelling technology need for specific services on two different clouds? Choose a \\nsingle-cloud deployment strategy if these scenarios don‚Äôt apply to you. \\nOn the other hand, have an escape plan. As we‚Äôve emphasized before, every \\ntechnology‚Äîeven open source software ‚Äîcomes with some degree of lock -in. A \\nsingle-cloud strategy has significant advantages of simplicity and integration but comes \\nwith significant lock-in attached. In this instance, we‚Äôre talking about mental flexibility, \\nthe flexibility to evaluate the current state of the world and imagine alternatives. Ideally, \\nyour escape plan will remain locked behind glass, but preparing this plan will help you \\nto make better decisions in the present and give you a way out if things go wrong in the \\nfuture. \\nCloud Repatriation Arguments \\nAs we wrote this book, Sarah Wang and Martin Casado published ‚ÄúThe Cost of Cloud, \\nA Trillion Dollar Paradox‚Äù, an article that generated significant sound and fury in the \\ntech space. Readers widely interpreted the article as a call for the repatriation of cloud \\nworkloads to on-premises servers. They make a somewhat more subtle argument that \\ncompanies should expend significant resources to control cloud spending and should \\nconsider repatriation as a possible option. \\nWe want to take a moment to dissect one part of their discussion. Wang and Casado cite \\nDropbox‚Äôs repatriation of significant workloads from AWS to Dropbox-owned servers \\nas a case study for companies considering similar repatriation moves.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 150, 'page_label': '151'}, page_content='Location  |  143 \\nYou are not Dropbox, nor are you Cloudflare \\nWe believe that this case study is frequently used without appropriate context and is a \\ncompelling example of the false equivalence  logical fallacy. Dropbox provides \\nparticular services where ownership of hardware and data centers can offer a \\ncompetitive advantage. Companies should not rely excessively on Dropbox‚Äôs example \\nwhen assessing cloud and on-premises deployment options. \\nFirst, it‚Äôs important to understand that Dropbox stores enormous amounts of data. The \\ncompany is tight -lipped about exactly how much data it hosts but says it is many \\nexabytes and continues to grow. \\nSecond, Dropbox handles a vast amount of network traffic. We know that its bandwidth \\nconsumption in 2017 was significant enough for the company to add ‚Äúhundreds of \\ngigabits of internet connectivity with transit providers (regional and global ISPs), and \\nhundreds of new peering partners (where we exchange traffic directly rather than \\nthrough an ISP).‚Äù52 The data egress costs would be extremely high in a public cloud \\nenvironment. \\nThird, Dropbox is essentially a cloud storage vendor, but one with a highly specialized \\nstorage product that combines object and block storage characteristics. Dropbox‚Äôs core \\ncompetence is a differential file-update system that can efficiently synchronize actively \\nedited files among users while minimizing network and CPU usage. The product is not \\na good fit for object storage, block storage, or other standard cloud offerings. Dropbox \\nhas instead benefited from building a custom, highly integrated software and hardware \\nstack.53 \\nFourth, while Dropbox moved its core product to its hardware, it continued building \\nout other AWS workloads. This allows Dropbox to focus on building one highly tuned \\ncloud service at an extraordinary scale rather than trying to replace multiple services. \\nDropbox can focus on its core competence in cloud storage and data synchronization \\nwhile offloading software and hardware management in other areas, such as data \\nanalytics.54 \\nOther frequently cited success stories that companies have built outside the cloud \\ninclude Backblaze and Cloudflare, but these offer similar lessons. Backblaze began life \\nas a personal cloud data backup product but has since begun to offer B2, an object \\nstorage service similar to Amazon S3. Backblaze currently stores over an exabyte of \\n \\n52 Raghav Bhargava, ‚ÄúEvolution of Dropbox‚Äôs Edge Network,‚Äù Dropbox.Tech, June 19, 2017, \\nhttps://oreil.ly/RAwPf. \\n53 Akhil Gupta, ‚ÄúScaling to Exabytes and Beyond,‚Äù Dropbox.Tech, March 14, 2016, https://oreil.ly/5XPKv. \\n54 ‚ÄúDropbox Migrates 34 PB of Data to an Amazon S3 Data Lake for Analytics,‚Äù AWS website, 2020, \\nhttps://oreil.ly/wpVoM.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 151, 'page_label': '152'}, page_content='144  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\ndata. Cloudflare claims to provide services for over 25 million internet properties, with \\npoints of presence in over 200 cities and 51 terabits per second (Tbps) of total network \\ncapacity. \\nNetflix offers yet another useful example. The company is famous for running its tech \\nstack on AWS, but this is only partially true. Netflix does run video transcoding on \\nAWS, accounting for roughly 70% of its compute needs in 2017.55 Netflix also runs its \\napplication backend and data analytics on AWS. However, rather than using the AWS \\ncontent distribution network, Netflix has built a custom CDN in collaboration with \\ninternet service providers, utilizing a highly specialized combination of software and \\nhardware. For a company that consumes a substantial slice of all internet \\ntraffic,56building out this critical infrastructure allowed it to deliver high-quality video \\nto a huge customer base cost-effectively. \\nThese case studies suggest that it makes sense for companies to manage their own \\nhardware and network connections in particular circumstances. The biggest modern \\nsuccess stories of companies building and maintaining hardware involve extraordinary \\nscale (exabytes of data, terabits per second of bandwidth, etc.) and limited use cases \\nwhere companies can realize a competitive advantage by engineering highly integrated \\nhardware and software stacks. In addition, all of these companies consume massive \\nnetwork bandwidth, suggesting that data egress charges would be a major cost if they \\nchose to operate fully from a public cloud. \\nConsider continuing to run workloads on premises or repatriating cloud workloads if \\nyou run a truly cloud-scale service. What is cloud scale? You might be at cloud scale if \\nyou are storing an exabyte of data or handling terabits per second of traffic to and from \\nthe internet. (Achieving a terabit per second of internal network traffic is fairly easy.) \\nIn addition, consider owning your servers if data egress costs are a major factor for \\nyour business. To give a concrete example of cloud scale workloads that could benefit \\nfrom repatriation, Apple might gain a significant financial and performance advantage \\nby migrating iCloud storage to its own servers.57 \\nBuild Versus Buy \\n \\n55 Todd Hoff, ‚ÄúThe Eternal Cost Savings of Netflix‚Äôs Internal Spot Market,‚Äù High Scalability, December 4, \\n2017, https://oreil.ly/LLoFt. \\n56 Todd Spangler, ‚ÄúNetflix Bandwidth Consumption Eclipsed by Web Media Streaming Applications,‚Äù Variety, \\nSeptember 10, 2019, https://oreil.ly/tTm3k. \\n57 Amir Efrati and Kevin McLaughlin, ‚ÄúApple‚Äôs Spending on Google Cloud Storage on Track to Soar 50% This \\nYear,‚Äù The Information, June 29, 2021, https://oreil.ly/OlFyR.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 152, 'page_label': '153'}, page_content='Location  |  145 \\nBuild versus buy is an age-old debate in technology. The argument for building is that \\nyou have end-to-end control over the solution and are not at the mercy of a vendor or \\nopen source community. The argument supporting buying comes down to resource \\nconstraints and expertise; do you have the expertise to build a'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 153, 'page_label': '154'}, page_content='146  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nbetter solution than something already available? Either decision comes down to TCO, \\nTOCO, and whether the solution provides a competitive advantage to your \\norganization. \\nIf you‚Äôve caught on to a theme in the book so far, it‚Äôs that we suggest investing in \\nbuilding and customizing when doing so will provide a competitive advantage for your \\nbusiness. Otherwise, stand on the shoulders of giants and use what‚Äô s already available \\nin the market. Given the number of open source and paid services‚Äîboth of which may \\nhave communities of volunteers or highly paid teams of amazing engineers ‚Äî you‚Äôre \\nfoolish to build everything yourself. \\nAs we often ask, ‚ÄúWhen you need new tires for your car, do you get the raw materials, \\ncreate the tires from scratch, and install them yourself?‚Äù Like most people, you‚Äôre \\nprobably buying tires and having someone install them. The same argument applies to \\nbuild versus buy. We‚Äôve seen teams that have built their databases from scratch. A \\nsimple open source RDBMS would have served their needs much better upon closer \\ninspection. Imagine the amount of time and money invested in this homegrown \\ndatabase. Talk about low ROI for TCO and opportunity cost. \\nThis is where the distinction between the type A and type B data engineer comes in \\nhandy. As we pointed out earlier, type A and type B roles are often embodied in the \\nsame engineer, especially in a small organization. Whenever possible, lean toward type \\nA behavior; avoid undifferentiated heavy lifting and embrace abstraction. Use open \\nsource frameworks, or if this is too much trouble, look at buying a suitable managed \\nor proprietary solution. Plenty of great modular services are available to choose from \\nin either case. \\nThe shifting reality of how companies adopt software is worth mentioning. Whereas \\nin the past, IT used to make most of the software purchase and adoption decisions in a \\ntop-down manner, these days, the trend is for bottom -up software adoption in a \\ncompany, driven by developers, data engineers, data scientists, and other technical \\nroles. Technology adoption within companies is becoming an organic, continuous \\nprocess. \\nLet‚Äôs look at some options for open source and proprietary solutions. \\nOpen Source Software \\nOpen source software (OSS) is a software distribution model in which software, and \\nthe underlying codebase, is made available for general use, typically under specific \\nlicensing terms. Often OSS is created and maintained by a distributed team of \\ncollaborators. OSS is free to use, change, and distribute most of the time, but with'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 154, 'page_label': '155'}, page_content='Build Versus Buy |  147 \\nspecific caveats. For example, many licenses require that the source code of open \\nsource‚Äìderived software be included when the software is distributed. \\nThe motivations for creating and maintaining OSS vary. Sometimes OSS is organic, \\nspringing from the mind of an individual or a small team that creates a novel solution \\nand chooses to release it into the wild for public use. Other times, a company may \\nmake a specific tool or technology available to the public under an OSS license. \\nOSS has two main flavors: community managed and commercial OSS. \\nCommunity-managed OSS \\nOSS projects succeed with a strong community and vibrant user base. \\nCommunitymanaged OSS is a prevalent path for OSS projects. The community opens \\nup high rates of innovations and contributions from developers worldwide with \\npopular OSS projects. \\nThe following are factors to consider with a community-managed OSS project: \\nMindshare \\nAvoid adopting OSS projects that don‚Äôt have traction and popularity. Look at the \\nnumber of GitHub stars, forks, and commit volume and recency. Another thing to \\npay attention to is community activity on related chat groups and forums. Does \\nthe project have a strong sense of community? A strong community creates a \\nvirtuous cycle of strong adoption. It also means that you‚Äôll have an easier time \\ngetting technical assistance and finding talent qualified to work with the \\nframework. \\nMaturity \\nHow long has the project been around, how active is it today, and how usable are \\npeople finding it in production? A project‚Äôs maturity indicates that people find it \\nuseful and are willing to incorporate it into their production workflows. \\nTroubleshooting \\nHow will you have to handle problems if they arise? Are you on your own to \\ntroubleshoot issues, or can the community help you solve your problem? \\nProject management \\nLook at Git issues and the way they‚Äôre addressed. Are they addressed quickly? If \\nso, what‚Äôs the process to submit an issue and get it resolved? \\nTeam \\nIs a company sponsoring the OSS project? Who are the core contributors? \\nDeveloper relations and community management'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 155, 'page_label': '156'}, page_content='148  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nWhat is the project doing to encourage uptake and adoption? Is there a vibrant chat \\ncommunity (e.g., in Slack) that provides encouragement and support? \\nContributing \\nDoes the project encourage and accept pull requests? What are the process and \\ntimelines for pull requests to be accepted and included in main codebase? \\nRoadmap \\nIs there a project roadmap? If so, is it clear and transparent? \\nSelf-hosting and maintenance \\nDo you have the resources to host and maintain the OSS solution? If so, what‚Äôs \\nthe TCO and TOCO versus buying a managed service from the OSS vendor? \\nGiving back to the community \\nIf you like the project and are actively using it, consider investing in it. You can \\ncontribute to the codebase, help fix issues, and give advice in the community \\nforums and chats. If the project allows donations, consider making one. Many OSS \\nprojects are  essentially community-service projects, and the maintainers often \\nhave full-time jobs in addition to helping with the OSS project. Sadly, it‚Äôs often a \\nlabor of love that doesn‚Äôt afford the maintainer a living wage. If you can afford to \\ndonate, please do so. \\nCommercial OSS \\nSometimes OSS has some drawbacks. Namely, you have to host and maintain the \\nsolution in your environment. This may be trivial or extremely complicated and \\ncumbersome, depending on the OSS application. Commercial vendors try to solve this \\nmanagement headache by hosting and managing the OSS solution for you, typically as \\na cloud SaaS offering. Examples of such vendors include Databricks (Spark), \\nConfluent (Kafka), DBT Labs (dbt), and there are many, many others. \\nThis model is called commercial OSS (COSS). Typically, a vendor will offer the ‚Äúcore‚Äù \\nof the OSS for free while charging for enhancements, curated code distributions, or \\nfully managed services. \\nA vendor is often affiliated with the community OSS project. As an OSS project \\nbecomes more popular, the maintainers may create a separate business for a managed \\nversion of the OSS. This typically becomes a cloud SaaS platform built around a \\nmanaged version of the open source code. This is a widespread trend: an OSS project \\nbecomes popular, an affiliated company raises truckloads of venture capital (VC) \\nmoney to commercialize the OSS project, and the company scales as a fast -moving \\nrocket ship.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 156, 'page_label': '157'}, page_content='Build Versus Buy |  149 \\nAt this point, the data engineer has two options. You can continue using the \\ncommunity-managed OSS version, which you need to continue maintaining on your \\nown (updates, server/container maintenance, pull requests for bug fixes, etc.). Or, you \\ncan pay the vendor and let it take care of the administrative management of the COSS \\nproduct. \\nThe following are factors to consider with a commercial OSS project: \\nValue \\nIs the vendor offering a better value than if you managed the OSS technology \\nyourself? Some vendors will add many bells and whistles to their managed \\nofferings that aren‚Äôt available in the community OSS version. Are these additions \\ncompelling to you? \\nDelivery model \\nHow do you access the service? Is the product available via download, API, or \\nweb/mobile UI? Be sure you can easily access the initial version and subsequent \\nreleases. \\nSupport \\nSupport cannot be understated, and it‚Äôs often opaque to the buyer. What is the \\nsupport model for the product, and is there an extra cost for support? Frequently, \\nvendors will sell support for an additional fee. Be sure you clearly understand the \\ncosts of obtaining support. Also, understand what is covered in support, and what \\nis not covered. Anything that‚Äôs not covered by support will be your responsibility \\nto own and manage. \\nReleases and bug fixes \\nIs the vendor transparent about the release schedule, improvements, and bug fixes? \\nAre these updates easily available to you? \\nSales cycle and pricing \\nOften a vendor will offer on -demand pricing, especially for a SaaS product, and \\noffer you a discount if you commit to an extended agreement. Be sure to \\nunderstand the trade-offs of paying as you go versus paying up front. Is it worth \\npaying a lump sum, or is your money better spent elsewhere? \\nCompany finances \\nIs the company viable? If the company has raised VC funds, you can check their \\nfunding on sites like Crunchbase. How much runway does the company have, and \\nwill it still be in business in a couple of years? \\nLogos versus revenue \\nIs the company focused on growing the number of customers (logos), or is it trying \\nto grow revenue? You may be surprised by the number of companies primarily'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 157, 'page_label': '158'}, page_content='150  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nconcerned with growing their customer count, GitHub stars, or Slack channel \\nmembership without the revenue to establish sound finances. \\nCommunity support \\nIs the company truly supporting the community version of the OSS project? How \\nmuch is the company contributing to the community OSS codebase? \\nControversies have arisen with certain vendors co -opting OSS projects and \\nsubsequently providing little value back  to the community. How likely will the \\nproduct remain viable as a community -supported open source if the company \\nshuts down? \\nNote also that clouds offer their own managed open source products. If a cloud vendor \\nsees traction with a particular product or project, expect that vendor to offer its version. \\nThis can range from simple examples (open source Linux offered on VMs) to \\nextremely complex managed services (fully managed Kafka). The motivation for these \\nofferings is simple: clouds make their money through consumption. More offerings in \\na cloud ecosystem mean a greater chance of ‚Äústickiness‚Äù and increased customer \\nspending. \\nProprietary Walled Gardens \\nWhile OSS is ubiquitous, a big market also exists for non-OSS technologies. Some of \\nthe biggest companies in the data industry sell closed source products. Let‚Äôs look at \\ntwo major types of proprietary walled gardens , independent companies and \\ncloudplatform offerings. \\nIndependent offerings \\nThe data-tool landscape has seen exponential growth over the last several years. Every \\nday, new independent offerings arise for data tools. With the ability to raise funds from \\nVCs flush with capital, these data companies can scale and hire great engineeri ng, \\nsales, and marketing teams. This presents a situation where users have some great \\nproduct choices in the marketplace while having to wade through endless sales and \\nmarketing clutter. At the time of this writing, the good times of freely available capital \\nfor data companies are coming to an end, but that‚Äôs another long story whose \\nconsequences are still unfolding. \\nOften a company selling a data tool will not release it as OSS, instead offering a \\nproprietary solution. Although you won‚Äôt have the transparency of a pure OSS solution, \\na proprietary independent solution can work quite well, especially as a fully managed \\nservice in the cloud. \\nThe following are things to consider with an independent offering:'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 158, 'page_label': '159'}, page_content='Build Versus Buy |  151 \\nInteroperability \\nMake sure that the tool interoperates with other tools you‚Äôve chosen (OSS, other \\nindependents, cloud offerings, etc.). Interoperability is key, so make sure you can \\ntry it before you buy. \\nMindshare and market share \\nIs the solution popular? Does it command a presence in the marketplace? Does it \\nenjoy positive customer reviews? \\nDocumentation and support \\nProblems and questions will inevitably arise. Is it clear how to solve your problem, \\neither through documentation or support? \\nPricing \\nIs the pricing understandable? Map out low-, medium-, and high-probability usage \\nscenarios, with respective costs. Are you able to negotiate a contract, along with a \\ndiscount? Is it worth it? How much flexibility do you lose if you sign a contract, \\nboth in  negotiation and the ability to try new options? Are you able to obtain \\ncontractual commitments on future pricing? \\nLongevity \\nWill the company survive long enough for you to get value from its product? If \\nthe company has raised money, search around for its funding situation. Look at \\nuser reviews. Ask friends and post questions on social networks about users‚Äô \\nexperiences with the product. Make sure you know what you‚Äôre getting into. \\nCloud platform proprietary service offerings \\nCloud vendors develop and sell their proprietary services for storage, databases, and \\nmore. Many of these solutions are internal tools used by respective sibling companies. \\nFor example, Amazon created the database DynamoDB to overcome the limitations of \\ntraditional relational databases and handle the large amounts of user and order data as \\nAmazon.com grew into a behemoth. Amazon later offered the DynamoDB service \\nsolely on AWS; it‚Äôs now a top -rated product used by companies of all sizes and \\nmaturity levels. Cloud vendors will often bundle their products to work well together. \\nEach cloud can create stickiness with its user base by creating a strong integrated \\necosystem. \\nThe following are factors to consider with a proprietary cloud offering: \\nPerformance versus price comparisons \\nIs the cloud offering substantially better than an independent or OSS version? \\nWhat‚Äôs the TCO of choosing a cloud‚Äôs offering? \\nPurchase considerations'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 159, 'page_label': '160'}, page_content='152  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nOn-demand pricing can be expensive. Can you lower your cost by purchasing \\nreserved capacity or entering into a long-term commitment agreement? \\nOur Advice \\nBuild versus buy comes back to knowing your competitive advantage and where it \\nmakes sense to invest resources toward customization. In general, we favor OSS and \\nCOSS by default, which frees you to focus on improving those areas where these \\noptions are ins ufficient. Focus on a few areas where building something will add \\nsignificant value or reduce friction substantially.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 160, 'page_label': '161'}, page_content='|  153 \\nDon‚Äôt treat internal operational overhead as a sunk cost. There‚Äôs excellent value in \\nupskilling your existing data team to build sophisticated systems on managed platforms \\nrather than babysitting on -premises servers. In addition, think about how a company \\nmakes money, especially its sales and customer experience teams, which will generally \\nindicate how you‚Äôre treated during the sales cycle and when you‚Äôre a paying customer. \\nFinally, who is responsible for the budget at your company? How does this person \\ndecide the projects and technologies that get funded? Before making the business case \\nfor COSS or managed services, does it make sense to try to use OSS first? The last \\nthing you want is for your technology choice to be stuck in limbo while waiting for \\nbudget approval. As the old saying goes, time kills deals. In your case, more time spent \\nin limbo means a higher likelihood your budget approval will die. Know beforehand \\nwho controls the budget and what will successfully get approved. \\nMonolith Versus Modular \\nMonoliths versus modular systems is another longtime debate in the software \\narchitecture space. Monolithic systems are self -contained, often performing multiple \\nfunctions under a single system. The monolith camp favors the simplicity of having \\neverything in one place. It‚Äôs easier to reason about a single entity, and you can move \\nfaster because there are fewer moving parts. The modular camp leans toward \\ndecoupled, best -of-breed technologies performing tasks at which they are uniquely \\ngreat. Especially given the rate of change in products in the data world, the argument \\nis you should aim for interoperability among an ever-changing array of solutions. \\nWhat approach should you take in your data engineering stack? Let‚Äôs explore the trade-\\noffs. \\nMonolith \\nThe monolith (Figure 4-4) has been a technology mainstay for decades. The old days \\nof waterfall meant that software releases were huge, tightly coupled, and moved at a \\nslow cadence. Large teams worked together to deliver a single working codebase. \\nMonolithic data systems continue  to this day, with older software vendors such as \\nInformatica and open source frameworks such as Spark. \\nThe pros of the monolith are it‚Äôs easy to reason about, and it requires a lower cognitive \\nburden and context switching since everything is self-contained. Instead of dealing with \\ndozens of technologies, you deal with ‚Äúone‚Äù technology and typically one prin cipal \\nprogramming language. Monoliths are an excellent option if you want simplicity in \\nreasoning about your architecture and processes.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 161, 'page_label': '162'}, page_content='154  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nMonolith Versus Modular  \\n \\nFigure 4-4. The monolith tightly couples its services \\nOf course, the monolith has cons. For one thing, monoliths are brittle. Because of the \\nvast number of moving parts, updates and releases take longer and tend to bake in ‚Äúthe \\nkitchen sink.‚Äù If the system has a bug‚Äîhopefully, the software‚Äôs been thoroughly tested \\nbefore release!‚Äîit can harm the entire system. \\nUser-induced problems also happen with monoliths. For example, we saw a monolithic \\nETL pipeline that took 48 hours to run. If anything broke anywhere in the pipeline, the \\nentire process had to restart. Meanwhile, anxious business users were waiting for their \\nreports, which were already two days late by default and usually arrived much later. \\nBreakages were common enough that the monolithic system was eventually thrown \\nout. \\nMultitenancy in a monolithic system can also be a significant problem. It can be \\nchallenging to isolate the workloads of multiple users. In an on-prem data warehouse, \\none user-defined function might consume enough CPU to slow the system for other \\nusers. Conflicts between dependencies and resource contention are frequent sources of \\nheadaches. \\nAnother con of monoliths is that switching to a new system will be painful if the vendor \\nor open source project dies. Because all of your processes are contained in the monolith, \\nextracting yourself out of that system, and onto a new platform, will be cost ly in both \\ntime and money. \\nModularity \\nModularity (Figure 4 -5) is an old concept in software engineering, but modular \\ndistributed systems truly came into vogue with the rise of microservices. Instead of \\nrelying on a massive monolith to handle your needs, why not break apart systems and \\nprocesses into their self-contained areas of concern? Microservices can communicate \\nvia APIs, allowing developers to focus on their domains while making their \\napplications accessible to other microservices. This is the trend in software engineering \\nand is increasingly seen in modern data systems.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 162, 'page_label': '163'}, page_content='|  155 \\n \\nFigure 4-5. With modularity, each service is decoupled from another \\nMajor tech companies have been key drivers in the microservices movement. The \\nfamous Bezos API mandate decreases coupling between applications, allowing \\nrefactoring and decomposition. Bezos also imposed the two-pizza rule (no team should \\nbe so large that two pizzas can‚Äôt feed the whole group). Effectively, this means that a \\nteam will have at most five members. This cap also limits the complexity of a team‚Äôs \\ndomain of responsibility‚Äîin particular, the codebase that it can manage. Whereas an \\nextensive monolithic application might entail a group of one hundred people, dividing \\ndevelopers into small groups of five requires that this application be broken into small, \\nmanageable, loosely coupled pieces. \\nIn a modular microservice environment, components are swappable, and it‚Äôs possible \\nto create a polyglot (multiprogramming language) application; a Java service can \\nreplace a service written in Python. Service customers need worry only about the \\ntechnical specifications of the service API, not behind-the-scenes details of \\nimplementation. \\nData-processing technologies have shifted toward a modular model by providing strong \\nsupport for interoperability. Data is stored in object storage in a standard format such \\nas Parquet in data lakes and lakehouses. Any processing tool that supports the for mat \\ncan read the data and write processed results back into the lake for processing by \\nanother tool. Cloud data warehouses support interoperation with object storage through \\nimport/export using standard formats and external tables‚Äî i.e., queries run directly on \\ndata in a data lake. \\nNew technologies arrive on the scene at a dizzying rate in today‚Äôs data ecosystem, and \\nmost get stale and outmoded quickly. Rinse and repeat. The ability to swap out tools as \\ntechnology changes is invaluable. We view data modularity as a more powerful \\nparadigm than monolithic data engineering. Modularity allows engineers to choose the \\nbest technology for each job or step along the pipeline. \\nThe cons of modularity are that there‚Äôs more to reason about. Instead of handling a \\nsingle system of concern, now you potentially have countless systems to understand \\nand operate. Interoperability is a potential headache; hopefully, these systems all play \\nnicely together. \\nThis very problem led us to break out orchestration as a separate undercurrent instead \\nof placing it under data management. Orchestration is also important for monolithic \\ndata architectures; witness the success of tools like BMC Software‚Äôs Control-M in the \\ntraditional data warehousing space. But orchestrating five or ten'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 163, 'page_label': '164'}, page_content='156  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nMonolith Versus Modular  \\ntools is dramatically more complex than orchestrating one. Orchestration becomes the \\nglue that binds data stack modules together. \\nThe Distributed Monolith Pattern \\nThe distributed monolith pattern is a distributed architecture that still suffers from many \\nof the limitations of monolithic architecture. The basic idea is that one runs a distributed \\nsystem with different services to perform different tasks. Still, services and nodes share \\na common set of dependencies or a common codebase. \\nOne standard example is a traditional Hadoop cluster. A Hadoop cluster can \\nsimultaneously host several frameworks, such as Hive, Pig, or Spark. The cluster also \\nhas many internal dependencies. In addition, the cluster runs core Hadoop components: \\nHadoop common libraries, HDFS, Y ARN, and Java. In practice, a cluster often has one \\nversion of each component installed. \\nA standard on -prem Hadoop system entails managing a common environment that \\nworks for all users and all jobs. Managing upgrades and installations is a significant \\nchallenge. Forcing jobs to upgrade dependencies risks breaking them; maintaining two \\nversions of a framework entails extra complexity. \\nSome modern Python -based orchestration technologies ‚Äîe.g., Apache Airflow‚Äîalso \\nsuffer from this problem. While they utilize a highly decoupled and asynchronous \\narchitecture, every service runs the same codebase with the same dependencies. Any \\nexecutor can execute any task, so a client library for a single task run in one DAG must \\nbe installed on the whole cluster. Orchestrating many tools entails installing client \\nlibraries for a host of APIs. Dependency conflicts are a constant problem. \\nOne solution to the problems of the distributed monolith is ephemeral infrastructure in \\na cloud setting. Each job gets its own temporary server or cluster installed with \\ndependencies. Each cluster remains highly monolithic, but separating jobs dramatically \\nreduces conflicts. For example, this pattern is now quite common for Spark with \\nservices like Amazon EMR and Google Cloud Dataproc. \\nA second solution is to properly decompose the distributed monolith into multiple \\nsoftware environments using containers. We have more to say on containers in \\n‚ÄúServerless Versus Servers‚Äù on page 143. \\nOur Advice \\nWhile monoliths are attractive because of ease of understanding and reduced \\ncomplexity, this comes at a high cost. The cost is the potential loss of flexibility, \\nopportunity cost, and high-friction development cycles. \\nHere are some things to consider when evaluating monoliths versus modular options:'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 164, 'page_label': '165'}, page_content='|  157 \\nInteroperability \\nArchitect for sharing and interoperability. \\nAvoiding the ‚Äúbear trap‚Äù \\nSomething that is easy to get into might be painful or impossible to escape. \\nFlexibility \\nThings are moving so fast in the data space right now. Committing to a monolith \\nreduces flexibility and reversible decisions. \\nServerless Versus Servers \\nA big trend for cloud providers is serverless, allowing developers and data engineers to \\nrun applications without managing servers behind the scenes. Serverless provides a \\nquick time to value for the right use cases. For other cases, it might not be a good fit. \\nLet‚Äôs look at how to evaluate whether serverless is right for you. \\nServerless \\nThough serverless has been around for quite some time, the serverless trend kicked off \\nin full force with AWS Lambda in 2014. With the promise of executing small chunks \\nof code on an as-needed basis without having to manage a server, serverless exploded \\nin popularity. The main reasons for its popularity are cost and convenience. Instead of \\npaying the cost of a server, why not just pay when your code is evoked? \\nServerless has many flavors. Though function as a service (FaaS) is wildly popular, \\nserverless systems predate the advent of AWS Lambda. For example, Google Cloud‚Äôs \\nBigQuery is serverless in that data engineers don‚Äôt need to manage backend \\ninfrastructure, and the system scales to zero and scales up automatically to handle large \\nqueries. Just load data into the system and start querying. You pay for the amount of \\ndata your query consumes and a small cost to store your data. This payment model ‚Äî\\npaying for cons umption and storage ‚Äîis becoming more prevalent. When does \\nserverless make sense? As with many other cloud services, it depends; and data \\nengineers would do well to understand the details of cloud pricing to predict when \\nserverless deployments will become e xpensive. Looking specifically at the case of \\nAWS Lambda, various engineers have found hacks to run batch workloads at meager \\ncosts.58  On the other hand, serverless functions suffer from an inherent overhead \\ninefficiency. Handling one event per function call at a high event rate can be \\n \\n58 Evan Sangaline, ‚ÄúRunning FFmpeg on AWS Lambda for 1.9% the Cost of AWS Elastic Transcoder,‚Äù Intoli \\nblog, May 2, 2018, https://oreil.ly/myzOv. \\nServerless Versus Servers'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 165, 'page_label': '166'}, page_content='158  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\ncatastrophically expensive, especially when simpler approaches like multithreading or \\nmultiprocessing are great alternatives. \\nAs with other areas of ops, it‚Äôs critical to monitor and model. Monitor to determine cost \\nper event in a real -world environment and maximum length of serverless execution, \\nand model using this cost per event to determine overall costs as event rates grow. \\nModeling should also include worst-case scenarios‚Äîwhat happens if my site gets hit \\nby a bot swarm or DDoS attack? \\nContainers \\nIn conjunction with serverless and microservices, containers are one of the most \\npowerful trending operational technologies as of this writing. Containers play a role in \\nboth serverless and microservices. \\nContainers are often referred to as lightweight virtual machines. Whereas a traditional \\nVM wraps up an entire operating system, a container packages an isolated user space \\n(such as a filesystem and a few processes); many such containers can coexist on a single \\nhost operating system. This provides some of the principal benefits of virtualization \\n(i.e., dependency and code isolation) without the overhead of carrying around an entire \\noperating system kernel. \\nA single hardware node can host numerous containers with fine -grained resource \\nallocations. At the time of this writing, containers continue to grow in popularity, along \\nwith Kubernetes, a container management system. Serverless environments typically \\nrun on containers behind the scenes. Indeed, Kubernetes is a kind of serverless \\nenvironment because it allows developers and ops teams to deploy microservices \\nwithout worrying about the details of the machines where they are deployed. Containers \\nprovide a partial solution to problems of the distributed monolith mentioned earlier in \\nthis chapter. For example, Hadoop now supports containers, allowing each job to have \\nits own isolated dependencies. \\nContainer clusters do not provide the same security and isolation \\nthat full VMs offer. Container escape‚Äîbroadly, a class of exploits \\nwhereby code in a container gains privileges outside the container \\nat the OS level ‚Äîis common enough to be considered a risk for \\nmultitenancy. While Amazon EC2 is a truly multitenant \\nenvironment with VMs from many customers hosted on the same hardware, a \\nKubernetes cluster should host code only within an environment of mutual trust \\n(e.g., inside the walls of a single company). In addition, code review processes \\nand vulnerability scanning are critical to ensure that a developer doesn‚Äôt introduce \\na security hole. \\nVarious flavors of container platforms add additional serverless features. Containerized \\nfunction platforms run containers as ephemeral units triggered by events rather than'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 166, 'page_label': '167'}, page_content='|  159 \\npersistent services. 59  This gives users the simplicity of AWS Lambda with the full \\nflexibility of a container environment instead of the highly restrictive Lambda runtime. \\nAnd services such as AWS Fargate and Google App Engine run containers without \\nmanaging a compute cluster required for Kubernetes. These services also fully isolate \\ncontainers, preventing the security issues associated with multitenancy. \\nAbstraction will continue working its way across the data stack. Consider the impact of \\nKubernetes on cluster management. While you can manage your Kubernetes cluster‚Äî\\nand many engineering teams do so‚Äîeven Kubernetes is widely available as a managed \\nservice. What comes after Kubernetes? We‚Äôre as excited as you to find out. \\nHow to Evaluate Server Versus Serverless \\nWhy would you want to run your own servers instead of using serverless? There are a \\nfew reasons. Cost is a big factor. Serverless makes less sense when the usage and cost \\nexceed the ongoing cost of running and maintaining a server (Figure 4-6). However, at \\na certain scale, the economic benefits of serverless may diminish, and running servers \\nbecomes more attractive. \\n \\nFigure 4-6. Cost of serverless versus utilizing a server \\nCustomization, power, and control are other major reasons to favor servers over \\nserverless. Some serverless frameworks can be underpowered or limited for certain use \\ncases. Here are some things to consider when using servers, particularly in the cloud, \\nwhere server resources are ephemeral: \\nExpect servers to fail. \\nServer failure will happen. Avoid using a ‚Äúspecial snowflake‚Äù server that is overly \\ncustomized and brittle, as this introduces a glaring vulnerability in your \\narchitecture. Instead, treat servers as ephemeral resources that you can create as \\nneeded and then delete. If your application requires specific code to be installed \\n \\n59 Examples include OpenFaaS, Knative, and Google Cloud Run. \\nServerless Versus Servers'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 167, 'page_label': '168'}, page_content='160  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\non the server, use a boot script or build an image. Deploy code to the server \\nthrough a CI/CD pipeline. \\nUse clusters and autoscaling. \\nTake advantage of the cloud‚Äôs ability to grow and shrink compute resources on \\ndemand. As your application increases its usage, cluster your application servers, \\nand use autoscaling capabilities to automatically horizontally scale your \\napplication as demand grows. \\nTreat your infrastructure as code. \\nAutomation doesn‚Äôt apply to just servers and should extend to your infrastructure \\nwhenever possible. Deploy your infrastructure (servers or otherwise) using \\ndeployment managers such as Terraform, AWS CloudFormation, and Google \\nCloud Deployment Manager. \\nUse containers. \\nFor more sophisticated or heavy -duty workloads with complex installed \\ndependencies, consider using containers on either a single server or Kubernetes. \\nOur Advice \\nHere are some key considerations to help you determine whether serverless is right for \\nyou: \\nWorkload size and complexity \\nServerless works best for simple, discrete tasks and workloads. It‚Äôs not as suitable \\nif you have many moving parts or require a lot of compute or memory horsepower. \\nIn that case, consider using containers and a container workflow orchestration \\nframework like Kubernetes. \\nExecution frequency and duration \\nHow many requests per second will your serverless application process? How long \\nwill each request take to process? Cloud serverless platforms have limits on \\nexecution frequency, concurrency, and duration. If your application can‚Äôt function \\nneatly within these limits, it is time to consider a container-oriented approach. \\nRequests and networking \\nServerless platforms often utilize some form of simplified networking and don‚Äôt \\nsupport all cloud virtual networking features, such as VPCs and firewalls. \\nLanguage \\nWhat language do you typically use? If it‚Äôs not one of the officially supported \\nlanguages supported by the serverless platform, you should consider containers \\ninstead. \\nRuntime limitations'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 168, 'page_label': '169'}, page_content='|  161 \\nServerless platforms don‚Äôt give you complete operating system abstractions. \\nInstead, you‚Äôre limited to a specific runtime image. \\nCost \\nServerless functions are incredibly convenient but potentially expensive. When \\nyour serverless function processes only a few events, your costs are low; costs rise \\nrapidly as the event count increases. This scenario is a frequent source of surprise \\ncloud bills. \\nIn the end, abstraction tends to win. We suggest looking at using serverless first and \\nthen servers‚Äîwith containers and orchestration if possible ‚Äîonce you‚Äôve outgrown \\nserverless options. \\nOptimization, Performance, and the \\nBenchmark Wars \\nImagine that you are a billionaire shopping for new transportation. You‚Äôve narrowed \\nyour choice to two options: \\n‚Ä¢ 787 Business Jet \\n‚Äî Range: 9,945 nautical miles (with 25 passengers) \\n‚Äî Maximum speed: 0.90 Mach \\n‚Äî Cruise speed: 0.85 Mach \\n‚Äî Fuel capacity: 101,323 kilograms \\n‚Äî Maximum takeoff weight: 227,930 kilograms \\n‚Äî Maximum thrust: 128,000 pounds \\n‚Ä¢ Tesla Model S Plaid \\n‚Äî Range: 560 kilometers \\n‚Äî Maximum speed: 322 kilometers/hour \\n‚Äî 0‚Äì100 kilometers/hour: 2.1 seconds \\n‚Äî Battery capacity: 100 kilowatt hours \\n‚Äî Nurburgring lap time: 7 minutes, 30.9 seconds \\n‚Äî Horsepower: 1020 \\n‚Äî Torque: 1050 lb-ft \\nWhich of these options offers better performance? You don‚Äôt have to know much about \\ncars or aircraft to recognize that this is an idiotic comparison. One option is a wide -'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 169, 'page_label': '170'}, page_content='162  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nbody private jet designed for intercontinental operation, while the other is an electric \\nsupercar. \\nOptimization, Performance, and the Benchmark Wars  \\nWe see such apples-to-oranges comparisons made all the time in the database space. \\nBenchmarks either compare databases that are optimized for completely different use \\ncases, or use test scenarios that bear no resemblance to real-world needs. \\nRecently, we saw a new round of benchmark wars flare up among major vendors in the \\ndata space. We applaud benchmarks and are glad to see many database vendors finally \\ndropping DeWitt clauses from their customer contracts.60 Even so, let the buyer beware: \\nthe data space is full of nonsensical benchmarks.61 Here are a few common tricks used \\nto place a thumb on the benchmark scale. \\nBig Data...for the 1990s \\nProducts that claim to support ‚Äúbig data‚Äù at petabyte scale will often use benchmark \\ndatasets small enough to easily fit in the storage on your smartphone. For systems that \\nrely on caching layers to deliver performance, test datasets fully reside in solids tate \\ndrive (SSD) or memory, and benchmarks can show ultra -high performance by \\nrepeatedly querying the same data. A small test dataset also minimizes RAM and SSD \\ncosts when comparing pricing. \\nTo benchmark for real-world use cases, you must simulate anticipated real-world data \\nand query size. Evaluate query performance and resource costs based on a detailed \\nevaluation of your needs. \\nNonsensical Cost Comparisons \\nNonsensical cost comparisons are a standard trick when analyzing a price/performance \\nor TCO. For instance, many MPP systems can‚Äôt be readily created and deleted even \\nwhen they reside in a cloud environment; these systems run for years on end once \\nthey‚Äôve b een configured. Other databases support a dynamic compute model and \\ncharge either per query or per second of use. Comparing ephemeral and nonephemeral \\nsystems on a cost -per-second basis is nonsensical, but we see this all the time in \\nbenchmarks. \\n \\n60 Justin Olsson and Reynold Xin, ‚ÄúEliminating the Anti-competitive DeWitt Clause for Database \\nBenchmarking,‚Äù Databricks, November 8, 2021, https://oreil.ly/3iFOE. \\n61 For a classic of the genre, see William McKnight and Jake Dolezal, ‚ÄúData Warehouse in the Cloud \\nBenchmark,‚Äù GigaOm, February 7, 2019, https://oreil.ly/QjCmA.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 170, 'page_label': '171'}, page_content='|  163 \\nAsymmetric Optimization \\nThe deceit of asymmetric optimization appears in many guises, but here‚Äôs one example. \\nOften a vendor will compare a row-based MPP system against a columnar database by \\nusing a benchmark that runs complex join queries on highly normalized data. The \\nnormalized data model is optimal for the row -based system, but the columnar system \\nwould realize its full potential only with some schema changes. To make matters worse, \\nvendors juice their systems with an extra shot of join optimization (e.g., preindexing \\njoins) without applying comparable tuning in the competing database (e.g., putting \\njoins in a materialized view). \\nCaveat Emptor \\nAs with all things in data technology, let the buyer beware. Do your homework before \\nblindly relying on vendor benchmarks to evaluate and choose technology. \\nUndercurrents and Their \\nImpacts on Choosing \\nTechnologies \\nAs seen in this chapter, a data engineer has a lot to consider when evaluating \\ntechnologies. Whatever technology you choose, be sure to understand how it supports \\nthe undercurrents of the data engineering lifecycle. Let‚Äôs briefly review them again. \\nData Management \\nData management is a broad area, and concerning technologies, it isn‚Äôt always apparent \\nwhether a technology adopts data management as a principal concern. For example, \\nbehind the scenes, a third -party vendor may use data management best practices ‚Äî\\nregulatory compliance, security, privacy, data quality, and governance‚Äî but hide these \\ndetails behind a limited UI layer. In this case, while evaluating the product, it helps to \\nask the company about its data management practices. Here are some sample questions \\nyou should ask: \\n‚Ä¢ How are you protecting data against breaches, both from the outside and within? \\n‚Ä¢ What is your product‚Äôs compliance with GDPR, CCPA, and other data privacy \\nregulations? \\n‚Ä¢ Do you allow me to host my data to comply with these regulations? \\n‚Ä¢ How do you ensure data quality and that I‚Äôm viewing the correct data in your \\nsolution?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 171, 'page_label': '172'}, page_content='164  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nThere are many other questions to ask, and these are just a few of the ways to think \\nabout data management as it relates to choosing the right technologies. These same \\nquestions should also apply to the OSS solutions you‚Äôre considering. \\nDataOps \\nProblems will happen. They just will. A server or database may die, a cloud‚Äôs region \\nmay have an outage, you might deploy buggy code, bad data might be introduced into \\nyour data warehouse, and other unforeseen problems may occur. \\nUndercurrents and Their Impacts on Choosing Technologies  \\nWhen evaluating a new technology, how much control do you have over deploying new \\ncode, how will you be alerted if there‚Äôs a problem, and how will you respond when \\nthere‚Äôs a problem? The answer largely depends on the type of technology you‚Äôre \\nconsidering. If the technology is OSS, you‚Äôre likely responsible for setting up \\nmonitoring, hosting, and code deployment. How will you handle issues? What‚Äôs your \\nincident response? \\nMuch of the operations are outside your control if you‚Äôre using a managed offering. \\nConsider the vendor‚Äôs SLA, the way they alert you to issues, and whether they‚Äôre \\ntransparent about how they‚Äôre addressing the case, including providing an ETA to a fix. \\nData Architecture \\nAs discussed in Chapter 3 , good data architecture means assessing trade -offs and \\nchoosing the best tools for the job while keeping your decisions reversible. With the \\ndata landscape morphing at warp speed, the best tool for the job is a moving target. The \\nmain goals are to avoid unnecessary lock -in, ensure interoperability across the data \\nstack, and produce high ROI. Choose your technologies accordingly. \\nOrchestration Example: Airflow \\nThroughout most of this chapter, we have actively avoided discussing any particular \\ntechnology too extensively. We make an exception for orchestration because the space \\nis currently dominated by one open source technology, Apache Airflow. \\nMaxime Beauchemin kicked off the Airflow project at Airbnb in 2014. Airflow was \\ndeveloped from the beginning as a noncommercial open source project. The framework \\nquickly grew significant mindshare outside Airbnb, becoming an Apache Incubator \\nproject in 2016 and a full Apache-sponsored project in 2019. \\nAirflow enjoys many advantages, largely because of its dominant position in the open \\nsource marketplace. First, the Airflow open source project is extremely active, with a \\nhigh rate of commits and a quick response time for bugs and security issues, and the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 172, 'page_label': '173'}, page_content='|  165 \\nproject recently released Airflow 2, a major refactor of the codebase. Second, Airflow \\nenjoys massive mindshare. Airflow has a vibrant, active community on many \\ncommunications platforms, including Slack, Stack Overflow, and GitHub. Users can \\neasily find an swers to questions and problems. Third, Airflow is available \\ncommercially as a managed service or software distribution through many vendors, \\nincluding GCP, AWS, and Astronomer.io. \\nAirflow also has some downsides. Airflow relies on a few core nonscalable components \\n(the scheduler and backend database) that can become bottlenecks for performance, \\nscale, and reliability; the scalable parts of Airflow still follow a distributed monolith  \\npattern. (See ‚ÄúMonolith Versus Modular‚Äù on page 139.) Finally, Airflow lacks support \\nfor many data-native constructs, such as schema management, lineage, and cataloging; \\nand it is challenging to develop and test Airflow workflows. \\nWe do not attempt an exhaustive discussion of Airflow alternatives here but just \\nmention a couple of the key orchestration contenders at the time of writing. Prefect and \\nDagster aim to solve some of the problems discussed previously by rethinking \\ncomponents of the Airflow architecture. Will there be other orchestration frameworks \\nand technologies not discussed here? Plan on it. \\nWe highly recommend that anyone choosing an orchestration technology study the \\noptions discussed here. They should also acquaint themselves with activity in the space, \\nas new developments will certainly occur by the time you read this. \\nSoftware Engineering \\nAs a data engineer, you should strive for simplification and abstraction across the data \\nstack. Buy or use prebuilt open source solutions whenever possible. Eliminating \\nundifferentiated heavy lifting should be your big goal. Focus your resources‚Äîcustom \\ncoding and tooling‚Äîon areas that give you a solid competitive advantage. For example, \\nis hand-coding a database connection between your production database and your cloud \\ndata warehouse a competitive advantage for you? Probably not. This is very much a \\nsolved problem. Pick an off-the-shelf solution (open source or managed SaaS) instead. \\nThe world doesn‚Äôt need the millionth +1 database-to-cloud data warehouse connector. \\nOn the other hand, why do customers buy from you? Your business likely has \\nsomething special about the way it does things. Maybe it‚Äôs a particular algorithm that \\npowers your fintech platform. By abstracting away a lot of the redundant workflows \\nand processes, you can continue chipping away, refining, and customizing the things \\nthat move the needle for the business. \\nConclusion'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 173, 'page_label': '174'}, page_content='166  | Chapter 4: Choosing Technologies Across the Data Engineering Lifecycle \\nChoosing the right technologies is no easy task, especially when new technologies and \\npatterns emerge daily. Today is possibly the most confusing time in history for \\nevaluating and selecting technologies. Choosing technologies is a balance of use case, \\ncost, build versus buy, and modularization. Always approach technology the same way \\nas architecture: assess trade-offs and aim for reversible decisions. \\nAdditional Resources \\n‚Ä¢ Cloud FinOps by J. R. Storment and Mike Fuller (O‚ÄôReilly) \\n‚Ä¢ ‚ÄúCloud Infrastructure: The Definitive Guide for Beginners‚Äù by Matthew Smith \\nConclusion  \\n‚Ä¢ ‚ÄúThe Cost of Cloud, a Trillion Dollar Paradox‚Äù by Sarah Wang and Martin \\nCasado \\n‚Ä¢ FinOps Foundation‚Äôs ‚ÄúWhat Is FinOps‚Äù web page \\n‚Ä¢ ‚ÄúRed Hot: The 2021 Machine Learning, AI and Data (MAD) Landscape‚Äù by Matt \\nTurck \\n‚Ä¢ Ternary Data‚Äôs ‚ÄúWhat‚Äôs Next for Analytical Databases? w/ Jordan Tigani \\n(MotherDuck)‚Äù video \\n‚Ä¢ ‚ÄúThe Unfulfilled Promise of Serverless‚Äù by Corey Quinn \\n‚Ä¢ ‚ÄúWhat Is the Modern Data Stack?‚Äù by Charles Wang'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 174, 'page_label': '175'}, page_content='PART II \\nThe Data Engineering \\nLifecycle in Depth'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 175, 'page_label': '176'}, page_content=''),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 176, 'page_label': '177'}, page_content='169 \\nCHAPTE\\nR 5 \\nData \\nGeneration in Source \\nSystems \\nWelcome to the first stage of the data engineering lifecycle: data generation in source \\nsystems. As we described earlier, the job of a data engineer is to take data from source \\nsystems, do something with it, and make it helpful in serving downstream use ca ses. \\nBut before you get raw data, you must understand where the data exists, how it is \\ngenerated, and its characteristics and quirks. \\nThis chapter covers some popular operational source system patterns and the \\nsignificant types of source systems. Many source systems exist for data generation, \\nand we‚Äôre not exhaustively covering them all. We‚Äôll consider the data these systems \\ngenerate and things you should consider when working with source systems. We also \\ndiscuss how the undercurrents of data engineering apply to this first phase of the data \\nengineering lifecycle (Figure 5-1).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 177, 'page_label': '178'}, page_content='170  | Chapter 5: Data Generation in Source Systems \\n \\nFigure 5-1. Source systems generate the data for the rest of the data engineering \\nlifecycle As data proliferates, especially with the rise of data sharing (discussed next), \\nwe expect that a data engineer‚Äôs role will shift heavily toward understanding the \\ninterplay between data sources and destinations. The basic plumbing tasks of data \\nengineering‚Äîmoving data from A to B‚Äîwill simplify dramatically. On the other \\nhand, it will remain critical to understand the nature of data as it‚Äôs created in source \\nsystems. \\nSources of Data: How Is Data Created? \\nAs you learn about the various underlying operational patterns of the systems that \\ngenerate data, it‚Äôs essential to understand how data is created. Data is an unorganized, \\ncontext-less collection of facts and figures. It can be created in many ways, both analog \\nand digital. \\nAnalog data creation occurs in the real world, such as vocal speech, sign language, \\nwriting on paper, or playing an instrument. This analog data is often transient; how \\noften have you had a verbal conversation whose contents are lost to the ether after the \\nconversation ends? \\nDigital data is either created by converting analog data to digital form or is the native \\nproduct of a digital system. An example of analog to digital is a mobile texting app that \\nconverts analog speech into digital text. An example of digital data creation is a credit \\ncard transaction on an ecommerce platform. A customer places an order, the transaction \\nis charged to their credit card, and the information for the transaction is saved to \\nvarious databases.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 178, 'page_label': '179'}, page_content='Source Systems: Main Ideas  | \\n 171 \\nWe‚Äôll utilize a few common examples in this chapter, such as data created when \\ninteracting with a website or mobile application. But in truth, data is everywhere in the \\nworld around us. We capture data from IoT devices, credit card terminals, telescope \\nsensors, stock trades, and more. \\nGet familiar with your source system and how it generates data. Put in the effort to \\nread the source system documentation and understand its patterns and quirks. If your \\nsource system is an RDBMS, learn how it operates (writes, commits, queries, etc.); \\nlearn the ins and outs of the source system that might affect your ability to ingest from \\nit. \\nSource Systems: Main Ideas \\nSource systems produce data in various ways. This section discusses the main ideas \\nyou‚Äôll frequently encounter as you work with source systems. \\nFiles and Unstructured Data \\nA file is a sequence of bytes, typically stored on a disk. Applications often write data \\nto files. Files may store local parameters, events, logs, images, and audio. \\nIn addition, files are a universal medium of data exchange. As much as data engineers \\nwish that they could get data programmatically, much of the world still sends and \\nreceives files. For example, if you‚Äôre getting data from a government agency, there‚Äôs \\nan excellent chance you‚Äôll download the data as an Excel or CSV file or receive the \\nfile in an email. \\nThe main types of source file formats you‚Äôll run into as a data engineer ‚Äîfiles that \\noriginate either manually or as an output from a source system process ‚Äîare Excel, \\nCSV , TXT, JSON, and XML. These files have their quirks and can be structured (Excel, \\nCSV), semistructured (JSON, XML, CSV), or unstructured (TXT, CSV). Although \\nyou‚Äôll use certain formats heavily as a data engineer (such as Parquet, ORC, and Avro), \\nwe‚Äôll cover these later and put the spotlight here on source system files. Chapter 6 \\ncovers the technical details of files. \\nAPIs \\nApplication programming interfaces  (APIs) are a standard way of exchanging data \\nbetween systems. In theory, APIs simplify the data ingestion task for data engineers. \\nIn practice, many APIs still expose a good deal of data complexity for engineers to \\nmanage. Even with the rise of various se rvices and frameworks, and services for \\nautomating API data ingestion, data engineers must often invest a good deal of energy'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 179, 'page_label': '180'}, page_content='172  | Chapter 5: Data Generation in Source Systems \\ninto maintaining custom API connections. We discuss APIs in greater detail later in \\nthis chapter. \\nApplication Databases (OLTP Systems) \\nAn application database stores the state of an application. A standard example is a \\ndatabase that stores account balances for bank accounts. As customer transactions and \\npayments happen, the application updates bank account balances. \\nTypically, an application database is an online transaction processing  (OLTP) \\nsystem‚Äî a database that reads and writes individual data records at a high rate. OLTP \\nsystems are often referred to as transactional databases, but this does not necessarily \\nimply that the system in question supports atomic transactions. \\nMore generally, OLTP databases support low latency and high concurrency. An \\nRDBMS database can select or update a row in less than a millisecond (not accounting \\nfor network latency) and handle thousands of reads and writes per second. A document \\ndatabase cluster can manage even higher document commit rates at the expense of \\npotential inconsistency. Some graph databases can also handle transactional use cases. \\nFundamentally, OLTP databases work well as application backends when thousands or \\neven millions of users might be interacting with the application simultaneously, \\nupdating and writing data concurrently. OLTP systems are less suited to use cases \\ndriven by analytics at scale, where a single query must scan a vast amount of data. \\nACID \\nSupport for atomic transactions is one of a critical set of database characteristics known \\ntogether as ACID (as you may recall from Chapter 3 , this stands for atomicity, \\nconsistency, isolation, durability ). Consistency means that any database read will \\nreturn the last written version of the retrieved item. Isolation entails that if two updates \\nare in flight concurrently for the same thing, the end database state will be consistent \\nwith the sequential execution of these updates in the order they were submitt ed. \\nDurability indicates that committed data will never be lost, even in the event of power \\nloss. \\nNote that ACID characteristics are not required to support application backends, and \\nrelaxing these constraints can be a considerable boon to performance and scale. \\nHowever, ACID characteristics guarantee that the database will maintain a consistent \\npicture of the world, dramatically simplifying the app developer‚Äôs task. \\nAll engineers (data or otherwise) must understand operating with and without ACID. \\nFor instance, to improve performance, some distributed databases use relaxed \\nconsistency constraints, such as eventual consistency, to improve performance.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 180, 'page_label': '181'}, page_content='Source Systems: Main Ideas  | \\n 173 \\nUnderstanding the consistency model you‚Äôre working with helps you prevent \\ndisasters. \\nAtomic transactions \\nAn atomic transaction is a set of several changes that are committed as a unit. In the \\nexample in Figure 5 -2, a traditional banking application running on an RDBMS \\nexecutes a SQL statement that checks two account balances, one in Account A (the \\nsource) and another in Account B (the destination). Money is then moved from \\nAccount A to Account B if sufficient funds  are in Account A. The entire transaction \\nshould run with updates to both account balances or fail without updating either \\naccount balance. That is, the whole operation should happen as a transaction. \\n \\nFigure 5-2. Example of an atomic transaction: a bank account transfer using OLTP \\nOLTP and analytics \\nOften, small companies run analytics directly on an OLTP. This pattern works in the \\nshort term but is ultimately not scalable. At some point, running analytical queries on \\nOLTP runs into performance issues due to structural limitations of OLTP or resource \\ncontention with competing transactional workloads. Data engineers must understand \\nthe inner workings of OLTP and application backends to set up appropriate integrations \\nwith analytics systems without degrading production application performance. \\nAs companies offer more analytics capabilities in SaaS applications, the need for \\nhybrid capabilities‚Äîquick updates with combined analytics capabilities‚Äîhas created \\nnew challenges for data engineers. We‚Äôll use the term data application to refer to \\napplications that hybridize transactional and analytics workloads. \\nOnline Analytical Processing System \\nIn contrast to an OLTP system, an online analytical processing (OLAP) system is built \\nto run large analytics queries and is typically inefficient at handling lookups of \\nindividual records. For example, modern column databases are optimized to scan large \\nvolumes of data, dispensing with indexes to improve scalability and scan performance. \\nAny query typically involves scanning a minimal data block, often 100 MB or more in \\nsize. Trying to look up thousands of individual items per second in such a system will'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 181, 'page_label': '182'}, page_content='174  | Chapter 5: Data Generation in Source Systems \\nbring it to its knees unless it is combined with a caching layer designed for this use \\ncase. \\nNote that we‚Äôre using the term OLAP to refer to any database system that supports \\nhigh-scale interactive analytics queries; we are not limiting ourselves to systems that \\nsupport OLAP cubes (multidimensional arrays of data). The online part of OLAP \\nimplies that the system constantly listens for incoming queries, making OLAP systems \\nsuitable for interactive analytics. \\nAlthough this chapter covers source systems, OLAPs are typically storage and query \\nsystems for analytics. Why are we talking about them in our chapter on source \\nsystems? In practical use cases, engineers often need to read data from an OLAP \\nsystem. For example, a data warehouse might serve data used to train an ML model. \\nOr, an OLAP system might serve a reverse ETL workflow, where derived data in an \\nanalytics system is sent back to a source system, such as a CRM, SaaS platform, or \\ntransactional application. \\nChange Data Capture \\nChange data capture  (CDC) is a method for extracting each change event (insert, \\nupdate, delete) that occurs in a database. CDC is frequently leveraged to replicate \\nbetween databases in near real time or create an event stream for downstream \\nprocessing. \\nCDC is handled differently depending on the database technology. Relational \\ndatabases often generate an event log stored directly on the database server that can be \\nprocessed to create a stream. (See ‚ÄúDatabase Logs‚Äù on page 161.) Many cloud NoSQL \\ndatabases can send a log or event stream to a target storage location. \\nLogs \\nA log captures information about events that occur in systems. For example, a log may \\ncapture traffic and usage patterns on a web server. Your desktop computer‚Äôs operating \\nsystem (Windows, macOS, Linux) logs events as the system boots and when \\napplications start or crash, for example. \\nLogs are a rich data source, potentially valuable for downstream data analysis, ML, \\nand automation. Here are a few familiar sources of logs: \\n‚Ä¢ Operating systems \\n‚Ä¢ Applications \\n‚Ä¢ Servers \\n‚Ä¢ Containers'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 182, 'page_label': '183'}, page_content='Source Systems: Main Ideas  | \\n 175 \\n‚Ä¢ Networks \\n‚Ä¢ IoT devices \\nAll logs track events and event metadata. At a minimum, a log should capture who, \\nwhat, and when: \\nWho \\nThe human, system, or service account associated with the event (e.g., a web \\nbrowser user agent or a user ID) \\nWhat happened \\nThe event and related metadata \\nWhen \\nThe timestamp of the event \\nLog encoding \\nLogs are encoded in a few ways: \\nBinary-encoded logs \\nThese encode data in a custom compact format for space efficiency and fast I/O. \\nDatabase logs, discussed in ‚ÄúDatabase Logs‚Äù on page 161, are a standard example. \\nSemistructured logs \\nThese are encoded as text in an object serialization format (JSON, more often than \\nnot). Semistructured logs are machine -readable and portable. However, they are \\nmuch less efficient than binary logs. And though they are nominally machine -\\nreadable, extracting value from them often requires significant custom code. \\nPlain-text (unstructured) logs \\nThese essentially store the console output from software. As such, no \\ngeneralpurpose standards exist. These logs can provide helpful information for \\ndata scientists and ML engineers, though extracting useful information from the \\nraw text data might be complicated. \\nLog resolution \\nLogs are created at various resolutions and log levels. The log resolution refers to the \\namount of event data captured in a log. For example, database logs capture enough \\ninformation from database events to allow reconstructing the database state at any \\npoint in time. \\nOn the other hand, capturing all data changes in logs for a big data system often isn‚Äôt \\npractical. Instead, these logs may note only that a particular type of commit event has \\noccurred. The log level  refers to the conditions required to record a log entry,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 183, 'page_label': '184'}, page_content='176  | Chapter 5: Data Generation in Source Systems \\nspecifically concerning errors and debugging. Software is often configurable to log \\nevery event or to log only errors, for example. \\nLog latency: Batch or real time \\nBatch logs are often written continuously to a file. Individual log entries can be written \\nto a messaging system such as Kafka or Pulsar for real-time applications. \\nDatabase Logs \\nDatabase logs  are essential enough that they deserve more detailed coverage. \\nWriteahead logs‚Äîtypically, binary files stored in a specific database-native format‚Äî\\nplay a crucial role in database guarantees and recoverability. The database server \\nreceives write and update  requests to a database table (see Figure 5-3), storing each \\noperation in the log before acknowledging the request. The acknowledgment comes \\nwith a log -associated guarantee: even if the server fails, it can recover its state on \\nreboot by completing the unfinished work from the logs. \\nDatabase logs are extremely useful in data engineering, especially for CDC to generate \\nevent streams from database changes. \\n \\nFigure 5-3. Database logs record operations on a table \\nCRUD \\nCRUD, which stands for create, read, update, and delete, is a transactional pattern \\ncommonly used in programming and represents the four basic operations of persistent \\nstorage. CRUD is the most common pattern for storing application state in a database. \\nA basic tenet of CRUD is that data must be created before  being used. After the data \\nhas been created, the data can be read and updated. Finally, the data may need to be'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 184, 'page_label': '185'}, page_content='Source Systems: Main Ideas  | \\n 177 \\ndestroyed. CRUD guarantees these four operations will occur on data, regardless of its \\nstorage. \\nCRUD is a widely used pattern in software applications, and you‚Äôll commonly find \\nCRUD used in APIs and databases. For example, a web application will make heavy \\nuse of CRUD for RESTful HTTP requests and storing and retrieving data from a \\ndatabase. \\nAs with any database, we can use snapshot-based extraction to get data from a database \\nwhere our application applies CRUD operations. On the other hand, event extraction \\nwith CDC gives us a complete history of operations and potentially allows for near \\nreal-time analytics. \\nInsert-Only \\nThe insert-only pattern retains history directly in a table containing data. Rather than \\nupdating records, new records get inserted with a timestamp indicating when they were \\ncreated (Table 5-1). For example, suppose you have a table of customer addresses. \\nFollowing a CRUD pattern, you would simply update the record if the customer \\nchanged their address. With the insert -only pattern, a new address record is inserted \\nwith the same customer ID. To read the current customer address by customer ID, you \\nwould look up the latest record under that ID. \\nTable 5-1. An insert-only pattern produces multiple versions of a record \\nRecord \\nID \\nValue Timestamp \\n1 40 2021-09-\\n19T00:10:23+00:00 \\n1 51 2021-09-\\n30T00:12:00+00:00 \\nIn a sense, the insert-only pattern maintains a database log directly in the table itself, \\nmaking it especially useful if the application needs access to history. For example, the \\ninsert-only pattern would work well for a banking application designed to pr esent \\ncustomer address history. \\nA separate analytics insert-only pattern is often used with regular CRUD application \\ntables. In the insert-only ETL pattern, data pipelines insert a new record in the target \\nanalytics table anytime an update occurs in the CRUD table. \\nInsert-only has a couple of disadvantages. First, tables can grow quite large, especially \\nif data frequently changes, since each change is inserted into the table. Sometimes \\nrecords are purged based on a record sunset date or a maximum number of record \\nversions to keep table size reasonable. The second disadvantage is that record lookups \\nincur extra overhead because looking up the current state involves running MAX'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 185, 'page_label': '186'}, page_content='178  | Chapter 5: Data Generation in Source Systems \\n(created_timestamp). If hundreds or thousands of records are under a single ID, \\nthis lookup operation is expensive to run. \\nMessages and Streams \\nRelated to event -driven architecture, two terms that you‚Äôll often see used \\ninterchangeably are message queue and streaming platform, but a subtle but essential \\ndifference exists between the two. Defining and contrasting these terms is worthwhile \\nsince they encompass many big ideas related to source systems and practices and \\ntechnologies spanning the entire data engineering lifecycle. \\nA message is raw data communicated across two or more systems ( Figure 5-4). For \\nexample, we have System 1 and System 2, where System 1 sends a message to System \\n2. These systems could be different microservices, a server sending a message to a \\nserverless function, etc. A message is typically sent through a message queue from a \\npublisher to a consumer, and once the message is delivered, it is removed from the \\nqueue. \\n \\nFigure 5-4. A message passed between two systems \\nMessages are discrete and singular signals in an event-driven system. For example, an \\nIoT device might send a message with the latest temperature reading to a message \\nqueue. This message is then ingested by a service that determines whether the furnace \\nshould be turned on or off. This service sends a message to a furnace controller that \\ntakes the appropriate action. Once the message is received, and the action is taken, the \\nmessage is removed from the message queue. \\nBy contrast, a stream is an append-only log of event records. (Streams are ingested and \\nstored in event-streaming platforms, which we discuss at greater length in ‚ÄúMessages \\nand Streams‚Äù on page 163 .) As events occur, they are accumulated in an ordered \\nsequence (Figure 5-5); a timestamp or an ID might order events. (Note that events \\naren‚Äôt always delivered in exact order because of the subtleties of distributed systems.) \\n \\nFigure 5-5. A stream, which is an ordered append-only log of records'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 186, 'page_label': '187'}, page_content='Source Systems: Main Ideas  | \\n 179 \\nYou‚Äôll use streams when you care about what happened over many events. Because of \\nthe append -only nature of streams, records in a stream are persisted over a long \\nretention window ‚Äîoften weeks or months ‚Äîallowing for complex operations on \\nrecords such as aggregations on multiple records or the ability to rewind to a point in \\ntime within the stream. \\nIt‚Äôs worth noting that systems that process streams can process messages, and \\nstreaming platforms are frequently used for message passing. We often accumulate \\nmessages in streams when we want to perform message analytics. In our IoT example, \\nthe temperature readings that trigger the furnace to turn on or off might also be later \\nanalyzed to determine temperature trends and statistics. \\nTypes of Time \\nWhile time is an essential consideration for all data ingestion, it becomes that much \\nmore critical and subtle in the context of streaming, where we view data as continuous \\nand expect to consume it shortly after it is produced. Let‚Äôs look at the key types of time \\nyou‚Äôll run into when ingesting data: the time that the event is generated, when it‚Äôs \\ningested and processed, and how long processing took (Figure 5-6).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 187, 'page_label': '188'}, page_content='180  | Chapter 5: Data Generation in Source Systems \\n \\nFigure 5-6. Event, ingestion, process, and processing time \\nEvent time  indicates when an event is generated in a source system, including the \\ntimestamp of the original event itself. An undetermined time lag will occur upon event \\ncreation, before the event is ingested and processed downstream. Always include \\ntimestamps for each phase through which an event travels. Log events as they occur \\nand at each stage of time ‚Äîwhen they‚Äôre created, ingested, and processed. Use these \\ntimestamp logs to accurately track the movement of your data through your data \\npipelines. \\nAfter data is created, it is ingested somewhere. Ingestion time indicates when an event \\nis ingested from source systems into a message queue, cache, memory, object storage, \\na database, or any place else that data is stored (see Chapter 6). After ingestion, data \\nmay be processed immediately; or within minutes, hours, or days; or simply persist in \\nstorage indefinitely. \\nProcess time  occurs after ingestion time, when the data is processed (typically, a \\ntransformation). Processing time is how long the data took to process, measured in \\nseconds, minutes, hours, etc. \\nYou‚Äôll want to record these various times, preferably in an automated way. Set up \\nmonitoring along your data workflows to capture when events occur, when they‚Äôre \\ningested and processed, and how long it took to process events. \\nSource System Practical Details \\nThis section discusses the practical details of interacting with modern source systems. \\nWe‚Äôll dig into the details of commonly encountered databases, APIs, and other aspects. \\nThis information will have a shorter shelf life than the main ideas discussed previously; \\npopular API frameworks, databases, and other details will continue to change rapidly. \\nNevertheless, these details are critical knowledge for working data engineers. We \\nsuggest that you study this information as baseline knowledge but read extensively to \\nstay abreast of ongoing developments.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 188, 'page_label': '189'}, page_content='Source System Practical Details  | \\n 181 \\nDatabases \\nIn this section, we‚Äôll look at common source system database technologies that you‚Äôll \\nencounter as a data engineer and high -level considerations for working with these \\nsystems. There are as many types of databases as there are use cases for data. \\nMajor considerations for understanding database technologies \\nHere, we introduce major ideas that occur across a variety of database technologies, \\nincluding those that back software applications and those that support analytics use \\ncases: \\nDatabase management system \\nA database system used to store and serve data. Abbreviated as DBMS, it consists \\nof a storage engine, query optimizer, disaster recovery, and other key components \\nfor managing the database system. \\nLookups \\nHow does the database find and retrieve data? Indexes can help speed up lookups, \\nbut not all databases have indexes. Know whether your database uses indexes; if \\nso, what are the best patterns for designing and maintaining them? Understand \\nhow to leverage for efficient extraction. It also helps to have a basic knowledge of \\nthe major types of indexes, including B -tree and log -structured merge -trees \\n(LSM). \\nQuery optimizer \\nDoes the database utilize an optimizer? What are its characteristics? \\nScaling and distribution \\nDoes the database scale with demand? What scaling strategy does it deploy? Does \\nit scale horizontally (more database nodes) or vertically (more resources on a \\nsingle machine)? \\nModeling patterns \\nWhat modeling patterns work best with the database (e.g., data normalization or \\nwide tables)? (See Chapter 8 for our discussion of data modeling.) \\nCRUD \\nHow is data queried, created, updated, and deleted in the database? Every type of \\ndatabase handles CRUD operations differently. \\nConsistency \\nIs the database fully consistent, or does it support a relaxed consistency model \\n(e.g., eventual consistency)? Does the database support optional consistency \\nmodes for reads and writes (e.g., strongly consistent reads)?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 189, 'page_label': '190'}, page_content='182  | Chapter 5: Data Generation in Source Systems \\nWe divide databases into relational and nonrelational categories. In truth, the \\nnonrelational category is far more diverse, but relational databases still occupy \\nsignificant space in application backends. \\nRelational databases \\nA relational database management system  (RDBMS) is one of the most common \\napplication backends. Relational databases were developed at IBM in the 1970s and \\npopularized by Oracle in the 1980s. The growth of the internet saw the rise of the \\nLAMP stack (Linux, Apache web server, MySQL, PHP) and an  explosion of vendor \\nand open source RDBMS options. Even with the rise of NoSQL databases (described \\nin the following section), relational databases have remained extremely popular. \\nData is stored in a table of relations (rows), and each relation contains multiple fields \\n(columns); see Figure 5-7. Note that we use the terms column and field interchangeably \\nthroughout this book. Each relation in the table has the same schema (a sequence of \\ncolumns with assigned static types such as string, integer, or float). Rows are typically \\nstored as a contiguous sequence of bytes on disk. \\n \\nFigure 5-7. RDBMS stores and retrieves data at a row level \\nTables are typically indexed by a primary key, a unique field for each row in the table. \\nThe indexing strategy for the primary key is closely connected with the layout of the \\ntable on disk. \\nTables can also have various foreign keys‚Äîfields with values connected with the \\nvalues of primary keys in other tables, facilitating joins, and allowing for complex \\nschemas that spread data across multiple tables. In particular, it is possible to design a \\nnormalized schema. Normalization is a strategy for ensuring that data in records is not \\nduplicated in multiple places, thus avoiding the need to update states in multiple \\nlocations at once and preventing inconsistencies (see Chapter 8). \\nRDBMS systems are typically ACID compliant. Combining a normalized schema, \\nACID compliance, and support for high transaction rates makes relational database \\nsystems ideal for storing rapidly changing application states. The challenge for data \\nengineers is to determine how to capture state information over time.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 190, 'page_label': '191'}, page_content='Source System Practical Details  | \\n 183 \\nA full discussion of the theory, history, and technology of RDBMS is beyond the scope \\nof this book. We encourage you to study RDBMS systems, relational algebra, and \\nstrategies for normalization because they‚Äôre widespread, and you‚Äôll encounter them \\nfrequently. See ‚ÄúAdditional Resources‚Äù on page 188 for suggested books. \\nNonrelational databases: NoSQL \\nWhile relational databases are terrific for many use cases, they‚Äôre not a one-size-fitsall \\nsolution. We often see that people start with a relational database under the impression \\nit‚Äôs a universal appliance and shoehorn in a ton of use cases and workloads.  As data \\nand query requirements morph, the relational database collapses under its weight. At \\nthat point, you‚Äôll want to use a database that‚Äôs appropriate for the specific workload \\nunder pressure. Enter nonrelational or NoSQL databases. NoSQL, which stands for not \\nonly SQL, refers to a whole class of databases that abandon the relational paradigm. \\nOn the one hand, dropping relational constraints can improve performance, scalability, \\nand schema flexibility. But as always in architecture, trade -offs exist. NoSQL \\ndatabases also typically abandon various RDBMS characteristics, such as strong \\nconsistency, joins, or a fixed schema. \\nA big theme of this book is that data innovation is constant. Let‚Äôs take a quick look at \\nthe history of NoSQL, as it‚Äôs helpful to gain a perspective on why and how data \\ninnovations impact your work as a data engineer. In the early 2000s, tech companies \\nsuch as Google and Amazon began to outgrow their relational databases and pioneered \\nnew distributed, nonrelational databases to scale their web platforms. \\nWhile the term NoSQL first appeared in 1998, the modern version was coined by Eric \\nEvans in the 2000s.62 He tells the story in a 2009 blog post: \\nI‚Äôve spent the last couple of days at nosqleast and one of the hot topics here is the name \\n‚Äúnosql.‚Äù Understandably, there are a lot of people who worry that the name is Bad, \\nthat it sends an inappropriate or inaccurate message. While I make no claims to the \\nidea, I do have to accept some blame for what it is now being called. How‚Äôs that? Johan \\nOskarsson was organizing the first meetup and asked the question ‚ÄúWhat‚Äôs a good \\nname?‚Äù on IRC; it was one of three or four suggestions that I spouted off in the span \\nof like 45 seconds, without thinking. \\nMy regret, however, isn‚Äôt about what the name says; it‚Äôs about what it doesn‚Äôt. When \\nJohan originally had the idea for the first meetup, he seemed to be thinking Big Data \\nand linearly scalable distributed systems, but the name is so vague that it opened th e \\ndoor to talk submissions for literally anything that stored data, and wasn‚Äôt an RDBMS. \\n \\n62 Keith D. Foote, ‚ÄúA Brief History of Non-Relational Databases,‚Äù Dataversity, June 19, 2018, \\nhttps://oreil.ly/5Ukg2.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 191, 'page_label': '192'}, page_content='184  | Chapter 5: Data Generation in Source Systems \\nNoSQL remains vague in 2022, but it‚Äôs been widely adopted to describe a universe of \\n‚Äúnew school‚Äù databases, alternatives to relational databases. \\nThere are numerous flavors of NoSQL database designed for almost any imaginable \\nuse case. Because there are far too many NoSQL databases to cover exhaustively in \\nthis section, we consider the following database types: key -value, document, wide -\\ncolumn, graph, search, and time series. These databases are all wildly popular and \\nenjoy widespread adoption. A data engineer should understand these types of \\ndatabases, including usage considerations, the structure of the data they store, and how \\nto leverage each in the data engineering lifecycle. \\nKey-value stores.    A key-value database is a nonrelational database that retrieves \\nrecords using a key that uniquely identifies each record. This is similar to hash map or \\ndictionary data structures presented in many programming languages but potentially \\nmore scalable. Key -value stores encompas s several NoSQL database types ‚Äîfor \\nexample, document stores and wide column databases (discussed next). Different types \\nof key-value databases offer a variety of performance characteristics to serve various \\napplication needs. For example, in -memory key -value databases are popular for \\ncaching session data for web and mobile applications, where ultra-fast lookup and high \\nconcurrency are required. Storage in these systems is typically temporary; if the \\ndatabase shuts down, the data disappears. Such caches can reduce pressure on the main \\napplication database and serve speedy responses. \\nOf course, key -value stores can also serve applications requiring high -durability \\npersistence. An ecommerce application may need to save and update massive amounts \\nof event state changes for a user and their orders. A user logs into the ecommerce \\napplication, clicks around various screens, adds items to a shopping cart, and then \\nchecks out. Each event must be durably stored for retrieval. Key -value stores often \\npersist data to disk and across multiple nodes to support such use cases. \\nDocument stores.    As mentioned previously, a document store is a specialized key-\\nvalue store. In this context, a document is a nested object; we can usually think of each \\ndocument as a JSON object for practical purposes. Documents are stored in collections \\nand retrieved by key. A collection is roughly equivalent to a table in a relational \\ndatabase (see Table 5-2). \\nTable 5-2. Comparison of RDBMS and document terminology \\nRDBMS Document \\ndatabase \\nTable Collection \\nRow Document, \\nitems, entity'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 192, 'page_label': '193'}, page_content='Source System Practical Details  | \\n 185 \\nOne key difference between relational databases and document stores is that the latter \\ndoes not support joins. This means that data cannot be easily normalized, i.e., split \\nacross multiple tables. (Applications can still join manually. Code can look up a \\ndocument, extract a property, and then retrieve another document.) Ideally, all related \\ndata can be stored in the same document. \\nIn many cases, the same data must be stored in multiple documents spread across \\nnumerous collections; software engineers must be careful to update a property \\neverywhere it is stored. (Many document stores support a notion of transactions to \\nfacilitate this.) \\nDocument databases generally embrace all the flexibility of JSON and don‚Äôt enforce \\nschema or types; this is a blessing and a curse. On the one hand, this allows the schema \\nto be highly flexible and expressive. The schema can also evolve as an application \\ngrows. On the flip side, we‚Äôve seen document databases become absolute nightmares \\nto manage and query. If developers are not careful in managing schema evolution, data \\nmay become inconsistent and bloated over time. Schema evolution can also break \\ndownstream ingestion and cause headaches for data engineers if it‚Äôs not communicated \\nin a timely fashion (before deployment). \\nThe following is an example of data that is stored in a collection called users. The \\ncollection key is the id. We also have a name (along with first and last as child \\nelements) and an array of the user‚Äôs favorite bands within each document: \\n{ \\n  \"users\":[ \\n     { \\n     \"id\":1234, \\n     \"name\":{ \\n     \"first\":\"Joe\", \\n     \"last\":\"Reis\" \\n     }, \\n     \"favorite_bands\":[ \\n     \"AC/DC\", \\n     \"Slayer\", \\n     \"WuTang Clan\", \\n     \"Action Bronson\" \\n     ] \\n     }, \\n     { \\n     \"id\":1235, \\n     \"name\":{ \\n     \"first\":\"Matt\", \\n     \"last\":\"Housley\" \\n     }, \\n     \"favorite_bands\":[      \\n\"Dave Matthews Band\",'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 193, 'page_label': '194'}, page_content='186  | Chapter 5: Data Generation in Source Systems \\n     \"Creed\", \\n     \"Nickelback\" \\n     ] \\n     } \\n  ] \\n} \\nTo query the data in this example, you can retrieve records by key. Note that most \\ndocument databases also support the creation of indexes and lookup tables to allow \\nretrieval of documents by specific properties. This is often invaluable in application \\ndevelopment when you need to search for documents in various ways. For example, \\nyou could set an index on name. \\nAnother critical technical detail for data engineers is that document stores are generally \\nnot ACID compliant, unlike relational databases. Technical expertise in a particular \\ndocument store is essential to understanding performance, tuning, configuration,  \\nrelated effects on writes, consistency, durability, etc. For example, many document \\nstores are eventually consistent. Allowing data distribution across a cluster is a boon \\nfor scaling and performance but can lead to catastrophes when engineers and \\ndevelopers don‚Äôt understand the implications.63 \\nTo run analytics on document stores, engineers generally must run a full scan to extract \\nall data from a collection or employ a CDC strategy to send events to a target stream. \\nThe full scan approach can have both performance and cost implications. The scan  \\noften slows the database as it runs, and many serverless cloud offerings charge a \\nsignificant fee for each full scan. In document databases, it‚Äôs often helpful to create an \\nindex to help speed up queries. We discuss indexes and query patterns in Chapter 8. \\nWide-column.    A wide-column database is optimized for storing massive amounts \\nof data with high transaction rates and extremely low latency. These databases can \\nscale to extremely high write rates and vast amounts of data. Specifically, widecolumn \\ndatabases can support petabytes of data, millions of requests per second, and sub-10ms \\nlatency. These characteristics have made wide -column databases popular in \\necommerce, fintech, ad tech, IoT, and real -time personalization applications. Data \\nengineers must be aware of the operational characteristics of the wide -column \\ndatabases they work with to set up a suitable configuration, design the schema, and \\nchoose an appropriate row key to optimize performance and avoid common \\noperational issues. \\nThese databases support rapid scans of massive amounts of data, but they do not \\nsupport complex queries. They have only a single index (the row key) for lookups. \\n \\n63 Nemil Dalal‚Äôs excellent series on the history of MongoDB recounts some harrowing tales of database abuse \\nand its consequences for fledgling startups.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 194, 'page_label': '195'}, page_content='Source System Practical Details  | \\n 187 \\nData engineers must generally extract data and send it to a secondary analytics system \\nto run complex queries to deal with these limitations. This can be accomplished by \\nrunning large scans for the extraction or employing CDC to capture an event stream. \\nGraph databases.    Graph databases explicitly store data with a mathematical graph \\nstructure (as a set of nodes and edges). 64 Neo4j has proven extremely popular, while \\nAmazon, Oracle, and other vendors offer their graph database products. Roughly \\nspeaking, graph databases are a good fit when you want to analyze the connectivity \\nbetween elements. \\nFor example, you could use a document database to store one document for each user \\ndescribing their properties. You could add an array element for connections that \\ncontains directly connected users‚Äô IDs in a social media context. It‚Äôs pretty easy to \\ndetermine the number of direct connections a user has, but suppose you want to know \\nhow many users can be reached by traversing two direct connections. You could \\nanswer this question by writing complex code, but each query would run slowly and \\nconsume significant resources. The document store is simply not optimized for this use \\ncase. \\nGraph databases are designed for precisely this type of query. Their data structures \\nallow for queries based on the connectivity between elements; graph databases are \\nindicated when we care about understanding complex traversals between elements. In \\nthe parlance of graphs, we store nodes (users in the preceding example) and edges \\n(connections between users). Graph databases support rich data models for both nodes \\nand edges. Depending on the underlying graph database engine, graph databases utilize \\nspecialized query languages such as SPARQL, Resource Description Framework \\n(RDF), Graph Query Language (GQL), and Cypher. \\nAs an example of a graph, consider a network of four users. User 1 follows User 2, \\nwho follows User 3 and User 4; User 3 also follows User 4 (Figure 5-8). \\n \\nFigure 5-8. A social network graph \\n \\n64 Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA: O‚ÄôReilly, 2017), 49, \\nhttps://oreil.ly/v1NhG.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 195, 'page_label': '196'}, page_content='188  | Chapter 5: Data Generation in Source Systems \\nWe anticipate that graph database applications will grow dramatically outside of tech \\ncompanies; market analyses also predict rapid growth.65 Of course, graph databases are \\nbeneficial from an operational perspective and support the kinds of complex social \\nrelationships critical to modern applications. Graph structures are also fascinating from \\nthe perspective of data science and ML, potentially revealing deep insights into human \\ninteractions and behavior. \\nThis introduces unique challenges for data engineers who may be more accustomed to \\ndealing with structured relations, documents, or unstructured data. Engineers must \\nchoose whether to do the following: \\n‚Ä¢ Map source system graph data into one of their existing preferred paradigms \\n‚Ä¢ Analyze graph data within the source system itself \\n‚Ä¢ Adopt graph-specific analytics tools \\nGraph data can be reencoded into rows in a relational database, which may be a suitable \\nsolution depending on the analytics use case. Transactional graph databases are also \\ndesigned for analytics, although large queries may overload production systems. \\nContemporary cloud -based graph databases support read -heavy graph analytics on \\nmassive quantities of data. \\nSearch.    A search database is a nonrelational database used to search your data‚Äôs \\ncomplex and straightforward semantic and structural characteristics. Two prominent \\nuse cases exist for a search database: text search and log analysis. Let‚Äôs cover each of \\nthese separately. \\nText search involves searching a body of text for keywords or phrases, matching on \\nexact, fuzzy, or semantically similar matches. Log analysis  is typically used for \\nanomaly detection, real-time monitoring, security analytics, and operational analytics. \\nQueries can be optimized and sped up with the use of indexes. \\nDepending on the type of company you work at, you may use search databases either \\nregularly or not at all. Regardless, it‚Äôs good to be aware they exist in case you come \\nacross them in the wild. Search databases are popular for fast search and retrieval and \\ncan be found in various applications; an ecommerce site may power its product search \\nusing a search database. As a data engineer, you might be expected to bring data from \\na search database (such as Elasticsearch, Apache Solr or Lucene, or Algolia) into \\ndownstream KPI reports or something similar. \\n \\n65 Aashish Mehra, ‚ÄúGraph Database Market Worth $5.1 Billion by 2026: Exclusive Report by \\nMarketsandMarkets,‚Äù Cision PR Newswire, July 30, 2021, https://oreil.ly/mGVkY.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 196, 'page_label': '197'}, page_content='Source System Practical Details  | \\n 189 \\nTime series.    A time series is a series of values organized by time. For example, \\nstock prices might move as trades are executed throughout the day, or a weather sensor \\nwill take atmospheric temperatures every minute. Any events that are recorded over \\ntime‚Äîeither regularly or sporadically‚Äîare time-series data. A time-series database is \\noptimized for retrieving and statistical processing of time-series data. \\nWhile time-series data such as orders, shipments, logs, and so forth have been stored \\nin relational databases for ages, these data sizes and volumes were often tiny. As data \\ngrew faster and bigger, new special -purpose databases were needed. Time -series \\ndatabases address the needs of growing, high -velocity data volumes from IoT, event \\nand application logs, ad tech, and fintech, among many other use cases. Often these \\nworkloads are write -heavy. As a result, time -series databases often utilize memory \\nbuffering to support fast writes and reads. \\nWe should distinguish between measurement and event-based data, common in time-\\nseries databases. Measurement data is generated regularly, such as temperature or air-\\nquality sensors. Event-based data is irregular and created every time an event occurs‚Äî\\nfor instance, when a motion sensor detects movement. \\nThe schema for a time series typically contains a timestamp and a small set of fields. \\nBecause the data is time-dependent, the data is ordered by the timestamp. This makes \\ntime-series databases suitable for operational analytics but not great for BI use ca ses. \\nJoins are not common, though some quasi time-series databases such as Apache Druid \\nsupport joins. Many time-series databases are available, both as open source and paid \\noptions. \\nAPIs \\nAPIs are now a standard and pervasive way of exchanging data in the cloud, for SaaS \\nplatforms, and between internal company systems. Many types of API interfaces exist \\nacross the web, but we are principally interested in those built around HTTP, the most \\npopular type on the web and in the cloud. \\nREST \\nWe‚Äôll first talk about REST, currently the dominant API paradigm. As noted in Chapter \\n4, REST stands for representational state transfer . This set of practices and \\nphilosophies for building HTTP web APIs was laid out by Roy Fielding in 2000 in a \\nPhD dissertation. REST is built around HTTP verbs, such as GET and PUT; in practice, \\nmodern REST uses only a handful of the verb mappings outlined  in the original \\ndissertation. \\nOne of the principal ideas of REST is that interactions are stateless. Unlike in a Linux \\nterminal session, there is no notion of a session with associated state variables such as \\na working directory; each REST call is independent. REST calls can change the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 197, 'page_label': '198'}, page_content='190  | Chapter 5: Data Generation in Source Systems \\nsystem‚Äôs state, but these changes are global, applying to the full system rather than a \\ncurrent session. \\nCritics point out that REST is in no way a full specification. 66 REST stipulates basic \\nproperties of interactions, but developers utilizing an API must gain a significant \\namount of domain knowledge to build applications or pull data effectively. \\nWe see great variation in levels of API abstraction. In some cases, APIs are merely a \\nthin wrapper over internals that provides the minimum functionality required to protect \\nthe system from user requests. In other examples, a REST data API is a masterpiece of \\nengineering that prepares data for analytics applications and supports advanced \\nreporting. \\nA couple of developments have simplified setting up data -ingestion pipelines from \\nREST APIs. First, data providers frequently supply client libraries in various \\nlanguages, especially in Python. Client libraries remove much of the boilerplate labor \\nof build ing API interaction code. Client libraries handle critical details such as \\nauthentication and map fundamental methods into accessible classes. \\nSecond, various services and open source libraries have emerged to interact with APIs \\nand manage data synchronization. Many SaaS and open source vendors provide off -\\nthe-shelf connectors for common APIs. Platforms also simplify the process of building \\ncustom connectors as required. \\nThere are numerous data APIs without client libraries or out -of-the-box connector \\nsupport. As we emphasize throughout the book, engineers would do well to reduce \\nundifferentiated heavy lifting by using off -the-shelf tools. However, low -level \\nplumbing tasks still consume many resources. At virtually any large company, data \\nengineers will need to deal with the problem of writing and maintaining custom code \\nto pull data from APIs, which requires understanding the structure of the data as \\nprovided, developing appropriate data-extraction code, and determining a suitable data \\nsynchronization strategy. \\nGraphQL \\nGraphQL was created at Facebook as a query language for application data and an \\nalternative to generic REST APIs. Whereas REST APIs generally restrict your queries \\nto a specific data model, GraphQL opens up the possibility of retrieving multiple data \\nmodels in a single request. This allows for more flexible and expressive queries than \\nwith REST. GraphQL is built around JSON and returns data in a shape resembling the \\nJSON query. \\n \\n66  For one example, see Michael S. Mikowski, ‚ÄúRESTful APIs: The Big Lie,‚Äù August 10, 2015, \\nhttps://oreil.ly/rqja3.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 198, 'page_label': '199'}, page_content='Source System Practical Details  | \\n 191 \\nThere‚Äôs something of a holy war between REST and GraphQL, with some engineering \\nteams partisans of one or the other and some using both. In reality, engineers will \\nencounter both as they interact with source systems. \\nWebhooks \\nWebhooks are a simple event-based data-transmission pattern. The data source can be \\nan application backend, a web page, or a mobile app. When specified events happen in \\nthe source system, this triggers a call to an HTTP endpoint hosted by the data \\nconsumer. Notice that the connection goes from the source system to the data sink, the \\nopposite of typical APIs. For this reason, webhooks are often called reverse APIs. \\nThe endpoint can do various things with the POST event data, potentially triggering a \\ndownstream process or storing the data for future use. For analytics purposes, we‚Äôre \\ninterested in collecting these events. Engineers commonly use message queues to \\ningest data at high velocity and volume. We will talk about message queues and event \\nstreams later in this chapter. \\nRPC and gRPC \\nA remote procedure call (RPC) is commonly used in distributed computing. It allows \\nyou to run a procedure on a remote system. \\ngRPC is a remote procedure call library developed internally at Google in 2015 and \\nlater released as an open standard. Its use at Google alone would be enough to merit \\ninclusion in our discussion. Many Google services, such as Google Ads and GCP, offer \\ngRPC APIs. gRPC is built around the Protocol Buffers open data serialization standard, \\nalso developed by Google. \\ngRPC emphasizes the efficient bidirectional exchange of data over HTTP/2. Efficiency \\nrefers to aspects such as CPU utilization, power consumption, battery life, and \\nbandwidth. Like GraphQL, gRPC imposes much more specific technical standards \\nthan REST, thus allowing the use of common client libraries and allowing engineers \\nto develop a skill set that will apply to any gRPC interaction code. \\nData Sharing \\nThe core concept of cloud data sharing is that a multitenant system supports security \\npolicies for sharing data among tenants. Concretely, any public cloud object storage \\nsystem with a fine -grained permission system can be a platform for data sharing. \\nPopular cloud data -warehouse platforms also support data -sharing capabilities. Of \\ncourse, data can also be shared through download or exchange over email, but a \\nmultitenant system makes the process much easier.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 199, 'page_label': '200'}, page_content='192  | Chapter 5: Data Generation in Source Systems \\nMany modern sharing platforms (especially cloud data warehouses) support row, \\ncolumn, and sensitive data filtering. Data sharing also streamlines the notion of the \\ndata marketplace , available on several popular clouds and data platforms. Data \\nmarketplaces provide a centralized location for data commerce, where data providers \\ncan advertise their offerings and sell them without worrying about the details of \\nmanaging network access to data systems. \\nData sharing can also streamline data pipelines within an organization. Data sharing \\nallows units of an organization to manage their data and selectively share it with other \\nunits while still allowing individual units to manage their compute and query cost s \\nseparately, facilitating data decentralization. This facilitates decentralized data \\nmanagement patterns such as data mesh.67 \\nData sharing and data mesh align closely with our philosophy of common architecture \\ncomponents. Choose common components (see Chapter 3) that allow the simple and \\nefficient interchange of data and expertise rather than embracing the most exciting and \\nsophisticated technology. \\nThird-Party Data Sources \\nThe consumerization of technology means every company is essentially now a \\ntechnology company. The consequence is that these companies ‚Äîand increasingly \\ngovernment agencies‚Äîwant to make their data available to their customers and users, \\neither as part of their service or as a separate subscription. For example, the US Bureau \\nof Labor Statistics publishes various statistics about the US labor market. The National \\nAeronautics and Space Administration (NASA) publishes various data from its \\nresearch initiatives.  Facebook shares data with businesses that advertise on its \\nplatform. \\nWhy would companies want to make their data available? Data is sticky, and a flywheel \\nis created by allowing users to integrate and extend their application into a user‚Äôs \\napplication. Greater user adoption and usage means more data, which means users can \\nintegrate more data into their applications and data systems. The side effect is there are \\nnow almost infinite sources of third-party data. \\nDirect third-party data access is commonly done via APIs, through data sharing on a \\ncloud platform, or through data download. APIs often provide deep integration \\ncapabilities, allowing customers to pull and push data. For example, many CRMs offer \\nAPIs that  their users can integrate into their systems and applications. We see a \\ncommon workflow to get data from a CRM, blend the CRM data through the customer \\n \\n67 Martin Fowler, ‚ÄúHow to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh,‚Äù \\nMartinFowler.com, May 20, 2019, https://oreil.ly/TEdJF.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 200, 'page_label': '201'}, page_content='Source System Practical Details  | \\n 193 \\nscoring model, and then use reverse ETL to send that data back into CRM for \\nsalespeople to contact better-qualified leads. \\nMessage Queues and Event-Streaming Platforms \\nEvent-driven architectures are pervasive in software applications and are poised to \\ngrow their popularity even further. First, message queues and event -streaming \\nplatforms‚Äîcritical layers in event -driven architectures ‚Äîare easier to set up and \\nmanage in a c loud environment. Second, the rise of data apps ‚Äîapplications that \\ndirectly integrate real-time analytics‚Äîare growing from strength to strength. Event -\\ndriven architectures are ideal in this setting because events can both trigger work in the \\napplication and feed near real-time analytics. \\nPlease note that streaming data (in this case, messages and streams) cuts across many \\ndata engineering lifecycle stages. Unlike an RDBMS, which is often directly attached \\nto an application, the lines of streaming data are sometimes less clear -cut. These \\nsystems are used as source systems, but they will often cut across the data engineering \\nlifecycle because of their transient nature. For example, you can use an event -\\nstreaming platform for message passing in an event -driven application, a source \\nsystem. The  same event -streaming platform can be used in the ingestion and \\ntransformation stage to process data for real-time analytics. \\nAs source systems, message queues and event -streaming platforms are used in \\nnumerous ways, from routing messages between microservices ingesting millions of \\nevents per second of event data from web, mobile, and IoT applications. Let‚Äôs look at \\nmessage queues and event-streaming platforms a bit more closely. \\nMessage queues \\nA message queue  is a mechanism to asynchronously send data (usually as small \\nindividual messages, in the kilobytes) between discrete systems using a publish and \\nsubscribe model. Data is published to a message queue and is delivered to one or more \\nsubscribers ( Figure 5 -9). The subscriber acknowledges receipt of the message, \\nremoving it from the queue. \\n \\nFigure 5-9. A simple message queue \\nMessage queues allow applications and systems to be decoupled from each other and \\nare widely used in microservices architectures. The message queue buffers messages'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 201, 'page_label': '202'}, page_content='194  | Chapter 5: Data Generation in Source Systems \\nto handle transient load spikes and makes messages durable through a distributed \\narchitecture with replication. \\nMessage queues are a critical ingredient for decoupled microservices and eventdriven \\narchitectures. Some things to keep in mind with message queues are frequency of \\ndelivery, message ordering, and scalability. \\nMessage ordering and delivery.    The order in which messages are created, sent, \\nand received can significantly impact downstream subscribers. In general, order in \\ndistributed message queues is a tricky problem. Message queues often apply a fuzzy \\nnotion of order and first in, first out (FI FO). Strict FIFO means that if message A is \\ningested before message B, message A will always be delivered before message B. In \\npractice, messages might be published and received out of order, especially in highly \\ndistributed message systems. \\nFor example, Amazon SQS standard queues make the best effort to preserve message \\norder. SQS also offers FIFO queues, which offer much stronger guarantees at the cost \\nof extra overhead. \\nIn general, don‚Äôt assume that your messages will be delivered in order unless your \\nmessage queue technology guarantees it. You typically need to design for out-oforder \\nmessage delivery. \\nDelivery frequency.    Messages can be sent exactly once or at least once. If a message \\nis sent exactly once, then after the subscriber acknowledges the message, the message \\ndisappears and won‚Äôt be delivered again. 68  Messages sent at least once  can be \\nconsumed by multiple subscribers or by the same subscriber more than once. This is \\ngreat when duplications or redundancy don‚Äôt matter. \\nIdeally, systems should be idempotent. In an idempotent system, the outcome of \\nprocessing a message once is identical to the outcome of processing it multiple times. \\nThis helps to account for a variety of subtle scenarios. For example, even if our system \\ncan guarantee exactly-once delivery, a consumer might fully process a message but fail \\nright before acknowledging processing. The message will effectively be processed \\ntwice, but an idempotent system handles this scenario gracefully. \\nScalability.    The most popular message queues utilized in event-driven applications \\nare horizontally scalable, running across multiple servers. This allows these queues to \\nscale up and down dynamically, buffer messages when systems fall behind, and \\n \\n68 Whether exactly once is possible is a semantical debate. Technically, exactly once delivery is impossible to \\nguarantee, as illustrated by the Two Generals Problem.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 202, 'page_label': '203'}, page_content='Source System Practical Details  | \\n 195 \\ndurably store messages for resilience against failure. However, this can create a variety \\nof complications, as mentioned previously (multiple deliveries and fuzzy ordering). \\nEvent-streaming platforms \\nIn some ways, an event-streaming platform is a continuation of a message queue in \\nthat messages are passed from producers to consumers. As discussed previously in this \\nchapter, the big difference between messages and streams is that a message queue is \\nprimarily used to route messages with certain  delivery guarantees. In contrast, an \\nevent-streaming platform is used to ingest and process data in an ordered log of \\nrecords. In an event-streaming platform, data is retained for a while, and it is possible \\nto replay messages from a past point in time. \\nLet‚Äôs describe an event related to an event -streaming platform. As mentioned in \\nChapter 3, an event is ‚Äúsomething that happened, typically a change in the state of \\nsomething.‚Äù An event has the following features: a key, a value, and a timestamp. \\nMultiple key-value timestamps might be contained in a single event. For example, an \\nevent for an ecommerce order might look like this: \\n{ \\n  \"Key\":\"Order # 12345\", \\n  \"Value\":\"SKU 123, purchase price of $100\", \\n  \"Timestamp\":\"2023-01-02 06:01:00\" \\n} \\nLet‚Äôs look at some of the critical characteristics of an event -streaming platform that \\nyou should be aware of as a data engineer. \\nTopics.    In an event -streaming platform, a producer streams events to a topic, a \\ncollection of related events. A topic might contain fraud alerts, customer orders, or \\ntemperature readings from IoT devices, for example. A topic can have zero, one, or \\nmultiple producers and customers on most event-streaming platforms. \\nUsing the preceding event example, a topic might be web orders. Also, let‚Äôs send \\nthis topic to a couple of consumers, such as fulfillment and marketing. This is \\nan excellent example of blurred lines between analytics and an event -driven system. \\nThe fulfillment subscriber will use events to trigger a fulfillment process, while \\nmarketing runs real-time analytics or trains and runs ML models to tune marketing \\ncampaigns (Figure 5-10).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 203, 'page_label': '204'}, page_content='196  | Chapter 5: Data Generation in Source Systems \\nFigure 5-10. An order-processing system generates events (small squares) and \\npublishes them to the web orders topic. Two subscribers‚Äîmarketing and \\nfulfillment‚Äîpull events from the topic. \\nStream partitions.    Stream partitions are subdivisions of a stream into multiple \\nstreams. A good analogy is a multilane freeway. Having multiple lanes allows for \\nparallelism and higher throughput. Messages are distributed across partitions by \\npartition key. Messages with the same partition key will always end up in the same \\npartition.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 204, 'page_label': '205'}, page_content='|  197 \\nIn Figure 5-11, for example, each message has a numeric ID‚Äî shown inside the circle \\nrepresenting the message‚Äîthat we use as a partition key. To determine the partition, \\nwe divide by 3 and take the remainder. Going from bottom to top, the partitions have \\nremainder 0, 1, and 2, respectively. \\n \\nFigure 5-11. An incoming message stream broken into three partitions \\nSet a partition key so that messages that should be processed together have the same \\npartition key. For example, it is common in IoT settings to want to send all messages \\nfrom a particular device to the same processing server. We can achieve this by using a \\ndevice ID as the partition key, and then setting up one server to consume from each \\npartition. \\nA key concern with stream partitioning is ensuring that your partition key does not \\ngenerate hotspotting‚Äîa disproportionate number of messages delivered to one \\npartition. For example, if each IoT device were known to be located in a particular US \\nstate, we might use the state as the partition key. Given a device distribution \\nproportional to state population,  the partitions containing California, Texas, Florida, \\nand New York might be overwhelmed, with other partitions relatively underutilized. \\nEnsure that your partition key will distribute messages evenly across partitions. \\nFault tolerance and resilience.    Event-streaming platforms are typically distributed \\nsystems, with streams stored on various nodes. If a node goes down, another node \\nreplaces it, and the stream is still accessible. This means records aren‚Äôt lost; you may \\nchoose to delete records, but that ‚Äôs another story. This fault tolerance and resilience \\nmake streaming platforms a good choice when you need a system that can reliably \\nproduce, store, and ingest event data. \\nWhom You‚Äôll Work With \\nWhen accessing source systems, it‚Äôs essential to understand the people with whom \\nyou‚Äôll work. In our experience, good diplomacy and relationships with the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 205, 'page_label': '206'}, page_content='198  | Chapter 5: Data Generation in Source Systems \\nWhom You‚Äôll Work With  \\nstakeholders of source systems are an underrated and crucial part of successful data \\nengineering. \\nWho are these stakeholders? Typically, you‚Äôll deal with two categories of stakeholders: \\nsystems and data stakeholders ( Figure 5 -12). A systems stakeholder  builds and \\nmaintains the source systems; these might be software engineers, application \\ndevelopers, and third parties. Data stakeholders own and control access to the data you \\nwant, generally handled by IT, a data governance group, or third parties. The systems \\nand data stakeholders are often different people or teams; sometimes, they are the \\nsame. \\n \\nFigure 5-12. The data engineer‚Äô s upstream stakeholders \\nYou‚Äôre often at the mercy of the stakeholder‚Äôs ability to follow correct software \\nengineering, database management, and development practices. Ideally, the \\nstakeholders are doing DevOps and working in an agile manner. We suggest creating \\na feedback loop be tween data engineers and stakeholders of the source systems to \\ncreate awareness of how data is consumed and used. This is among the single most \\noverlooked areas where data engineers can get a lot of value. When something happens \\nto the upstream source data‚Äîand something will happen, whether it‚Äôs a schema or data \\nchange, a failed server or database, or other important events‚Äîyou want to make sure \\nthat you‚Äôre made aware of the impact these issues will have on your data engineering \\nsystems. \\nIt might help to have a data contract in place with your upstream source system owners. \\nWhat is a data contract? James Denmore offers this definition:69 \\nA data contract is a written agreement between the owner of a source system and the \\nteam ingesting data from that system for use in a data pipeline. The contract should \\nstate what data is being extracted, via what method (full, incremental), how often, as \\nwell as who (person, team) are the contacts for both the source system and the \\ningestion. Data contracts should be stored in a well -known and easy-to-find location \\nsuch as a GitHub repo or internal documentation site. If possible, format data contracts \\n \\n69 James Denmore, Data Pipelines Pocket Reference (Sebastopol, CA: O‚ÄôReilly), https://oreil.ly/8QdkJ. Read \\nthe book for more information on how a data contract should be written.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 206, 'page_label': '207'}, page_content='|  199 \\nin a standardized form so they can be integrated into the development process or \\nqueried programmatically. \\nIn addition, consider establishing an SLA with upstream providers. An SLA provides \\nexpectations of what you can expect from the source systems you rely upon. An \\nexample of an SLA might be ‚Äúdata from source systems will be reliably available and \\nof high quality.‚Äù A service-level objective (SLO) measures performance against what \\nyou‚Äôve agreed to in the SLA. For example, given your example SLA, an SLO might \\nbe ‚Äúsource systems will have 99% uptime.‚Äù If a data contract or SLA/SLO seems too \\nformal, at least verbally set expectations for source system guarantees for uptime, data \\nquality, and anything else of importance to you. Upstream owners of source systems \\nneed to understand your requirements so they can provide you with the data you need. \\nUndercurrents and Their Impact on Source \\nSystems \\nUnlike other parts of the data engineering lifecycle, source systems are generally \\noutside the control of the data engineer. There‚Äôs an implicit assumption (some might \\ncall it hope) that the stakeholders and owners of the source systems‚Äîand the data they \\nproduce‚Äîare following best practices concerning data management, DataOps (and \\nDevOps), DODD (mentioned in Chapter 2 ) data architecture, orchestration, and \\nsoftware engineering. The data engineer should get as much upstream support as \\npossible to ensure that the undercurrents are applied when data is generated in source \\nsystems. Doing so will make the rest of the steps in the data engineering lifecycle \\nproceed a lot more smoothly. \\nHow do the undercurrents impact source systems? Let‚Äôs have a look. \\nSecurity \\nSecurity is critical, and the last thing you want is to accidentally create a point of \\nvulnerability in a source system. Here are some areas to consider: \\n‚Ä¢ Is the source system architected so data is secure and encrypted, both with data at \\nrest and while data is transmitted? \\n‚Ä¢ Do you have to access the source system over the public internet, or are you using \\na virtual private network (VPN)? \\n‚Ä¢ Keep passwords, tokens, and credentials to the source system securely locked \\naway. For example, if you‚Äôre using Secure Shell (SSH) keys, use a key manager \\nto protect your keys; the same rule applies to passwords‚Äîuse a password manager \\nor a single sign-on (SSO) provider.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 207, 'page_label': '208'}, page_content='200  | Chapter 5: Data Generation in Source Systems \\n‚Ä¢ Do you trust the source system? Always be sure to trust but verify that the source \\nsystem is legitimate. You don‚Äôt want to be on the receiving end of data from a \\nmalicious actor. \\nUndercurrents and Their Impact on Source Systems  \\nData Management \\nData management of source systems is challenging for data engineers. In most cases, \\nyou will have only peripheral control‚Äîif any control at all‚Äîover source systems and \\nthe data they produce. To the extent possible, you should understand the way data is \\nmanaged in source systems since this will directly influence how you ingest, store, and \\ntransform the data. \\nHere are some areas to consider: \\nData governance \\nAre upstream data and systems governed in a reliable, easy-to-understand fashion? \\nWho manages the data? \\nData quality \\nHow do you ensure data quality and integrity in upstream systems? Work with \\nsource system teams to set expectations on data and communication. \\nSchema \\nExpect that upstream schemas will change. Where possible, collaborate with \\nsource system teams to be notified of looming schema changes. \\nMaster data management \\nIs the creation of upstream records controlled by a master data management \\npractice or system? \\nPrivacy and ethics \\nDo you have access to raw data, or will the data be obfuscated? What are the \\nimplications of the source data? How long is it retained? Does it shift locations \\nbased on retention policies? \\nRegulatory \\nBased upon regulations, are you supposed to access the data? \\nDataOps \\nOperational excellence‚ÄîDevOps, DataOps, MLOps, XOps‚Äîshould extend up and \\ndown the entire stack and support the data engineering and lifecycle. While this is \\nideal, it‚Äôs often not fully realized.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 208, 'page_label': '209'}, page_content='|  201 \\nBecause you‚Äôre working with stakeholders who control both the source systems and \\nthe data they produce, you need to ensure that you can observe and monitor the uptime \\nand usage of the source systems and respond when incidents occur. For example, when \\nthe application database you depend on for CDC exceeds its I/O capacity and needs to \\nbe rescaled, how will that affect your ability to receive data from this system? Will you \\nbe able to access the data, or will it be unavailable until the database is rescaled? How \\nwill this affect reports? In another example, if the software engineering team is \\ncontinuously deploying, a code change may cause unanticipated failures in the \\napplication itself. How will the failure impact your ability to access the databases \\npowering the application? Will the data be up-to-date? \\nSet up a clear communication chain between data engineering and the teams supporting \\nthe source systems. Ideally, these stakeholder teams have incorporated DevOps into \\ntheir workflow and culture. This will go a long way to accomplishing the goals of \\nDataOps (a sibling of DevOps), to address and reduce errors quickly. As we mentioned \\nearlier, data engineers need to weave themselves into the DevOps practices of \\nstakeholders, and vice versa. Successful DataOps works when all people are on board \\nand focus on making systems holistically work. \\nA few DataOps considerations are as follows: \\nAutomation \\nThere‚Äôs the automation impacting the source system, such as code updates and \\nnew features. Then there‚Äôs the DataOps automation that you‚Äôve set up for your \\ndata workflows. Does an issue in the source system‚Äôs automation impact your data \\nworkflow automation?  If so, consider decoupling these systems so they can \\nperform automation independently. \\nObservability \\nHow will you know when there‚Äôs an issue with a source system, such as an outage \\nor a data -quality issue? Set up monitoring for source system uptime (or use the \\nmonitoring created by the team that owns the source system). Set up checks to \\nensure that data f rom the source system conforms with expectations for \\ndownstream usage. For example, is the data of good quality? Is the schema \\nconformant? Are customer records consistent? Is data hashed as stipulated by the \\ninternal policy? \\nIncident response \\nWhat‚Äôs your plan if something bad happens? For example, how will your data \\npipeline behave if a source system goes offline? What‚Äôs your plan to backfill the \\n‚Äúlost‚Äù data once the source system is back online?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 209, 'page_label': '210'}, page_content='202  | Chapter 5: Data Generation in Source Systems \\nData Architecture \\nSimilar to data management, unless you‚Äôre involved in the design and maintenance of \\nthe source system architecture, you‚Äôll have little impact on the upstream source system \\narchitecture. You should also understand how the upstream architecture is designed \\nand its strengths and weaknesses. Talk often with the teams responsible for the source \\nsystems to understand the factors discussed in this section and ensure that their systems \\ncan meet your expectations. Knowing where the architecture performs well and where \\nit doesn‚Äôt will impact how you design your data pipeline. \\nUndercurrents and Their Impact on Source Systems  \\nHere are some things to consider regarding source system architectures: \\nReliability \\nAll systems suffer from entropy at some point, and outputs will drift from what‚Äôs \\nexpected. Bugs are introduced, and random glitches happen. Does the system \\nproduce predictable outputs? How often can we expect the system to fail? What‚Äôs \\nthe mean time to repair to get the system back to sufficient reliability? \\nDurability \\nEverything fails. A server might die, a cloud‚Äôs zone or region could go offline, or \\nother issues may arise. You need to account for how an inevitable failure or outage \\nwill affect your managed data systems. How does the source system handle data \\nloss from hardware failures or network outages? What‚Äôs the plan for handling \\noutages for an extended period and limiting the blast radius of an outage? \\nAvailability \\nWhat guarantees that the source system is up, running, and available when it‚Äôs \\nsupposed to be? \\nPeople \\nWho‚Äôs in charge of the source system‚Äôs design, and how will you know if breaking \\nchanges are made in the architecture? A data engineer needs to work with the \\nteams who maintain the source systems and ensure that these systems are \\narchitected reliably. Create an SLA with the source system team to set expectations \\nabout potential system failure. \\nOrchestration \\nWhen orchestrating within your data engineering workflow, you‚Äôll primarily be \\nconcerned with making sure your orchestration can access the source system, which \\nrequires the correct network access, authentication, and authorization.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 210, 'page_label': '211'}, page_content='|  203 \\nHere are some things to think about concerning orchestration for source systems: \\nCadence and frequency \\nIs the data available on a fixed schedule, or can you access new data whenever \\nyou want? \\nCommon frameworks \\nDo the software and data engineers use the same container manager, such as \\nKubernetes? Would it make sense to integrate application and data workloads into \\nthe same Kubernetes cluster? If you‚Äôre using an orchestration framework like \\nAirflow, does it make s ense to integrate it with the upstream application team? \\nThere‚Äôs no correct answer here, but you need to balance the benefits of integration \\nwith the risks of tight coupling. \\nSoftware Engineering \\nAs the data landscape shifts to tools that simplify and automate access to source \\nsystems, you‚Äôll likely need to write code. Here are a few considerations when writing \\ncode to access a source system: \\nNetworking \\nMake sure your code will be able to access the network where the source system \\nresides. Also, always think about secure networking. Are you accessing an HTTPS \\nURL over the public internet, SSH, or a VPN? \\nAuthentication and authorization \\nDo you have the proper credentials (tokens, username/passwords) to access the \\nsource system? Where will you store these credentials so they don‚Äôt appear in your \\ncode or version control? Do you have the correct IAM roles to perform the coded \\ntasks? \\nAccess patterns \\nHow are you accessing the data? Are you using an API, and how are you handling \\nREST/GraphQL requests, response data volumes, and pagination? If you‚Äôre \\naccessing data via a database driver, is the driver compatible with the database \\nyou‚Äôre accessing? For ei ther access pattern, how are things like retries and \\ntimeouts handled? \\nOrchestration \\nDoes your code integrate with an orchestration framework, and can it be executed \\nas an orchestrated workflow? \\nParallelization \\nHow are you managing and scaling parallel access to source systems? \\nDeployment \\nHow are you handling the deployment of source code changes?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 211, 'page_label': '212'}, page_content='204  | Chapter 5: Data Generation in Source Systems \\nConclusion \\nSource systems and their data are vital in the data engineering lifecycle. Data engineers \\ntend to treat source systems as ‚Äúsomeone else‚Äôs problem‚Äù‚Äîdo this at your peril! Data \\nengineers who abuse source systems may need to look for another job when production \\ngoes down. \\nIf there‚Äôs a stick, there‚Äôs also a carrot. Better collaboration with source system teams \\ncan lead to higher -quality data, more successful outcomes, and better data products. \\nCreate a bidirectional flow of communications with your counterparts on these teams; \\nset up processes to notify of schema and application changes that affect \\nConclusion  \\nanalytics and ML. Communicate your data needs proactively to assist application \\nteams in the data engineering process. \\nBe aware that the integration between data engineers and source system teams is \\ngrowing. One example is reverse ETL, which has long lived in the shadows but has \\nrecently risen into prominence. We also discussed that the event -streaming platform \\ncould serve a role in event-driven architectures and analytics; a source system can also \\nbe a data engineering system. Build shared systems where it makes sense to do so. \\nLook for opportunities to build user -facing data products. Talk to application teams \\nabout analytics they would like to present to their users or places where ML could \\nimprove the user experience. Make application teams stakeholders in data engineering, \\nand find ways to share your successes. \\nNow that you understand the types of source systems and the data they generate, we‚Äôll \\nnext look at ways to store this data. \\nAdditional Resources \\n‚Ä¢ Confluent‚Äôs ‚ÄúSchema Evolution and Compatibility‚Äù documentation \\n‚Ä¢ Database Internals by Alex Petrov (O‚ÄôReilly) \\n‚Ä¢ Database System Concepts by Abraham (Avi) Silberschatz et al. (McGraw Hill) \\n‚Ä¢ ‚ÄúThe Log: What Every Software Engineer Should Know About Real-Time Data‚Äôs \\nUnifying Abstraction‚Äù by Jay Kreps \\n‚Ä¢ ‚ÄúModernizing Business Data Indexing‚Äù  by Benjamin Douglas and Mohammad \\nMohtasham \\n‚Ä¢ ‚ÄúNoSQL: What‚Äôs in a Name‚Äù by Eric Evans'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 212, 'page_label': '213'}, page_content='|  205 \\n‚Ä¢ ‚ÄúTest Data Quality at Scale with Deequ‚Äù by Dustin Lange et al. \\n‚Ä¢ ‚ÄúThe What, Why, and When of Single -Table Design with DynamoDB‚Äù by Alex \\nDeBrie'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 213, 'page_label': '214'}, page_content='206 \\nCHAPTE\\nR 6 \\nStora\\nge \\nStorage is the cornerstone of the data engineering lifecycle (Figure 6-1) and underlies \\nits major stages‚Äîingestion, transformation, and serving. Data gets stored many times \\nas it moves through the lifecycle. To paraphrase an old saying, it‚Äôs storage all the way \\ndown. Whether data is needed seconds, minutes, days, months, or ye ars later, it must \\npersist in storage until systems are ready to consume it for further processing and \\ntransmission. Knowing the use case of the data and the way you will retrieve it in the \\nfuture is the first step to choosing the proper storage solutions for your data \\narchitecture.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 214, 'page_label': '215'}, page_content='Raw Ingredients of Data Storage  | \\n 207 \\n \\nFigure 6-1. Storage plays a central role in the data engineering lifecycle \\nWe also discussed storage in Chapter 5, but with a difference in focus and domain of \\ncontrol. Source systems are generally not maintained or controlled by data engineers. \\nThe storage that data engineers handle directly, which we‚Äôll focus on in this chapter, \\nencompasses the data engineering lif ecycle stages of ingesting data from source \\nsystems to serving data to deliver value with analytics, data science, etc. Many forms \\nof storage undercut the entire data engineering lifecycle in some fashion. \\nTo understand storage, we‚Äôre going to start by studying the raw ingredients  that \\ncompose storage systems, including hard drives, solid state drives, and system memory \\n(see Figure 6 -2). It‚Äôs essential to understand the basic characteristics of physical \\nstorage technologies to assess the trade-offs inherent in any storage architecture. This \\nsection also discusses serialization and compression, key software elements of \\npractical storage.  (We defer a deeper technical discussion of serialization and \\ncompression to Appendix A.) We also discuss caching, which is critical in assembling \\nstorage systems.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 215, 'page_label': '216'}, page_content='208  | Chapter 6: Storage \\n \\nFigure 6-2. Raw ingredients, storage systems, and storage abstractions \\nNext, we‚Äôll look at storage systems . In practice, we don‚Äôt directly access system \\nmemory or hard disks. These physical storage components exist inside servers and \\nclusters that can ingest and retrieve data using various access paradigms. \\nFinally, we‚Äôll look at storage abstractions. Storage systems are assembled into a cloud \\ndata warehouse, a data lake, etc. When building data pipelines, engineers choose the \\nappropriate abstractions for storing their data as it moves through the ingestion, \\ntransformation, and serving stages. \\nRaw Ingredients of Data Storage \\nStorage is so common that it‚Äôs easy to take it for granted. We‚Äôre often surprised by the \\nnumber of software and data engineers who use storage every day but have little idea \\nhow it works behind the scenes or the trade-offs inherent in various storage media. As \\na result, we see storage used in some pretty...interesting ways. Though current managed \\nservices potentially free data engineers from the complexities of managing servers,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 216, 'page_label': '217'}, page_content='Raw Ingredients of Data Storage  | \\n 209 \\ndata engineers still need to be aware of underlying components‚Äô essential \\ncharacteristics, performance considerations, durability, and costs. \\nIn most data architectures, data frequently passes through magnetic storage, SSDs, and \\nmemory as it works its way through the various processing phases of a data pipeline. \\nData storage and query systems generally follow complex recipes involving distributed \\nsystems, numerous services, and multiple hardware storage layers. These systems \\nrequire the right raw ingredients to function correctly. \\nLet‚Äôs look at some of the raw ingredients of data storage: disk drives, memory, \\nnetworking and CPU, serialization, compression, and caching. \\nMagnetic Disk Drive \\nMagnetic disks utilize spinning platters coated with a ferromagnetic film (Figure 6-3). \\nThis film is magnetized by a read/write head during write operations to physically \\nencode binary data. The read/write head detects the magnetic field and outputs a \\nbitstream during read operations. Magnetic disk drives have been around for ages. \\nThey st ill form the backbone of bulk data storage systems because they are \\nsignificantly cheaper than SSDs per gigabyte of stored data. \\nOn the one hand, these disks have seen extraordinary improvements in performance, \\nstorage density, and cost.70 On the other hand, SSDs dramatically outperform magnetic \\ndisks on various metrics. Currently, commercial magnetic disk drives cost roughly 3 \\ncents per gigabyte of capacity. (Note that we‚Äôll frequently use the abbreviations HDD \\nand SSD to denote rotating magnetic disk and solid-state drives, respectively.) \\n \\n70 Andy Klein, ‚ÄúHard Disk Drive (HDD) vs. Solid-State Drive (SSD): What‚Äôs the Diff?,‚Äù Backblaze blog, \\nOctober 5, 2021, https://oreil.ly/XBps8.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 217, 'page_label': '218'}, page_content='210  | Chapter 6: Storage \\n \\nFigure 6-3. Magnetic disk head movement and rotation are essential in random \\naccess latency \\nIBM developed magnetic disk drive technology in the 1950s. Since then, magnetic disk \\ncapacities have grown steadily. The first commercial magnetic disk drive, the IBM \\n350, had a capacity of 3.75 megabytes. As of this writing, magnetic drives storing 20 \\nTB are commercially available. In fact, magnetic disks continue to see rapid \\ninnovation, with methods such as heat-assisted magnetic recording (HAMR), shingled \\nmagnetic recording (SMR), and helium -filled disk enclosures being used to realize \\never greater stor age densities. In spite of the continuing improvements in drive \\ncapacity, other aspects of HDD performance are hampered by physics. \\nFirst, disk transfer speed, the rate at which data can be read and written, does not scale \\nin proportion with disk capacity. Disk capacity scales with areal density (gigabits \\nstored per square inch), whereas transfer speed scales with linear density (bits per \\ninch). This means that if disk capacity grows by a factor of 4, transfer speed increases \\nby only a factor of 2. Consequently, current data center drives support maximum data \\ntransfer speeds of 200 ‚Äì300 MB/s. To frame this another way, it takes mo re than 20 \\nhours to read the entire contents of a 30 TB magnetic drive, assuming a transfer speed \\nof 300 MB/s. \\nA second major limitation is seek time. To access data, the drive must physically \\nrelocate the read/write heads to the appropriate track on the disk. Third, in order to find \\na particular piece of data on the disk, the disk controller must wait for that data to rotate \\nunder the read/write heads. This leads to rotational latency. Typical commercial drives \\nspinning at 7,200 revolutions per minute (RPM) seek time, and rotational latency, leads \\nto over four milliseconds of overall average latency (time to access a selected piece of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 218, 'page_label': '219'}, page_content='Raw Ingredients of Data Storage  | \\n 211 \\ndata). A fourth limitation is input/output operations per second (IOPS), critical for \\ntransactional databases. A magnetic drive ranges from 50 to 500 IOPS. \\nVarious tricks can improve latency and transfer speed. Using a higher rotational speed \\ncan increase transfer rate and decrease rotational latency. Limiting the radius of the \\ndisk platter or writing data into only a narrow band on the disk reduces seek time . \\nHowever, none of these techniques makes magnetic drives remotely competitive with \\nSSDs for random access lookups. SSDs can deliver data with significantly lower \\nlatency, higher IOPS, and higher transfer speeds, partially because there is no \\nphysically rotating disk or magnetic head to wait for. \\nAs mentioned earlier, magnetic disks are still prized in data centers for their low \\ndatastorage costs. In addition, magnetic drives can sustain extraordinarily high transfer \\nrates through parallelism. This is the critical idea behind cloud object storage: data can \\nbe distributed across thousands of disks in clusters. Data -transfer rates go up \\ndramatically by reading from numerous disks simultaneously, limited primarily by \\nnetwork performance rather than disk transfer rate. Thus, network components and \\nCPUs are also key raw ingredients in storage systems, and we will return to these topics \\nshortly. \\nSolid-State Drive \\nSolid-state drives (SSDs) store data as charges in flash memory cells. SSDs eliminate \\nthe mechanical components of magnetic drives; the data is read by purely electronic \\nmeans. SSDs can look up random data in less than 0.1 ms (100 microseconds). In \\naddition, SSDs can scale both data-transfer speeds and IOPS by slicing storage into \\npartitions with numerous storage controllers running in parallel. Commercial SSDs can \\nsupport transfer speeds of many gigabytes per second and tens of thousands of IOPS. \\nBecause of these exceptional performance characteristics, SSDs have revolutionized \\ntransactional databases and are the accepted standard for commercial deployments of \\nOLTP systems. SSDs allow relational databases such as PostgreSQL, MySQL, and \\nSQL Server to handle thousands of transactions per second. \\nHowever, SSDs are not currently the default option for high -scale analytics data \\nstorage. Again, this comes down to cost. Commercial SSDs typically cost 20‚Äì30 cents \\n(USD) per gigabyte of capacity, nearly 10 times the cost per capacity of a magnetic \\ndrive. Thus, object storage on magnetic disks has emerged as the leading option for \\nlarge-scale data storage in data lakes and cloud data warehouses. \\nSSDs still play a significant role in OLAP systems. Some OLAP databases leverage \\nSSD caching to support high-performance queries on frequently accessed data. As low-\\nlatency OLAP becomes more popular, we expect SSD usage in these systems to follow \\nsuit.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 219, 'page_label': '220'}, page_content='212  | Chapter 6: Storage \\nRandom Access Memory \\nWe commonly use the terms random access memory  (RAM) and memory \\ninterchangeably. Strictly speaking, magnetic drives and SSDs also serve as memory \\nthat stores data for later random access retrieval, but RAM has several specific \\ncharacteristics: \\n‚Ä¢ It is attached to a CPU and mapped into CPU address space. \\n‚Ä¢ It stores the code that CPUs execute and the data that this code directly processes. \\n‚Ä¢ It is volatile, while magnetic drives and SSDs are nonvolatile. Though they may \\noccasionally fail and corrupt or lose data, drives generally retain data when \\npowered off. RAM loses data in less than a second when it is unpowered. \\n‚Ä¢ It offers significantly higher transfer speeds and faster retrieval times than SSD \\nstorage. DDR5 memory‚Äîthe latest widely used standard for RAM ‚Äîoffers data \\nretrieval latency on the order of 100 ns, roughly 1,000 times faster than SSD. A \\ntypical CPU can support 100 GB/s bandwidth to attached memory and millions of \\nIOPS. (Statistics vary dramatically depending on the number of memory channels \\nand other configuration details.) \\n‚Ä¢ It is significantly more expensive than SSD storage, at roughly $10/GB (at the \\ntime of this writing). \\n‚Ä¢ It is limited in the amount of RAM attached to an individual CPU and memory \\ncontroller. This adds further to complexity and cost. High -memory servers \\ntypically utilize many interconnected CPUs on one board, each with a block of \\nattached RAM. \\n‚Ä¢ It is still significantly slower than CPU cache, a type of memory located directly \\non the CPU die or in the same package. Cache stores frequently and recently \\naccessed data for ultrafast retrieval during processing. CPU designs incorporate \\nseveral layers of cache of varying size and performance characteristics. \\nWhen we talk about system memory, we almost always mean dynamic RAM, a high-\\ndensity, low-cost form of memory. Dynamic RAM stores data as charges in capacitors. \\nThese capacitors leak over time, so the data must be frequently refreshed (read and \\nrewritten) to prevent data loss. The hardware memory controller handles these \\ntechnical details; data engineers simply need to worry about bandwidth and retrieval \\nlatency characteristics. Other forms of memory, such as static RAM , are used in \\nspecialized applications such as CPU caches. \\nCurrent CPUs virtually always employ the von Neumann architecture, with code and \\ndata stored together in the same memory space. However, CPUs typically also support'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 220, 'page_label': '221'}, page_content='Raw Ingredients of Data Storage  | \\n 213 \\nthe option to disable code execution in specific pages of memory for enhanced security. \\nThis feature is reminiscent of the Harvard architecture, which separates code and data. \\nRAM is used in various storage and processing systems and can be used for caching, \\ndata processing, or indexes. Several databases treat RAM as a primary storage layer, \\nallowing ultra-fast read and write performance. In these applications, data engineers \\nmust always keep in mind the volatility of RAM. Even if data stored in memory is \\nreplicated across a cluster, a power outage that brings down several nodes could cause \\ndata loss. Architectures intended to durably store data may use battery backups and \\nautomatically dump all data to disk in the event of power loss. \\nNetworking and CPU \\nWhy are we mentioning networking and CPU as raw ingredients for storing data? \\nIncreasingly, storage systems are distributed to enhance performance, durability, and \\navailability. We mentioned specifically that individual magnetic disks offer relatively \\nlow-transfer performance, but a cluster of disks parallelizes reads for significant \\nperformance scaling. While storage standards such as redundant arrays of independent \\ndisks (RAID) parallelize on a single server, cloud object storage clusters operate at a \\nmuch larger scale, with disks distributed across a network and even multiple data \\ncenters and availability zones. \\nAvailability zones are a standard cloud construct consisting of compute environments \\nwith independent power, water, and other resources. Multizonal storage enhances both \\nthe availability and durability of data. \\nCPUs handle the details of servicing requests, aggregating reads, and distributing \\nwrites. Storage becomes a web application with an API, backend service components, \\nand load balancing. Network device performance and network topology are key factors \\nin realizing high performance. \\nData engineers need to understand how networking will affect the systems they build \\nand use. Engineers constantly balance the durability and availability achieved by \\nspreading out data geographically versus the performance and cost benefits of keeping \\nstorage in a small geographic area and close to data consumers or writers. Appendix B \\ncovers cloud networking and major relevant ideas. \\nSerialization \\nSerialization is another raw storage ingredient and a critical element of database \\ndesign. The decisions around serialization will inform how well queries perform across \\na network, CPU overhead, query latency, and more. Designing a data lake, for example,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 221, 'page_label': '222'}, page_content='214  | Chapter 6: Storage \\ninvolves choosing a base storage system (e.g., Amazon S3) and standards for \\nserialization that balance interoperability with performance considerations. \\nWhat is serialization, exactly? Data stored in system memory by software is generally \\nnot in a format suitable for storage on disk or transmission over a network. \\nSerialization is the process of flattening and packing data into a standard format that a \\nreader will be able to decode. Serialization formats provide a standard of data \\nexchange. We might encode data in a row -based manner as an XML, JSON, or CSV \\nfile and pass it to another user who can then decode it using a standard library. A \\nserialization algorithm has logic for handling types, imposes rules on data structure, \\nand allows exchange between programming languages and CPUs. The serialization \\nalgorithm also has rules for handling exceptions. For instance, Python objects can \\ncontain cyclic references;  the serialization algorithm might throw an error or limit \\nnesting depth on encountering a cycle. \\nLow-level database storage is also a form of serialization. Row -oriented relational \\ndatabases organize data as rows on disk to support speedy lookups and in -place \\nupdates. Columnar databases organize data into column files to optimize for highly \\nefficient compression and support fast scans of large data volumes. Each serialization \\nchoice comes with a set of trade-offs, and data engineers tune these choices to optimize \\nperformance to requirements. \\nWe provide a more detailed catalog of common data serialization techniques and \\nformats in Appendix A. We suggest that data engineers become familiar with common \\nserialization practices and formats, especially the most popular current formats (e.g., \\nApache Parquet), hybrid serialization (e.g., Apache Hudi), and in-memory serialization \\n(e.g., Apache Arrow). \\nCompression \\nCompression is another critical component of storage engineering. On a basic level, \\ncompression makes data smaller, but compression algorithms interact with other \\ndetails of storage systems in complex ways. \\nHighly efficient compression has three main advantages in storage systems. First, the \\ndata is smaller and thus takes up less space on the disk. Second, compression increases \\nthe practical scan speed per disk. With a 10:1 compression ratio, we go from scanning \\n200 MB/s per magnetic disk to an effective rate of 2 GB/s per disk. \\nThe third advantage is in network performance. Given that a network connection \\nbetween an Amazon EC2 instance and S3 provides 10 gigabits per second (Gbps) of \\nbandwidth, a 10:1 compression ratio increases effective network bandwidth to 100 \\nGbps.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 222, 'page_label': '223'}, page_content='Data Storage Systems  | \\n 215 \\nCompression also comes with disadvantages. Compressing and decompressing data \\nentails extra time and resource consumption to read or write data. We undertake a more \\ndetailed discussion of compression algorithms and trade-offs in Appendix A. \\nCaching \\nWe‚Äôve already mentioned caching in our discussion of RAM. The core idea of caching \\nis to store frequently or recently accessed data in a fast access layer. The faster the \\ncache, the higher the cost and the less storage space available. Less frequently accessed \\ndata is stored in cheaper, slower storage. Caches are critical for data serving, \\nprocessing, and transformation. \\nAs we analyze storage systems, it is helpful to put every type of storage we utilize \\ninside a cache hierarchy (Table 6-1). Most practical data systems rely on many cache \\nlayers assembled from storage with varying performance characteristics. This starts \\ninside CPUs; processors may deploy up to four cache tiers. We move down the \\nhierarchy to RAM and SSDs. Cloud object storag e is a lower tier that supports long -\\nterm data retention and durability while allowing for data serving and dynamic data \\nmovement in pipelines. \\nTable 6-1. A heuristic cache hierarchy displaying storage types with approximate pricing \\nand performance characteristics \\nStorage type Data fetch \\nlatencya \\nBandwidth Price \\nCPU cache 1 nanosecond 1 TB/s N/A \\nRAM 0.1 \\nmicroseconds \\n100 GB/s $10/GB \\nSSD 0.1 milliseconds 4 GB/s $0.20/GB \\nHDD 4 milliseconds 300 MB/s $0.03/GB \\nObject \\nstorage \\n100 \\nmilliseconds \\n10 GB/s $0.02/GB per \\nmonth \\nArchival \\nstorage \\n12 hours Same as object storage once data \\nis available \\n$0.004/GB per \\nmonth \\na A microsecond is 1,000 nanoseconds, and a millisecond is 1,000 microseconds. \\nWe can think of archival storage as a reverse cache. Archival storage provides inferior \\naccess characteristics for low costs. Archival storage is generally used for data backups \\nand to meet data-retention compliance requirements. In typical scenarios, this data will \\nbe accessed only in an emergency (e.g., data in a database might be lost and need to be \\nrecovered, or a company might need to look back at historical data for legal discovery).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 223, 'page_label': '224'}, page_content='216  | Chapter 6: Storage \\nData Storage Systems \\nThis section covers the major data storage systems you‚Äôll encounter as a data engineer. \\nStorage systems exist at a level of abstraction above raw ingredients. For example, \\nmagnetic disks are a raw storage ingredient, while major cloud object storage platforms \\nand HDFS are storage systems that utilize magnetic disks. Still higher levels of storage \\nabstraction exist, such as data lakes and lakehouses (which we cover in ‚ÄúData \\nEngineering Storage Abstractions‚Äù on page 215). \\nSingle Machine Versus Distributed Storage \\nAs data storage and access patterns become more complex and outgrow the usefulness \\nof a single server, distributing data to more than one server becomes necessary. Data \\ncan be stored on multiple servers, known as distributed storage. This is a distributed \\nsystem whose purpose is to store data in a distributed fashion (Figure 6-4). \\n \\nFigure 6-4. Single machine versus distributed storage on multiple servers \\nDistributed storage coordinates the activities of multiple servers to store, retrieve, and \\nprocess data faster and at a larger scale, all while providing redundancy in case a server \\nbecomes unavailable. Distributed storage is common in architectures where you want \\nbuilt-in redundancy and scalability for large amounts of data. For example, object \\nstorage, Apache Spark, and cloud data warehouses rely on distributed storage \\narchitectures. \\nData engineers must always be aware of the consistency paradigms of the distributed \\nsystems, which we‚Äôll explore next. \\nEventual Versus Strong Consistency \\nA challenge with distributed systems is that your data is spread across multiple servers. \\nHow does this system keep the data consistent? Unfortunately, distributed systems \\npose a dilemma for storage and query accuracy. It takes time to replicate changes across \\nthe nodes of a system; often a balance exists between getting current data and getting \\n‚Äúsorta‚Äù current data in a distributed database. Let‚Äôs look at two common consistency \\npatterns in distributed systems: eventual and strong. \\nWe‚Äôve covered ACID compliance throughout this book, starting in Chapter 5. Another \\nacronym is BASE, which stands for basically available, soft -state, eventual'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 224, 'page_label': '225'}, page_content='Data Storage Systems  | \\n 217 \\nconsistency. Think of it as the opposite of ACID. BASE is the basis of eventual \\nconsistency. Let‚Äôs briefly explore its components: \\nBasically available \\nConsistency is not guaranteed, but database reads and writes are made on a best -\\neffort basis, meaning consistent data is available most of the time. \\nSoft-state \\nThe state of the transaction is fuzzy, and it‚Äôs uncertain whether the transaction is \\ncommitted or uncommitted. \\nEventual consistency \\nAt some point, reading data will return consistent values. \\nIf reading data in an eventually consistent system is unreliable, why use it? Eventual \\nconsistency is a common trade -off in large-scale, distributed systems. If you want to \\nscale horizontally (across multiple nodes) to process data in high volumes, then \\neventually, consistency is often the price you‚Äôll pay. Eventual consistency allows you \\nto retrieve data quickly without verifying that you have the latest version across all \\nnodes. \\nThe opposite of eventual consistency is strong consistency. With strong consistency, \\nthe distributed database ensures that writes to any node are first distributed with a \\nconsensus and that any reads against the database return consistent values. You‚Äôll use \\nstrong consistency when you can tolerate higher query latency and require correct data \\nevery time you read from the database. \\nGenerally, data engineers make decisions about consistency in three places. First, the \\ndatabase technology itself sets the stage for a certain level of consistency. Second, \\nconfiguration parameters for the database will have an impact on consistency. Third , \\ndatabases often support some consistency configuration at an individual query level. \\nFor example, DynamoDB supports eventually consistent reads and strongly consistent \\nreads. Strongly consistent reads are slower and consume more resources, so it is best \\nto use them sparingly, but they are available when consistency is required. You should \\nunderstand how your da tabase handles consistency. Again, data engineers are tasked \\nwith understanding technology deeply and using it to solve problems appropriately. A \\ndata engineer might need to negotiate consistency requirements with other technical \\nand business stakeholders.  Note that this is both a technology and organizational \\nproblem; ensure that you have gathered requirements from your stakeholders and \\nchoose your technologies appropriately. \\nFile Storage'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 225, 'page_label': '226'}, page_content='218  | Chapter 6: Storage \\nWe deal with files every day, but the notion of a file is somewhat subtle. A file is a data \\nentity with specific read, write, and reference characteristics used by software and \\noperating systems. We define a file to have the following characteristics: \\nFinite length \\nA file is a finite-length stream of bytes. \\nAppend operations \\nWe can append bytes to the file up to the limits of the host storage system. \\nRandom access \\nWe can read from any location in the file or write updates to any location. Object \\nstorage behaves much like file storage but with key differences. While we set the stage \\nfor object storage by discussing file storage first, object storage is arguably much more \\nimportant for the type of data engineering you‚Äôll do today. We will forward-reference \\nthe object storage discussion extensively over the next few pages. \\nFile storage systems organize files into a directory tree. The directory reference for a \\nfile might look like this: \\n/Users/matthewhousley/output.txt \\nWhen this file reference is passed to the operating system, it starts at the root directory \\n/, finds Users, matthewhousley, and finally output.txt. Working from the left, \\neach directory is contained inside a parent directory, until we finally reach the file \\noutput.txt. This example uses Unix semantics, but Windows file reference \\nsemantics are similar. The filesystem stores each directory as metadata about the files \\nand directories that it contains. This metadata consists of the name of each entity, \\nrelevant permission details, and a pointer to the actual entity. To find a file on disk, the \\noperating system looks at the metadata at each hierarchy level and follows the pointer \\nto the next subdirectory entity until finally reaching the file itself. \\nNote that other file -like data entities generally don‚Äôt necessarily have all these \\nproperties. For example, objects in object storage support only the first characteristic, \\nfinite length, but are still extremely useful. We discuss this in ‚ÄúObject Storage‚Äù on page \\n205. \\nIn cases where file storage paradigms are necessary for a pipeline, be careful with state \\nand try to use ephemeral environments as much as possible. Even if you must process \\nfiles on a server with an attached disk, use object storage for intermediate stora ge \\nbetween processing steps. Try to reserve manual, low -level file processing for one -\\ntime ingestion steps or the exploratory stages of pipeline development.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 226, 'page_label': '227'}, page_content='Data Storage Systems  | \\n 219 \\nLocal disk storage \\nThe most familiar type of file storage is an operating system‚Äìmanaged filesystem on a \\nlocal disk partition of SSD or magnetic disk. New Technology File System (NTFS) \\nand ext4 are popular filesystems on Windows and Linux, respectively. The operating \\nsystem handles the details of storing directory entities, files, and metadata. Filesystems \\nare designed to write data to allow for easy recovery in the event of power loss during \\na write, though any unwritten data will still be lost. \\nLocal filesystems generally support full read after write consistency; reading \\nimmediately after a write will return the written data. Operating systems also employ \\nvarious locking strategies to manage concurrent writing attempts to a file. \\nLocal disk filesystems may also support advanced features such as journaling, \\nsnapshots, redundancy, the extension of the filesystem across multiple disks, full disk \\nencryption, and compression. In ‚ÄúBlock Storage‚Äù on page 202, we also discuss RAID. \\nNetwork-attached storage \\nNetwork-attached storage (NAS) systems provide a file storage system to clients over \\na network. NAS is a prevalent solution for servers; they quite often ship with built -in \\ndedicated NAS interface hardware. While there are performance penalties to accessing \\nthe filesystem over a network, significant advantages to storage virtualization also \\nexist, including redundancy and reliability, fine-grained control of resources, storage \\npooling across multiple disks for large virtual volumes, and file sharing across multiple \\nmachines. Engineers should be aware of the consistency model provided by their NAS \\nsolution, especially when multiple clients will potentially access the same data. \\nA popular alternative to NAS is a storage area network (SAN), but SAN systems \\nprovide block-level access without the filesystem abstraction. We cover SAN systems \\nin ‚ÄúBlock Storage‚Äù on page 202. \\nCloud filesystem services \\nCloud filesystem services provide a fully managed filesystem for use with multiple \\ncloud VMs and applications, potentially including clients outside the cloud \\nenvironment. Cloud filesystems should not be confused with standard storage attached \\nto VMs‚Äîgenerally, block storage with a filesystem managed by the VM operating \\nsystem. Cloud filesystems behave much like NAS solutions, but the details of \\nnetworking, managing disk clusters, failures, and configuration are fully handled by \\nthe cloud vendor. \\nFor example, Amazon Elastic File System (EFS) is an extremely popular example of a \\ncloud filesystem service. Storage is exposed through the NFS 4 protocol, which is also \\nused by NAS systems. EFS provides automatic scaling and pay -per-storage pricing'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 227, 'page_label': '228'}, page_content='220  | Chapter 6: Storage \\nwith no advanced storage reservation required. The service also provides local read-\\nafter-write consistency (when reading from the machine that performed the write). It \\nalso offers open-after-close consistency across the full filesystem. In other words, once \\nan application closes a file, subsequent readers will see changes saved to t he closed \\nfile. \\nBlock Storage \\nFundamentally, block storage  is the type of raw storage provided by SSDs and \\nmagnetic disks. In the cloud, virtualized block storage is the standard for VMs. These \\nblock storage abstractions allow fine control of storage size, scalability, and data \\ndurability beyond that offered by raw disks. \\nIn our earlier discussion of SSDs and magnetic disks, we mentioned that with these \\nrandom-access devices, the operating system can seek, read, and write any data on the \\ndisk. A block is the smallest addressable unit of data supported by a disk. This was \\noften 512 bytes of usable data on older disks, but it has now grown to 4,096 bytes for \\nmost current disks, making writes less fine -grained but dramatically reducing the \\noverhead of man aging blocks. Blocks typically contain extra bits for error \\ndetection/correction and other metadata. \\nBlocks on magnetic disks are geometrically arranged on a physical platter. Two blocks \\non the same track can be read without moving the head, while reading two blocks on \\nseparate tracks requires a seek. Seek time can occur between blocks on an SSD, but \\nthis is infinitesimal compared to the seek time for magnetic disk tracks. \\nBlock storage applications \\nTransactional database systems generally access disks at a block level to lay out data \\nfor optimal performance. For row-oriented databases, this originally meant that rows \\nof data were written as continuous streams; the situation has grown more complicated \\nwith the arrival of SSDs and their associated seek -time performance improvements, \\nbut transactional databases still rely on the high random access performance offered \\nby direct access to a block storage device. \\nBlock storage also remains the default option for operating system boot disks on cloud \\nVMs. The block device is formatted much as it would be directly on a physical disk, \\nbut the storage is usually virtualized. (See ‚ÄúCloud virtualized block storage‚Äù on page \\n203.) \\nRAID \\nRAID stands for redundant array of independent disks , as noted previously. RAID \\nsimultaneously controls multiple disks to improve data durability, enhance \\nperformance, and combine capacity from multiple drives. An array can appear to the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 228, 'page_label': '229'}, page_content='Data Storage Systems  | \\n 221 \\noperating system as a single block device. Many encoding and parity schemes are \\navailable, depending on the desired balance between enhanced effective bandwidth \\nand higher fault tolerance (tolerance for many disk failures). \\nStorage area network \\nStorage area network (SAN) systems provide virtualized block storage devices over a \\nnetwork, typically from a storage pool. SAN abstraction can allow fine-grained storage \\nscaling and enhance performance, availability, and durability. You might encounter \\nSAN systems if you‚Äôre working with on -premises storage systems; you might also \\nencounter a cloud version of SAN, as in the next subsection. \\nCloud virtualized block storage \\nCloud virtualized block storage solutions are similar to SAN but free engineers from \\ndealing with SAN clusters and networking details. We‚Äôll look at Amazon Elastic Block \\nStore (EBS) as a standard example; other public clouds have similar offerings. EBS is \\nthe default storage for Amazon EC2 virtual machines; other cloud providers also treat \\nvirtualized object storage as a key component of their VM offerings. \\nEBS offers several tiers of service with different performance characteristics. \\nGenerally, EBS performance metrics are given in IOPS and throughput (transfer \\nspeed). The higher performance tiers of EBS storage are backed by SSD disks, while \\nmagnetic disk-backed storage offers lower IOPS but costs less per gigabyte. \\nEBS volumes store data separate from the instance host server but in the same zone to \\nsupport high performance and low latency (Figure 6-5). This allows EBS volumes to \\npersist when an EC2 instance shuts down, when a host server fails, or even when the \\ninstance is deleted. EBS storage is suitable for applications such as databases, where \\ndata durability is a high priority. In addition, EBS re plicates all data to at least two \\nseparate host machines, protecting data if a disk fails. \\n \\nFigure 6-5. EBS volumes replicate data to multiple hosts and disks for high \\ndurability and availability, but are not resilient to the failure of an availability zone \\nEBS storage virtualization also supports several advanced features. For example, EBS \\nvolumes allow instantaneous point-in-time snapshots while the drive is used. Although \\nit still takes some time for the snapshot to be replicated to S3, EBS can effectively'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 229, 'page_label': '230'}, page_content='222  | Chapter 6: Storage \\nfreeze the state of data blocks when the snapshot is taken, while allowing the client \\nmachine to continue using the disk. In addition, snapshots after the initial full backup \\nare differential; only changed blocks are written to S3 to minimize storage costs  and \\nbackup time. \\nEBS volumes are also highly scalable. At the time of this writing, some EBS volume \\nclasses can scale up to 64 TiB, 256,000 IOPS, and 4,000 MiB/s. \\nLocal instance volumes \\nCloud providers also offer block storage volumes that are physically attached to the \\nhost server running a virtual machine. These storage volumes are generally very low \\ncost (included with the price of the VM in the case of Amazon‚Äôs EC2 instance store) \\nand provide low latency and high IOPS. \\nInstance store volumes (Figure 6-6) behave essentially like a disk physically attached \\nto a server in a data center. One key difference is that when a VM shuts down or is \\ndeleted, the contents of the locally attached disk are lost, whether or not this event was \\ncaused by intentional user action. This ensures that a new virtual machine cannot read \\ndisk contents belonging to a different customer. \\n \\nFigure 6-6. Instance store volumes offer high performance and low cost but do not \\nprotect data in the event of disk failure or VM shutdown \\nLocally attached disks support none of the advanced virtualization features offered by \\nvirtualized storage services like EBS. The locally attached disk is not replicated, so a \\nphysical disk failure can lose or corrupt data even if the host VM continues run ning. \\nFurthermore, locally attached volumes do not support snapshots or other backup \\nfeatures. \\nDespite these limitations, locally attached disks are extremely useful. In many cases, \\nwe use disks as a local cache and hence don‚Äôt need all the advanced virtualization \\nfeatures of a service like EBS. For example, suppose we‚Äôre running AWS EMR on \\nEC2 instances. We may be running an ephemeral job that consumes data from S3, \\nstores it temporarily in the distributed filesystem running across the instances, \\nprocesses the data, and writes the results back to S3. The EMR filesystem builds in'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 230, 'page_label': '231'}, page_content='Data Storage Systems  | \\n 223 \\nreplication and redundancy and is serving as a cache rather than permanent storage. \\nThe EC2 instance store is a perfectly suitable solution in this case and can enhance \\nperformance since data can be read and processed locally without flowing over a \\nnetwork (see Figure 6-7). \\n \\nFigure 6-7. Instance store volumes can be used as a processing cache in an \\nephemeral Hadoop cluster \\nWe recommend that engineers think about locally attached storage in worst -case \\nscenarios. What are the consequences of a local disk failure? Of an accidental VM or \\ncluster shutdown? Of a zonal or regional cloud outage? If none of these scenarios will \\nhave catastrophic consequences when data on locally attached volumes is lost, local \\nstorage may be a cost-effective and performant option. In addition, simple mitigation \\nstrategies (periodic checkpoint backups to S3) can prevent data loss. \\nObject Storage \\nObject storage contains objects of all shapes and sizes ( Figure 6-8). The term object \\nstorage is somewhat confusing because object has several meanings in computer \\nscience. In this context, we‚Äôre talking about a specialized file -like construct. It could \\nbe any type of file‚ÄîTXT, CSV , JSON, images, videos, or audio.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 231, 'page_label': '232'}, page_content='224  | Chapter 6: Storage \\n \\nFigure 6-8. Object storage contains immutable objects of all shapes and sizes. \\nUnlike files on a local disk, objects cannot be modified in place. \\nObject stores have grown in importance and popularity with the rise of big data and \\nthe cloud. Amazon S3, Azure Blob Storage, and Google Cloud Storage (GCS) are \\nwidely used object stores. In addition, many cloud data warehouses (and a growing \\nnumber of databases) utilize object storage as their storage layer, and cloud data lakes \\ngenerally sit on object stores. \\nAlthough many on-premises object storage systems can be installed on server clusters, \\nwe‚Äôll focus mostly on fully managed cloud object stores. From an operational \\nperspective, one of the most attractive characteristics of cloud object storage is that it \\nis straightforward to manage and use. Object storage was arguably one of the first \\n‚Äúserverless‚Äù services; engineers don‚Äôt need to consider the characteristics of underlying \\nserver clusters or disks. \\nAn object store is a key -value store for immutable data objects. We lose much of the \\nwriting flexibility we expect with file storage on a local disk in an object store. Objects \\ndon‚Äôt support random writes or append operations; instead, they are written once as a \\nstream of bytes. After this initial write, objects become immutable. To change data in \\nan object or append data to it, we must rewrite the full object. Object stores generally \\nsupport random reads through range requests, but these lookups may perfor m much \\nworse than random reads from data stored on an SSD. \\nFor a software developer used to leveraging local random access file storage, the \\ncharacteristics of objects might seem like constraints, but less is more; object stores \\ndon‚Äôt need to support locks or change synchronization, allowing data storage across \\nmassive disk clusters. Object stores support extremely performant parallel stream \\nwrites and reads across many disks, and this parallelism is hidden from engineers, who \\ncan simply deal with the stream rather than communicating with individual disks. In a \\ncloud environment, write speed scales with the number of streams being written up to \\nquota limits set by the vendor. Read bandwidth can scale with the number of parallel \\nrequests, the number of virtual machines employed to read data, and the number of \\nCPU cores. These characteristics make object storage ideal for serving high -volume \\nweb traffic or delivering data to highly parallel distributed query engines.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 232, 'page_label': '233'}, page_content='Data Storage Systems  | \\n 225 \\nTypical cloud object stores save data in several availability zones, dramatically \\nreducing the odds that storage will go fully offline or be lost in an unrecoverable way. \\nThis durability and availability are built into the cost; cloud storage vendors offer other \\nstorage classes at discounted prices in exchange for reduced durability or availability. \\nWe‚Äôll discuss this in ‚ÄúStorage classes and tiers‚Äù on page 210. \\nCloud object storage is a key ingredient in separating compute and storage, allowing \\nengineers to process data with ephemeral clusters and scale these clusters up and down \\non demand. This is a key factor in making big data available to smaller organizations \\nthat can‚Äôt afford to own hardware for data jobs that they‚Äôll run only occasionally. Some \\nmajor tech companies will continue to run permanent Hadoop clusters on their \\nhardware. Still, the general trend is that most organizations will move data processing \\nto the cloud, using an object store as essential storage and serving layer while \\nprocessing data on ephemeral clusters. \\nIn object storage, available storage space is also highly scalable, an ideal characteristic \\nfor big data systems. Storage space is constrained by the number of disks the storage \\nprovider owns, but these providers handle exabytes of data. In a cloud environ ment, \\navailable storage space is virtually limitless; in practice, the primary limit on storage \\nspace for public cloud customers is budget. From a practical standpoint, engineers can \\nquickly store massive quantities of data for projects without planning mo nths in \\nadvance for necessary servers and disks. \\nObject stores for data engineering applications \\nFrom the standpoint of data engineering, object stores provide excellent performance \\nfor large batch reads and batch writes. This corresponds well to the use case for massive \\nOLAP systems. A bit of data engineering folklore says that object stores are not good \\nfor updates, but this is only partially true. Object stores are an inferior fit for \\ntransactional workloads with many small updates every second; these use cases are \\nmuch better served by transactional databases or block storage systems. Object stores \\nwork well for a low rate of update operations, where each operation updates a large \\nvolume of data. \\nObject stores are now the gold standard of storage for data lakes. In the early days of \\ndata lakes, write once, read many (WORM) was the operational standard, but this had \\nmore to do with the complexities of managing data versions and files than the \\nlimitations of HDFS and object stores. Since then, systems such as Apache Hudi and \\nDelta Lake have emerged to manage this complexity, and privacy regulations such as \\nGDPR and CCPA have made deletion and update capabilities imperative. Update \\nmanagement for object storage is the central idea behind the data lakehouse concept, \\nwhich we introduced in Chapter 3.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 233, 'page_label': '234'}, page_content='226  | Chapter 6: Storage \\nObject storage is an ideal repository for unstructured data in any format beyond these \\nstructured data applications. Object storage can house any binary data with no \\nconstraints on type or structure and frequently plays a role in ML pipelines for raw \\ntext, images, video, and audio. \\nObject lookup \\nAs we mentioned, object stores are key -value stores. What does this mean for \\nengineers? It‚Äôs critical to understand that, unlike file stores, object stores do not utilize \\na directory tree to find objects. The object store uses a top -level logical container (a \\nbucket in S3 and GCS) and references objects by key. A simple example in S3 might \\nlook like this: \\nS3://oreilly-data-engineering-book/data-example.json \\nIn this case, S3://oreilly-data-engineering-book/ is the bucket name, and \\ndataexample.json is the key pointing to a particular object. S3 bucket names must \\nbe unique across all of AWS. Keys are unique within a bucket. Although cloud object \\nstores may appear to support directory tree semantics, no true directory hierarchy \\nexists. We might store an object with the following full path: \\nS3://oreilly-data-engineering-book/project-data/11/23/2021/data.txt \\nOn the surface, this looks like subdirectories you might find in a regular file folder \\nsystem: project-data, 11, 23, and 2021. Many cloud console interfaces allow \\nusers to view the objects inside a ‚Äúdirectory,‚Äù and cloud command -line tools often \\nsupport Unix-style commands such as ls inside an object store directory. However, \\nbehind the scenes, the object system does not traverse a directory tree to reach the \\nobject. Instead, it simply sees a key ( project-data/11/23/2021/data.txt) \\nthat happens to match dire ctory semantics. This might seem like a minor technical \\ndetail, but engineers need to understand that certain ‚Äúdirectory‚Äù -level operations are \\ncostly in an object store. To run aws ls S3://oreilly-data-engineering-\\nbook/projectdata/11/ the object store must filter keys on the key prefix \\nproject-data/11/. If the bucket contains millions of objects, this operation might \\ntake some time, even if the ‚Äúsubdirectory‚Äù houses only a few objects. \\nObject consistency and versioning \\nAs mentioned, object stores don‚Äôt support in -place updates or appends as a general \\nrule. We write a new object under the same key to update an object. When data \\nengineers utilize updates in data processes, they must be aware of the consistency \\nmodel for the object store they‚Äôre using. Object stores may be eventually consistent or \\nstrongly consistent. For example, until recently, S3 was eventually consistent; after a \\nnew version of an object was written under the same key, the object store might \\nsometimes re turn the old version of the object. The eventual part of eventual'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 234, 'page_label': '235'}, page_content='Data Storage Systems  | \\n 227 \\nconsistency means that after enough time has passed, the storage cluster reaches a state \\nsuch that only the latest version of the object will be returned. This contrasts with the \\nstrong consistency model we expect of local disks attached to a server: reading after a \\nwrite will return the most recently written data. \\nIt might be desirable to impose strong consistency on an object store for various \\nreasons, and standard methods are used to achieve this. One approach is to add a \\nstrongly consistent database (e.g., PostgreSQL) to the mix. Writing an object is now a \\ntwo-step process: \\n1. Write the object. \\n2. Write the returned metadata for the object version to the strongly consistent \\ndatabase. \\nThe version metadata (an object hash or an object timestamp) can uniquely identify an \\nobject version in conjunction with the object key. To read an object, a reader undertakes \\nthe following steps: \\n1. Fetch the latest object metadata from the strongly consistent database. \\n2. Query object metadata using the object key. Read the object data if it matches the \\nmetadata fetched from the consistent database. \\n3. If the object metadata does not match, repeat step 2 until the latest version of the \\nobject is returned. \\nA practical implementation has exceptions and edge cases to consider, such as when \\nthe object gets rewritten during this querying process. These steps can be managed \\nbehind an API so that an object reader sees a strongly consistent object store at the cost \\nof higher latency for object access. \\nObject versioning is closely related to object consistency. When we rewrite an object \\nunder an existing key in an object store, we‚Äôre essentially writing a brand-new object, \\nsetting references from the existing key to the object, and deleting the old objec t \\nreferences. Updating all references across the cluster takes time, hence the potential \\nfor stale reads. Eventually, the storage cluster garbage collector deallocates the space \\ndedicated to the dereferenced data, recycling disk capacity for use by new objects. \\nWith object versioning turned on, we add additional metadata to the object that \\nstipulates a version. While the default key reference gets updated to point to the new \\nobject, we retain other pointers to previous versions. We also maintain a version list so \\nthat clients can get a list of all object versions, and then pull a specific version. Because \\nold versions of the object are still referenced, they aren‚Äôt cleaned up by the garbage \\ncollector.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 235, 'page_label': '236'}, page_content='228  | Chapter 6: Storage \\nIf we reference an object with a version, the consistency issue with some object storage \\nsystems disappears: the key and version metadata together form a unique reference to \\na particular, immutable data object. We will always get the same object back when we \\nuse this pair, provided that we haven‚Äôt deleted it. The consistency issue still exists when \\na client requests the ‚Äúdefault‚Äù or ‚Äúlatest‚Äù version of an object. \\nThe principal overhead that engineers need to consider with object versioning is the \\ncost of storage. Historical versions of objects generally have the same associated \\nstorage costs as current versions. Object version costs may be nearly insignificant or \\ncatastrophically expensive, depending on various factors. The data size is an issue, as \\nis update frequency; more object versions can lead to significantly larger data size. \\nKeep in mind that we‚Äôre talking about brute -force object versioning. Object storage  \\nsystems generally store full object data for each version, not differential snapshots. \\nEngineers also have the option of deploying storage lifecycle policies. Lifecycle \\npolicies allow automatic deletion of old object versions when certain conditions are \\nmet (e.g., when an object version reaches a certain age or many newer versions exist). \\nCloud vendors also offer various archival data tiers at heavily discounted prices, and \\nthe archival process can be managed using lifecycle policies. \\nStorage classes and tiers \\nCloud vendors now offer storage classes that discount data storage pricing in exchange \\nfor reduced access or reduced durability. We use the term reduced access here because \\nmany of these storage tiers still make data highly available, but with high retrieval \\ncosts in exchange for reduced storage costs. \\nLet‚Äôs look at a couple of examples in S3 since Amazon is a benchmark for cloud service \\nstandards. The S3 Standard-Infrequent Access storage class discounts monthly storage \\ncosts for increased data retrieval costs. (See ‚ÄúA Brief Detour on Cloud Economics‚Äù on \\npage 125 for a theoretical discussion of the economics of cloud storage tiers.) Amazon \\nalso offers the Amazon S3 One Zone-Infrequent Access tier, replicating only to a single \\nzone. Projected availability drops from 99.9% to 99.5% to account for the possi bility \\nof a zonal outage. Amazon still claims extremely high data durability, with the caveat \\nthat data will be lost if an availability zone is destroyed. \\nFurther down the tiers of reduced access are the archival tiers in S3 Glacier. S3 Glacier \\npromises a dramatic reduction in long-term storage costs for much higher access costs. \\nUsers have various retrieval speed options, from minutes to hours, with higher retrieval \\ncosts for faster access. For example, at the time of this writing, S3 Glacier Deep \\nArchive discounts storage costs even further; Amazon advertises that storage costs start \\nat $1 per terabyte per month. In exchange, data restoration takes 12 hours. In addition,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 236, 'page_label': '237'}, page_content='Data Storage Systems  | \\n 229 \\nthis storage class is designed for data that will be stored 7 ‚Äì10 years and be accessed \\nonly one to two times per year. \\nBe aware of how you plan to utilize archival storage, as it‚Äôs easy to get into and often \\ncostly to access data, especially if you need it more often than expected. See Chapter \\n4 for a more extensive discussion of archival storage economics. \\nObject store‚Äìbacked filesystems \\nObject store synchronization solutions have become increasingly popular. Tools like \\ns3fs and Amazon S3 File Gateway allow users to mount an S3 bucket as local storage. \\nUsers of these tools should be aware of the characteristics of writes to the filesystem \\nand how these will interact with the characteristics and pricing of object storage. File \\nGateway, for example, handles changes to files fairly efficiently by combining portions \\nof objects into a new object using the advanced capabilities of S3. However, hi gh-\\nspeed transactional writing will overwhelm the update capabilities of an object store. \\nMounting object storage as a local filesystem works well for files that are updated \\ninfrequently. \\nCache and Memory-Based Storage Systems \\nAs discussed in ‚ÄúRaw Ingredients of Data Storage‚Äù on page 191, RAM offers excellent \\nlatency and transfer speeds. However, traditional RAM is extremely vulnerable to data \\nloss because a power outage lasting even a second can erase data. RAM-based storage \\nsystems are generally focused on caching applications, presenting data for quick access \\nand high bandwidth. Data should generally be written to a more durable medium for \\nretention purposes. \\nThese ultra-fast cache systems are useful when data engineers need to serve data with \\nultra-fast retrieval latency. \\nExample: Memcached and lightweight object caching \\nMemcached is a key-value store designed for caching database query results, API call \\nresponses, and more. Memcached uses simple data structures, supporting either string \\nor integer types. Memcached can deliver results with very low latency while also \\ntaking the load off backend systems. \\nExample: Redis, memory caching with optional persistence \\nLike Memcached, Redis is a key-value store, but it supports somewhat more complex \\ndata types (such as lists or sets). Redis also builds in multiple persistence mechanisms, \\nincluding snapshotting and journaling. With a typical configuration, Redis writes data'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 237, 'page_label': '238'}, page_content='230  | Chapter 6: Storage \\nroughly every two seconds. Redis is thus suitable for extremely high -performance \\napplications but can tolerate a small amount of data loss. \\nThe Hadoop Distributed File System \\nIn the recent past, ‚ÄúHadoop‚Äù was virtually synonymous with ‚Äúbig data.‚Äù The Hadoop \\nDistributed File System is based on Google File System (GFS)  and was initially \\nengineered to process data with the MapReduce programming model . Hadoop is \\nsimilar to object storage but with a key difference: Hadoop combines compute and \\nstorage on the same nodes, where object stores typically have limited support for \\ninternal processing. \\nHadoop breaks large files into blocks, chunks of data less than a few hundred \\nmegabytes in size. The filesystem is managed by the NameNode, which maintains \\ndirectories, file metadata, and a detailed catalog describing the location of file blocks \\nin the cluster. In a typical configuration, each block of data is replicated to three nodes. \\nThis increases both the durability and availability of data. If a disk or node fails, the \\nreplication factor for some file blocks will fall below 3. The NameNode will instru ct \\nother nodes to replicate these file blocks so that they again reach the correct replication \\nfactor. Thus, the probability of losing data is very low, barring a correlated failure (e.g., \\nan asteroid hitting the data center). \\nHadoop is not simply a storage system. Hadoop combines compute resources with \\nstorage nodes to allow in-place data processing. This was originally achieved using the \\nMapReduce programming model, which we discuss in Chapter 8. \\nHadoop is dead. Long live Hadoop! \\nWe often see claims that Hadoop is dead. This is only partially true. Hadoop is no \\nlonger a hot, bleeding-edge technology. Many Hadoop ecosystem tools such as Apache \\nPig are now on life support and primarily used to run legacy jobs. The pure MapReduce \\nprogramming model has fallen by the wayside. HDFS remains widely used in various \\napplications and organizations. \\nHadoop still appears in many legacy installations. Many organizations that adopted \\nHadoop during the peak of the big data craze have no immediate plans to migrate to \\nnewer technologies. This is a good choice for companies that run massive \\n(thousandnode) Hadoop clusters and have the resources to maintain on -premises \\nsystems effectively. Smaller companies may want to reconsider the cost overhead and \\nscale limitations of running a small Hadoop cluster against migrating to cloud \\nsolutions. \\nIn addition, HDFS is a key ingredient of many current big data engines, such as \\nAmazon EMR. In fact, Apache Spark is still commonly run on HDFS clusters. We \\ndiscuss this in more detail in ‚ÄúSeparation of Compute from Storage‚Äù on page 220.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 238, 'page_label': '239'}, page_content='Data Storage Systems  | \\n 231 \\nStreaming Storage \\nStreaming data has different storage requirements than nonstreaming data. In the case \\nof message queues, stored data is temporal and expected to disappear after a certain \\nduration. However, distributed, scalable streaming frameworks like Apache Kafka now \\nallow extremely long-duration streaming data retention. Kafka supports indefinite data \\nretention by pushing old, infrequently accessed messages down to object storage. \\nKafka competitors (including Amazon Kinesis, Apache Pulsar, and Google Cloud \\nPub/Sub) also support long data retention. \\nClosely related to data retention in these systems is the notion of replay. Replay allows \\na streaming system to return a range of historical stored data. Replay is the standard \\ndata-retrieval mechanism for streaming storage systems. Replay can be used to run \\nbatch queries over a time range or to reprocess data in a streaming pipeline. Chapter 7 \\ncovers replay in more depth. \\nOther storage engines have emerged for real -time analytics applications. In some \\nsense, transactional databases emerged as the first real -time query engines; data \\nbecomes visible to queries as soon as it is written. However, these databases have well-\\nknown scaling and locking limitations, especially for analytics queries that run across \\nlarge volumes of data. While scalable versions of row-oriented transactional databases \\nhave overcome some of these limitations, they are still not truly optimized for analytics \\nat scale. \\nIndexes, Partitioning, and Clustering \\nIndexes provide a map of the table for particular fields and allow extremely fast lookup \\nof individual records. Without indexes, a database would need to scan an entire table \\nto find the records satisfying a WHERE condition. \\nIn most RDBMSs, indexes are used for primary table keys (allowing unique \\nidentification of rows) and foreign keys (allowing joins with other tables). Indexes can \\nalso be applied to other columns to serve the needs of specific applications. Using \\nindexes, an RDBMS can look up and update thousands of rows per second. \\nWe do not cover transactional database records in depth in this book; numerous \\ntechnical resources are available on this topic. Rather, we are interested in the evolution \\naway from indexes in analytics-oriented storage systems and some new developments \\nin indexes for analytics use cases. \\nThe evolution from rows to columns \\nAn early data warehouse was typically built on the same type of RDBMS used for \\ntransactional applications. The growing popularity of MPP systems meant a shift'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 239, 'page_label': '240'}, page_content='232  | Chapter 6: Storage \\ntoward parallel processing for significant improvements in scan performance across \\nlarge quantities of data for analytics purposes. However, these row-oriented MPPs still \\nused indexes to support joins and condition checking. \\nIn ‚ÄúRaw Ingredients of Data Storage‚Äù on page 191, we discuss columnar serialization. \\nColumnar serialization allows a database to scan only the columns required for a \\nparticular query, sometimes dramatically reducing the amount of data read from the \\ndisk. In addition, arranging data by column packs similar values next to each other, \\nyielding high -compression ratios with minimal compression overhead. This allows \\ndata to be scanned more quickly from disk and over a network. \\nColumnar databases perform poorly for transactional use cases ‚Äîi.e., when we try to \\nlook up large numbers of individual rows asynchronously. However, they perform \\nextremely well when large quantities of data must be scanned‚Äîe.g., for complex data \\ntransformations, aggregations, statistical calculations, or evaluation of complex \\nconditions on large datasets. \\nIn the past, columnar databases performed poorly on joins, so the advice for data \\nengineers was to denormalize data, using wide schemas, arrays, and nested data \\nwherever possible. Join performance for columnar databases has improved \\ndramatically in recent years, so while there can still be performance advantages in \\ndenormalization, this is no longer a necessity. You‚Äôll learn more about normalization \\nand denormalization in Chapter 8. \\nFrom indexes to partitions and clustering \\nWhile columnar databases allow for fast scan speeds, it‚Äôs still helpful to reduce the \\namount of data scanned as much as possible. In addition to scanning only data in \\ncolumns relevant to a query, we can partition a table into multiple subtables by splitting \\nit on a field. It is quite common in analytics and data science use cases to scan over a \\ntime range, so date - and time -based partitioning is extremely common. Columnar \\ndatabases generally support a variety of other partition schemes as well. \\nClusters allow finer-grained organization of data within partitions. A clustering scheme \\napplied within a columnar database sorts data by one or a few fields, colocating similar \\nvalues. This improves performance for filtering, sorting, and joining these values. \\nExample: Snowflake micro-partitioning \\nWe mention Snowflake micro-partitioning because it‚Äôs a good example of recent \\ndevelopments and evolution in approaches to columnar storage. Micro partitions are \\nsets of rows between 50 and 500 megabytes in uncompressed size. Snowflake uses an \\nalgorithmic approach that attempts to cluster together similar rows. This contrasts the \\ntraditional naive approach to partitioning on a single designated field, such as a date.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 240, 'page_label': '241'}, page_content=\"Data Storage Systems  | \\n 233 \\nSnowflake specifically looks for values that are repeated in a field across many rows. \\nThis allows aggressive pruning of queries based on predicates. For example, a WHERE \\nclause might stipulate the following: \\nWHERE created_date='2022-01-02' \\nIn such a query, Snowflake excludes any micro-partitions that don‚Äôt include this date, \\neffectively pruning this data. Snowflake also allows overlapping micro -partitions, \\npotentially partitioning on multiple fields showing significant repeats. \\nEfficient pruning is facilitated by Snowflake‚Äôs metadata database, which stores a \\ndescription of each micro-partition, including the number of rows and value ranges for \\nfields. At each query stage, Snowflake analyzes micro -partitions to determine which \\nones need to be scanned. Snowflake uses the term hybrid columnar storage,71partially \\nreferring to the fact that its tables are broken into small groups of rows, even though \\nstorage is fundamentally columnar. The metadata database plays a role similar to an \\nindex in a traditional relational database. \\n \\n71 Benoit Dageville, ‚ÄúThe Snowflake Elastic Data Warehouse,‚Äù SIGMOD ‚Äô16: Proceedings of the 2016 \\nInternational Conference on Management of Data (June 2016): 215‚Äì226, https://oreil.ly/Tc1su.\"),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 241, 'page_label': '242'}, page_content='234  | Chapter 6: Storage \\nData Engineering Storage Abstractions \\nData engineering storage abstractions are data organization and query patterns that sit \\nat the heart of the data engineering lifecycle and are built atop the data storage systems \\ndiscussed previously (see Figure 6-3). We introduced many of these abstractions in \\nChapter 3, and we will revisit them here. \\nThe main types of abstractions we‚Äôll concern ourselves with are those that support data \\nscience, analytics, and reporting use cases. These include data warehouse, data lake, \\ndata lakehouse, data platforms, and data catalogs. We won‚Äôt cover source systems, as \\nthey are discussed in Chapter 5. \\nThe storage abstraction you require as a data engineer boils down to a few key \\nconsiderations: \\nPurpose and use case \\nYou must first identify the purpose of storing the data. What is it used for? \\nUpdate patterns \\nIs the abstraction optimized for bulk updates, streaming inserts, or upserts? \\nCost \\nWhat are the direct and indirect financial costs? The time to value? The \\nopportunity costs? \\nSeparate storage and compute \\nThe trend is toward separating storage and compute, but most systems hybridize \\nseparation and colocation. We cover this in ‚ÄúSeparation of Compute from Storage‚Äù \\non page 220 since it affects purpose, speed, and cost. \\nYou should know that the popularity of separating storage from compute means the \\nlines between OLAP databases and data lakes are increasingly blurring. Major cloud \\ndata warehouses and data lakes are on a collision course. In the future, the differences \\nbetween these two may be in name only since they might functionally and technically \\nbe very similar under the hood. \\nThe Data Warehouse \\nData warehouses are a standard OLAP data architecture. As discussed in Chapter 3, the \\nterm data warehouse  refers to technology platforms (e.g., Google BigQuery and \\nTeradata), an architecture for data centralization, and an organizational pattern within \\na company. In terms of storage trends, we‚Äôve evolved from building data warehouses \\natop conventional transactional databases, row-based MPP systems (e.g., Teradata and \\nIBM Netezza), and columnar MPP systems (e.g., Vertica and Teradata Columnar) to \\ncloud data warehouses and data platforms. (See our data warehousing discussion in \\nChapter 3 for more details on MPP systems.)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 242, 'page_label': '243'}, page_content='|  235 \\nData Engineering Storage Abstractions  \\nIn practice, cloud data warehouses are often used to organize data into a data lake, a \\nstorage area for massive amounts of unprocessed raw data, as originally conceived by \\nJames Dixon.72 Cloud data warehouses can handle massive amounts of raw text and \\ncomplex JSON documents. The limitation is that cloud data warehouses cannot handle \\ntruly unstructured data, such as images, video, or audio, unlike a true data lake. Cloud \\ndata warehouses can be coupled with object storage to provide a complete data -lake \\nsolution. \\nThe Data Lake \\nThe data lake was originally conceived as a massive store where data was retained in \\nraw, unprocessed form. Initially, data lakes were built primarily on Hadoop systems, \\nwhere cheap storage allowed for retention of massive amounts of data without the cost \\noverhead of a proprietary MPP system. \\nThe last five years have seen two major developments in the evolution of data lake \\nstorage. First, a major migration toward separation of compute and storage  has \\noccurred. In practice, this means a move away from Hadoop toward cloud object \\nstorage for long-term retention of data. Second, data engineers discovered that much \\nof the functionality offered by MPP systems (schema management; update, merge and \\ndelete capabilities) and initially dismissed in the rush to data lakes was, in fact, \\nextremely useful. This led to the notion of the data lakehouse. \\nThe Data Lakehouse \\nThe data lakehouse is an architecture that combines aspects of the data warehouse and \\nthe data lake. As it is generally conceived, the lakehouse stores data in object storage \\njust like a lake. However, the lakehouse adds to this arrangement features designed to \\nstreamline data management and create an engineering experience similar to a data \\nwarehouse. This means robust table and schema support and features for managing \\nincremental updates and deletes. Lakehouses typically also support table history and \\nrollback; this is accomplished by retaining old versions of files and metadata. \\nA lakehouse system is a metadata and file -management layer deployed with data \\nmanagement and transformation tools. Databricks has heavily promoted the lakehouse \\nconcept with Delta Lake, an open source storage management system. We would be \\nremiss not to po int out that the architecture of the data lakehouse is similar to the \\narchitecture used by various commercial data platforms, including BigQuery and \\nSnowflake. These systems store data in object storage and provide automated metadata \\n \\n72 James Dixon, ‚ÄúData Lakes Revisited,‚Äù James Dixon‚Äô s Blog, September 25, 2014, https://oreil.ly/FH25v.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 243, 'page_label': '244'}, page_content='236  | Chapter 6: Storage \\nmanagement, table history, and update/delete capabilities. The complexities of \\nmanaging underlying files and storage are fully hidden from the user. \\nThe key advantage of the data lakehouse over proprietary tools is interoperability. \\nIt‚Äôs much easier to exchange data between tools when stored in an open file format. \\nReserializing data from a proprietary database format incurs overhead in processing, \\ntime, and cost. In a data lakehouse architecture, various tools can connect to the \\nmetadata layer and read data directly from object storage. \\nIt is important to emphasize that much of the data in a data lakehouse may not have a \\ntable structure imposed. We can impose data warehouse features where we need them \\nin a lakehouse, leaving other data in a raw or even unstructured format. \\nThe data lakehouse technology is evolving rapidly. A variety of new competitors to \\nDelta Lake have emerged, including Apache Hudi and Apache Iceberg. See Appendix \\nA for more details. \\nData Platforms \\nIncreasingly, vendors are styling their products as data platforms. These vendors have \\ncreated their ecosystems of interoperable tools with tight integration into the core data \\nstorage layer. In evaluating platforms, engineers must ensure that the tools offered meet \\ntheir needs. Tools not directly provided in the platform can still interoperate, with extra \\ndata overhead for data interchange. Platforms also emphasize close integration with \\nobject storage for unstructured use cases, as mentioned in our discussion of cloud data \\nwarehouses. \\nAt this point, the notion of the data platform frankly has yet to be fully fleshed out. \\nHowever, the race is on to create a walled garden of data tools, both simplifying the \\nwork of data engineering and generating significant vendor lock-in. \\nStream-to-Batch Storage Architecture \\nThe stream -to-batch storage architecture has many similarities to the Lambda \\narchitecture, though some might quibble over the technical details. Essentially, data \\nflowing through a topic in the streaming storage system is written out to multiple \\nconsumers. \\nSome of these consumers might be real-time processing systems that generate statistics \\non the stream. In addition, a batch storage consumer writes data for long-term retention \\nand batch queries. The batch consumer could be AWS Kinesis Firehose, which can \\ngenerate S3 objects based on configurable triggers (e.g., time and batch size). Systems \\nsuch as BigQuery ingest streaming data into a streaming buffer. This streaming buffer \\nis automatically reserialized into columnar object storage. The query engine supports \\nseamless querying of both the streaming buffer and the object data to provide users a \\ncurrent, nearly real-time view of the table.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 244, 'page_label': '245'}, page_content='|  237 \\nData Engineering Storage Abstractions  \\nBig Ideas and Trends in Storage \\nIn this section, we‚Äôll discuss some big ideas in storage ‚Äîkey considerations that you \\nneed to keep in mind as you build out your storage architecture. Many of these \\nconsiderations are part of larger trends. For example, data catalogs fit under the trend \\ntoward ‚Äúenterprisey‚Äù data engineering and data management. Separation of compute \\nfrom storage is now largely an accomplished fact in cloud data systems. And data \\nsharing is an increasingly important consideration as businesses adopt data technology. \\nData Catalog \\nA data catalog  is a centralized metadata store for all data across an organization. \\nStrictly speaking, a data catalog is not a top -level data storage abstraction, but it \\nintegrates with various systems and abstractions. Data catalogs typically work across \\noperational and analytics data sources, integrate data lineage and presentation of data \\nrelationships, and allow user editing of data descriptions. \\nData catalogs are often used to provide a central place where people can view their \\ndata, queries, and data storage. As a data engineer, you‚Äôll likely be responsible for \\nsetting up and maintaining the various data integrations of data pipeline and storage \\nsystems that will integrate with the data catalog and the integrity of the data catalog \\nitself. \\nCatalog application integration \\nIdeally, data applications are designed to integrate with catalog APIs to handle their \\nmetadata and updates directly. As catalogs are more widely used in an organization, it \\nbecomes easier to approach this ideal. \\nAutomated scanning \\nIn practice, cataloging systems typically need to rely on an automated scanning layer \\nthat collects metadata from various systems such as data lakes, data warehouses, and \\noperational databases. Data catalogs can collect existing metadata and may also use \\nscanning tools to infer metadata such as key relationships or the presence of sensitive \\ndata. \\nData portal and social layer \\nData catalogs also typically provide a human access layer through a web interface, \\nwhere users can search for data and view data relationships. Data catalogs can be \\nenhanced with a social layer offering Wiki functionality. This allows users to provide'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 245, 'page_label': '246'}, page_content='238  | Chapter 6: Storage \\ninformation on their datasets, request information from other users, and post updates \\nas they become available.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 246, 'page_label': '247'}, page_content='Big Ideas and Trends in Storage  | \\n 239 \\nData catalog use cases \\nData catalogs have both organizational and technical use cases. Data catalogs make \\nmetadata easily available to systems. For instance, a data catalog is a key ingredient of \\nthe data lakehouse, allowing table discoverability for queries. \\nOrganizationally, data catalogs allow business users, analysts, data scientists, and \\nengineers to search for data to answer questions. Data catalogs streamline \\ncrossorganizational communications and collaboration. \\nData Sharing \\nData sharing allows organizations and individuals to share specific data and carefully \\ndefined permissions with specific entities. Data sharing allows data scientists to share \\ndata from a sandbox with their collaborators within an organization. Across \\norganizations, data sharing facilitates collaboration between partner businesses. For \\nexample, an ad tech company can share advertising data with its customers. \\nA cloud multitenant environment makes interorganizational collaboration much easier. \\nHowever, it also presents new security challenges. Organizations must carefully \\ncontrol policies that govern who can share data with whom to prevent accidental \\nexposure or deliberate exfiltration. \\nData sharing is a core feature of many cloud data platforms. See Chapter 5 for a more \\nextensive discussion of data sharing. \\nSchema \\nWhat is the expected form of the data? What is the file format? Is it structured, \\nsemistructured, or unstructured? What data types are expected? How does the data fit \\ninto a larger hierarchy? Is it connected to other data through shared keys or other \\nrelationships? \\nNote that schema need not be relational. Rather, data becomes more useful when we \\nhave as much information about its structure and organization. For images stored in a \\ndata lake, this schema information might explain the image format, resolution, and the \\nway the images fit into a larger hierarchy. \\nSchema can function as a sort of Rosetta stone, instructions that tell us how to read the \\ndata. Two major schema patterns exist: schema on write and schema on read. Schema \\non write is essentially the traditional data warehouse pattern: a table has an integrated \\nschema; any writes to the table must conform. To support schema on write, a data lake \\nmust integrate a schema metastore. \\nWith schema on read, the schema is dynamically created when data is written, and a \\nreader must determine the schema when reading the data. Ideally, schema on read is'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 247, 'page_label': '248'}, page_content='240  | Chapter 6: Storage \\nimplemented using file formats that implement built -in schema information, such as \\nParquet or JSON. CSV files are notorious for schema inconsistency and are not \\nrecommended in this setting. \\nThe principal advantage of schema on write is that it enforces data standards, making \\ndata easier to consume and utilize in the future. Schema on read emphasizes flexibility, \\nallowing virtually any data to be written. This comes at the cost of greater diff iculty \\nconsuming data in the future. \\nSeparation of Compute from Storage \\nA key idea we revisit throughout this book is the separation of compute from storage. \\nThis has emerged as a standard data access and query pattern in today‚Äôs cloud era. Data \\nlakes, as we discussed, store data in object stores and spin up temporary compute \\ncapacity to read and process it. Most fully managed OLAP products now rely on object \\nstorage behind the scenes. To understand the motivations for separating compute and \\nstorage, we should first look at the colocation of compute and storage. \\nColocation of compute and storage \\nColocation of compute and storage has long been a standard method to improve \\ndatabase performance. For transactional databases, data colocation allows fast, \\nlowlatency disk reads and high bandwidth. Even when we virtualize storage (e.g., \\nusing Amazon EBS), data is located relatively close to the host machine. \\nThe same basic idea applies for analytics query systems running across a cluster of \\nmachines. For example, with HDFS and MapReduce, the standard approach is to locate \\ndata blocks that need to be scanned in the cluster, and then push individual map jobs \\nout to these blocks. The data scan and processing for the map step are strictly local. \\nThe reduce step involves shuffling data across the cluster, but keeping map steps local \\neffectively preserves more bandwidth for shuffling, delivering better overall \\nperformance; map steps that filter heavily also dramatically reduce the amount of data \\nto be shuffled. \\nSeparation of compute and storage \\nIf colocation of compute and storage delivers high performance, why the shift toward \\nseparation of compute and storage? Several motivations exist. \\nEphemerality and scalability.    In the cloud, we‚Äôve seen a dramatic shift toward \\nephemerality. In general, it‚Äôs cheaper to buy and host a server than to rent it from a \\ncloud provider, provided that you‚Äôre running it 24 hours a day nonstop for years on \\nend. In practice, workloads vary dramatically, and significant efficiencies are realized'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 248, 'page_label': '249'}, page_content='Big Ideas and Trends in Storage  | \\n 241 \\nwith a pay -as-you-go model if servers can scale up and down. This is true for web \\nservers in online retail, and it is also true for big data batch jobs that may run only \\nperiodically. \\nEphemeral compute resources allow engineers to spin up massive clusters to complete \\njobs on time and then delete clusters when these jobs are done. The performance \\nbenefits of temporarily operating at ultra -high scale can outweigh the bandwidth \\nlimitations of object storage. \\nData durability and availability.    Cloud object stores significantly mitigate the risk \\nof data loss and generally provide extremely high uptime (availability). For example, \\nS3 stores data across multiple zones; if a natural disaster destroys a zone, data is still \\navailable from the remaining zones. Having multiple zones available also reduces the \\nodds of a data outage. If resources in one zone go down, engineers can spin up the \\nsame resources in a different zone. \\nThe potential for a misconfiguration that destroys data in object storage is still \\nsomewhat scary, but simple -to-deploy mitigations are available. Copying data to \\nmultiple cloud regions reduces this risk since configuration changes are generally \\ndeployed to only one region at a time. Replicating data to multiple storage providers \\ncan further reduce the risk. \\nHybrid separation and colocation \\nThe practical realities of separating compute from storage are more complicated than \\nwe‚Äôve implied. In practice, we constantly hybridize colocation and separation to realize \\nthe benefits of both approaches. This hybridization is typically done in two ways:  \\nmultitier caching and hybrid object storage. \\nWith multitier caching , we utilize object storage for long -term data retention and \\naccess but spin up local storage to be used during queries and various stages of data \\npipelines. Both Google and Amazon offer versions of hybrid object storage (object \\nstorage that is tightly integrated with compute). \\nLet‚Äôs look at examples of how some popular processing engines hybridize separation \\nand colocation of storage and compute. \\nExample: AWS EMR with S3 and HDFS.    Big data services like Amazon EMR spin \\nup temporary HDFS clusters to process data. Engineers have the option of referencing \\nboth S3 and HDFS as a filesystem. A common pattern is to stand up HDFS on SSD \\ndrives, pull from S3, and save data from intermediate processing steps on local HDFS. \\nDoing so can realize significant performance gains over processing directly from S3.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 249, 'page_label': '250'}, page_content='242  | Chapter 6: Storage \\nFull results are written back to S3 once the cluster completes its steps, and the cluster \\nand HDFS are deleted. Other consumers read the output data directly from S3. \\nExample: Apache Spark.    In practice, Spark generally runs jobs on HDFS or some \\nother ephemeral distributed filesystem to support performant storage of data between \\nprocessing steps. In addition, Spark relies heavily on in -memory storage of data to \\nimprove processing. The problem with owning the infrastructure for running Spark is \\nthat dynamic RAM (DRAM) is extremely expensive; by separating compute and \\nstorage in the cloud, we can rent large quantities of memory and then release that \\nmemory when the job completes. \\nExample: Apache Druid.    Apache Druid relies heavily on SSDs to realize high \\nperformance. Since SSDs are significantly more expensive than magnetic disks, Druid \\nkeeps only one copy of data in its cluster, reducing ‚Äúlive‚Äù storage costs by a factor of \\nthree. \\nOf course, maintaining data durability is still critical, so Druid uses an object store as \\nits durability layer. When data is ingested, it‚Äôs processed, serialized into compressed \\ncolumns, and written to cluster SSDs and object storage. In the event of node  failure \\nor cluster data corruption, data can be automatically recovered to new nodes. In \\naddition, the cluster can be shut down and then fully recovered from SSD storage. \\nExample: Hybrid object storage.    Google‚Äôs Colossus file storage system supports \\nfinegrained control of data block location, although this functionality is not exposed \\ndirectly to the public. BigQuery uses this feature to colocate customer tables in a single \\nlocation, allowing ultra-high bandwidth for queries in that location.73 We refer to this \\nas hybrid object storage because it combines the clean abstractions of object storage \\nwith some advantages of colocating compute and storage. Amazon also offers some \\nnotion of hybrid object storage through S3 Select, a feature that allows users to filter \\nS3 data directly in S3 clusters before data is returned across the network. We speculate \\nthat public clouds will adopt hybrid object storage more widely to improve the \\nperformance of their offerings and make more efficient use of available network \\nresources. Some may be already doing so without disclosing this publicly. \\nThe concept of hybrid object storage underscores that there can still be advantages to \\nhaving low -level access to hardware rather than relying on someone else‚Äôs public \\ncloud. Public cloud services do not expose low-level details of hardware and systems \\n(e.g., data block locations for Colossus), but these details can be extremely useful in \\n \\n73 Valliappa Lakshmanan and Jordan Tigani, Google BigQuery: The Definitive Guide (Sebastopol, CA: \\nO‚ÄôReilly, 2019), 16‚Äì17, 188, https://oreil.ly/5aXXu.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 250, 'page_label': '251'}, page_content='Big Ideas and Trends in Storage  | \\n 243 \\nperformance optimization and enhancement. See our discussion of cloud economics in \\nChapter 4. \\nWhile we‚Äôre now seeing a mass migration of data to public clouds, we believe that \\nmany hyper-scale data service vendors that currently run on public clouds provided by \\nother vendors may build their data centers in the future, albeit with deep network \\nintegration into public clouds. \\nZero-copy cloning \\nCloud-based systems based around object storage support zero-copy cloning . This \\ntypically means that a new virtual copy of an object is created (e.g., a new table) \\nwithout necessarily physically copying the underlying data. Typically, new pointers are \\ncreated to the raw data files, and future changes to these tables will not be recorded in \\nthe old table. For those familiar with the inner workings of object-oriented languages \\nsuch as Python, this type of ‚Äúshallow‚Äù copying is familiar from other contexts. \\nZero-copy cloning is a compelling feature, but engineers must understand its strengths \\nand limitations. For example, cloning an object in a data lake environment and then \\ndeleting the files in the original object might also wipe out the new object. \\nFor fully managed object -store-based systems (e.g., Snowflake and BigQuery), \\nengineers need to be extremely familiar with the exact limits of shallow copying. \\nEngineers have more access to underlying object storage in data lake systems such as \\nDatabricks‚Äîa blessing and a curse. Data engineers should exercise great caution \\nbefore deleting any raw files in the underlying object store. Databricks and other data \\nlake management technologies sometimes also support a notion of deep copying , \\nwhereby all underlying data objects are copied. This is a more expensive process, but \\nalso more robust in the event that files are unintentionally lost or deleted. \\nData Storage Lifecycle and Data Retention \\nStoring data isn‚Äôt as simple as just saving it to object storage or disk and forgetting \\nabout it. You need to think about the data storage lifecycle and data retention. When \\nyou think about access frequency and use cases, ask, ‚ÄúHow important is the data to  \\ndownstream users, and how often do they need to access it?‚Äù This is the data storage \\nlifecycle. Another question you should ask is, ‚ÄúHow long should I keep this data?‚Äù Do \\nyou need to retain data indefinitely, or are you fine discarding it past a certain t ime \\nframe? This is data retention. Let‚Äôs dive into each of these. \\nHot, warm, and cold data \\nDid you know that data has a temperature? Depending on how frequently data is \\naccessed, we can roughly bucket the way it is stored into three categories of'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 251, 'page_label': '252'}, page_content='244  | Chapter 6: Storage \\npersistence: hot, warm, and cold. Query access patterns differ for each dataset (Figure \\n6-9). Typically, newer data is queried more often than older data. Let‚Äôs look at hot, \\ncold, and warm data in that order. \\n \\nFigure 6-9. Hot, warm, and cold data costs associated with access frequency \\nHot data.    Hot data has instant or frequent access requirements. The underlying \\nstorage for hot data is suited for fast access and reads, such as SSD or memory. Because \\nof the type of hardware involved with hot data, storing hot data is often the most \\nexpensive form of storage. Example use cases for hot data include retrieving product \\nrecommendations and product page results. The cost of storing hot data is the highest \\nof these three storage tiers, but retrieval is often inexpensive. \\nQuery results cache is another example of hot data. When a query is run, some query \\nengines will persist the query results in the cache. For a limited time, when the same \\nquery is run, instead of rerunning the same query against storage, the query results \\ncache serves the cached results. This allows for much faster query response times \\nversus redundantly issuing the same query repeatedly. In upcoming chapters, we cover \\nquery results caches in more detail. \\nWarm data.    Warm data is accessed semi-regularly, say, once per month. No hard \\nand fast rules indicate how often warm data is accessed, but it‚Äôs less than hot data and \\nmore than cold data. The major cloud providers offer object storage tiers that \\naccommodate warm data. For exa mple, S3 offers an Infrequently Accessed Tier, and \\nGoogle Cloud has a similar storage tier called Nearline. V endors give their models of \\nrecommended access frequency, and engineers can also do their cost modeling and \\nmonitoring. Stor age of warm data is cheaper than hot data, with slightly more \\nexpensive retrieval costs. \\nCold data.    On the other extreme, cold data  is infrequently accessed data. The \\nhardware used to archive cold data is typically cheap and durable, such as HDD, tape \\nstorage, and cloud -based archival systems. Cold data is mainly meant for long -term'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 252, 'page_label': '253'}, page_content='Big Ideas and Trends in Storage  | \\n 245 \\narchival, when there‚Äôs little to no intention to access the data. Though storing cold data \\nis cheap, retrieving cold data is often expensive. \\nStorage tier considerations.    When considering the storage tier for your data, \\nconsider the costs of each tier. If you store all of your data in hot storage, all of the data \\ncan be accessed quickly. But this comes at a tremendous price! Conversely, if you store \\nall data in cold storage to save on costs, you‚Äôll certainly lower your storage costs, but \\nat the expense of prolonged retrieval times and high retrieval costs if you need to access \\ndata. The storage price goes down from faster/higher performing sto rage to lower \\nstorage. \\nCold storage is popular for archiving data. Historically, cold storage involved physical \\nbackups and often mailing this data to a third party that would archive it in a literal \\nvault. Cold storage is increasingly popular in the cloud. Every cloud vendor of fers a \\ncold data solution, and you should weigh the cost of pushing data into cold storage \\nversus the cost and time to retrieve the data. \\nData engineers need to account for spillover from hot to warm/cold storage. Memory \\nis expensive and finite. For example, if hot data is stored in memory, it can be spilled \\nto disk when there‚Äôs too much new data to store and not enough memory. Some \\ndatabases may move infrequently accessed data to warm or cold tiers, offloading the \\ndata to either HDD or object storage. The latter is increasingly more common because \\nof the cost-effectiveness of object storage. If you‚Äôre in the cloud and using managed \\nservices, disk spillover will happen automatically. \\nIf you‚Äôre using cloud-based object storage, create automated lifecycle policies for your \\ndata. This will drastically reduce your storage costs. For example, if your data needs \\nto be accessed only once a month, move the data to an infrequent access storage tier. \\nIf your data is 180 days old and not accessed for current queries, move it to an archival \\nstorage tier. In both cases, you can automate the migration of data away from regular \\nobject storage, and you‚Äôll save money. That said, consider the retrieval costs‚Äîboth in \\ntime and money‚Äîusing infrequent or archival style storage tiers. Access and retrieval \\ntimes and costs may vary depending on the cloud provider. Some cloud providers make \\nit simple and cheap to migrate data into archive storage, but it is costl y and slow to \\nretrieve your data. \\nData retention \\nBack in the early days of ‚Äúbig data,‚Äù there was a tendency to err on the side of \\naccumulating every piece of data possible, regardless of its usefulness. The expectation \\nwas, ‚Äúwe might need this data in the future.‚Äù This data hoarding inevitably became \\nunwieldy and dirty, giving rise to data swamps and regulatory crackdowns on data \\nretention, among other consequences and nightmares. Nowadays, data engineers need'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 253, 'page_label': '254'}, page_content='246  | Chapter 6: Storage \\nto consider data retention: what data do you need to keep, and how long should you \\nkeep it? Here are some things to think about with data retention. \\nValue.    Data is an asset, so you should know the value of the data you‚Äôre storing. Of \\ncourse, value is subjective and depends on what it‚Äôs worth to your immediate use case \\nand your broader organization. Is this data impossible to re -create, or can it easily be \\nre-created by querying upstream systems? What‚Äôs the impact to downstream users if \\nthis data is available versus if it is not? \\nTime.    The value to downstream users also depends upon the age of the data. New \\ndata is typically more valuable and frequently accessed than older data. Technical \\nlimitations may determine how long you can store data in certain storage tiers. For \\nexample, if you store hot data in cache or memory, you‚Äôll likely need to set a time to \\nlive (TTL), so you can expire data after a certain point or persist it to warm or cold \\nstorage. Otherwise, your hot storage will become full, and queries against the hot data \\nwill suffer from performance lags. \\nCompliance.    Certain regulations (e.g., HIPAA and Payment Card Industry, or PCI) \\nmight require you to keep data for a certain time. In these situations, the data simply \\nneeds to be accessible upon request, even if the likelihood of an access request is low. \\nOther regulations might require you to hold data for only a limited period of time, and \\nyou‚Äôll need to have the ability to delete specific information on time and within \\ncompliance guidelines. You‚Äôll need a storage and archival data process ‚Äîalong with \\nthe ability to search the data ‚Äîthat fits the retention requirements of the particular \\nregulation with which you need to comply. Of course, you‚Äôll want to balance \\ncompliance against cost. \\nCost.    Data is an asset that (hopefully) has an ROI. On the cost side of ROI, an obvious \\nstorage expense is associated with data. Consider the timeline in which you need to \\nretain data. Given our discussion about hot, warm, and cold data, implement automatic \\ndata lifecycle management practices and move the data to cold storage if you don‚Äôt \\nneed the data past the required retention date. Or delete data if it‚Äôs truly not needed. \\nSingle-Tenant Versus Multitenant Storage \\nIn Chapter 3 , we covered the trade -offs between single -tenant and multitenant \\narchitecture. To recap, with single-tenant architecture, each group of tenants (e.g., \\nindividual users, groups of users, accounts, or customers) gets its own dedicated set of \\nresources such as networking, compute, and storage. A multitenant architecture inverts \\nthis and shares these resources among groups of users. Both architectures are widely \\nused. This section looks at the implications of single-tenant and multitenant storage.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 254, 'page_label': '255'}, page_content='Big Ideas and Trends in Storage  | \\n 247 \\nAdopting single-tenant storage means that every tenant gets their dedicated storage. In \\nthe example in Figure 6-10, each tenant gets a database. No data is shared among these \\ndatabases, and storage is totally isolated. An example of using single-tenant storage is \\nthat each customer‚Äôs data must be stored in isolation and cannot be blended with any \\nother customer‚Äôs data. In this case, each customer gets their own database.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 255, 'page_label': '256'}, page_content='248  | Chapter 6: Storage \\n \\nFigure 6-10. In single-tenant storage, each tenant gets their own database \\nSeparate data storage implies separate and independent schemas, bucket structures, and \\neverything related to storage. This means you have the liberty of designing each \\ntenant‚Äôs storage environment to be uniform or let them evolve however they may. \\nSchema variation across customers can be an advantage and a complication; as always, \\nconsider the trade -offs. If each tenant‚Äôs schema isn‚Äôt uniform across all tenants, this \\nhas major consequences if you need to query multiple tenants‚Äô tables to create a unified \\nview of all tenant data. \\nMultitenant storage allows for the storage of multiple tenants within a single database. \\nFor example, instead of the single -tenant scenario where customers get their own \\ndatabase, multiple customers may reside in the same database schemas or tables in a \\nmultitenant database. Storing multitenant data means each tenant‚Äôs data is stored in the \\nsame place (Figure 6-11). \\n \\nFigure 6-11. In this multitenant storage, four tenants occupy the same database \\nYou need to be aware of querying both single and multitenant storage, which we cover \\nin more detail in Chapter 8. \\nWhom You‚Äôll Work With \\nStorage is at the heart of data engineering infrastructure. You‚Äôll interact with the people \\nwho own your IT infrastructure ‚Äîtypically, DevOps, security, and cloud architects. \\nDefining domains of responsibility between data engineering and other teams is \\ncritical. Do data engineers have the authority to deploy their infrastructure in an AWS \\naccount, or must another team handle these changes? Work with other teams to define \\nstreamlined processes so that teams can work together efficiently and quickly.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 256, 'page_label': '257'}, page_content='|  249 \\nWhom You‚Äôll Work With  \\nThe division of responsibilities for data storage will depend significantly on the \\nmaturity of the organization involved. The data engineer will likely manage the storage \\nsystems and workflow if the company is early in its data maturity. If the company is \\nlater in its data maturity, the data engineer will probably manage a section of the \\nstorage system. This data engineer will also likely interact with engineers on either \\nside of storage‚Äîingestion and transformation. \\nThe data engineer needs to ensure that the storage systems used by downstream users \\nare securely available, contain high -quality data, have ample storage capacity, and \\nperform when queries and transformations are run. \\nUndercurrents \\nThe undercurrents for storage are significant because storage is a critical hub for all \\nstages of the data engineering lifecycle. Unlike other undercurrents for which data \\nmight be in motion (ingestion) or queried and transformed, the undercurrents for \\nstorage differ because storage is so ubiquitous. \\nSecurity \\nWhile engineers often view security as an impediment to their work, they should \\nembrace the idea that security is a key enabler. Robust security at rest and in motion \\nwith fine-grained data access control allows data to be shared and consumed more \\nwidely within a business. The value of data goes up significantly when this is possible. \\nAs always, exercise the principle of least privilege. Don‚Äôt give full database access to \\nanyone unless required. This means most data engineers don‚Äôt need full database \\naccess in practice. Also, pay attention to the column, row, and cell-level access controls \\nin your database. Give users only the information they need and no more. \\nData Management \\nData management is critical as we read and write data with storage systems. \\nData catalogs and metadata management \\nData is enhanced by robust metadata. Cataloging enables data scientists, analysts, and \\nML engineers by enabling data discovery. Data lineage accelerates the time to track \\ndown data problems and allows consumers to locate upstream raw sources. As you \\nbuild out your storage systems, invest in your metadata. Integration of a data dictionary \\nwith these other tools allows users to share and record institutional knowledge robustly.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 257, 'page_label': '258'}, page_content='250  | Chapter 6: Storage \\nMetadata management also significantly enhances data governance. Beyond simply \\nenabling passive data cataloging and lineage, consider implementing analytics over \\nthese systems to get a clear, active picture of what‚Äôs happening with your data. \\nData versioning in object storage \\nMajor cloud object storage systems enable data versioning. Data versioning can help \\nwith error recovery when processes fail, and data becomes corrupted. V ersioning is \\nalso beneficial for tracking the history of datasets used to build models. Just as code \\nversion control allows developers to track down commits that cause bugs, data version \\ncontrol can aid ML engineers in tracking changes that lead to model performance \\ndegradation. \\nPrivacy \\nGDPR and other privacy regulations have significantly impacted storage system \\ndesign. Any data with privacy implications has a lifecycle that data engineers must \\nmanage. Data engineers must be prepared to respond to data deletion requests and \\nselectively remove data as required. In addition, engineers can accommodate privacy \\nand security through anonymization and masking. \\nDataOps \\nDataOps is not orthogonal to data management, and a significant area of overlap exists. \\nDataOps concerns itself with traditional operational monitoring of storage systems and \\nmonitoring the data itself, inseparable from metadata and quality. \\nSystems monitoring \\nData engineers must monitor storage in a variety of ways. This includes monitoring \\ninfrastructure storage components, where they exist, but also monitoring object storage \\nand other ‚Äúserverless‚Äù systems. Data engineers should take the lead on FinOps (cost \\nmanagement), security monitoring, and access monitoring. \\nObserving and monitoring data \\nWhile metadata systems as we‚Äôve described are critical, good engineering must \\nconsider the entropic nature of data by actively seeking to understand its characteristics \\nand watching for major changes. Engineers can monitor data statistics, apply anomaly \\ndetection methods or simple rules, and actively test and validate for logical \\ninconsistencies.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 258, 'page_label': '259'}, page_content='|  251 \\nUndercurrents  \\nData Architecture \\nChapter 3 covers the basics of data architecture, as storage is the critical underbelly of \\nthe data engineering lifecycle. \\nConsider the following data architecture tips. Design for required reliability and \\ndurability. Understand the upstream source systems and how that data, once ingested, \\nwill be stored and accessed. Understand the types of data models and queries that will \\noccur downstream. \\nIf data is expected to grow, can you negotiate storage with your cloud provider? Take \\nan active approach to FinOps, and treat it as a central part of architecture conversations. \\nDon‚Äôt prematurely optimize, but prepare for scale if business opportunities ex ist in \\noperating on large data volumes. \\nLean toward fully managed systems, and understand provider SLAs. Fully managed \\nsystems are generally far more robust and scalable than systems you have to babysit. \\nOrchestration \\nOrchestration is highly entangled with storage. Storage allows data to flow through \\npipelines, and orchestration is the pump. Orchestration also helps engineers cope with \\nthe complexity of data systems, potentially combining many storage systems and query \\nengines. \\nSoftware Engineering \\nWe can think about software engineering in the context of storage in two ways. First, \\nthe code you write should perform well with your storage system. Make sure the code \\nyou write stores the data correctly and doesn‚Äôt accidentally cause data, memory leaks, \\nor performance issues. Second, define your storage infrastructure as code and use \\nephemeral compute resources when it‚Äôs time to process your data. Because storage is \\nincreasingly distinct from compute, you can automatically spin resources up and down \\nwhile keeping your data in object storage. This keeps your infrastructure clean and \\navoids coupling your storage and query layers. \\nConclusion \\nStorage is everywhere and underlays many stages of the data engineering lifecycle. In \\nthis chapter, you learned about the raw ingredients, types, abstractions, and big ideas \\naround storage systems. Gain deep knowledge of the inner workings and limitations of \\nthe storage systems you‚Äôll use. Know the types of data, activities, and workloads \\nappropriate for your storage.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 259, 'page_label': '260'}, page_content='252  | Chapter 6: Storage \\nAdditional Resources \\n‚Ä¢ ‚ÄúColumn-Oriented DBMS‚Äù Wikipedia page \\n‚Ä¢ ‚ÄúThe Design and Implementation of Modern Column-Oriented Database Systems‚Äù \\nby Daniel Abadi et al. \\n‚Ä¢ Designing Data-Intensive Applications by Martin Kleppmann (O‚ÄôReilly) \\n‚Ä¢ ‚ÄúDiving Into Delta Lake: Schema Enforcement and Evolution‚Äù by Burak Yavuz et \\nal. \\n‚Ä¢ ‚ÄúHot Data vs. Cold Data: Why It Matters‚Äù by Afzaal Ahmad Zeeshan \\n‚Ä¢ IDC‚Äôs ‚ÄúData Creation and Replication Will Grow at a Faster Rate than Installed \\nStorage Capacity, According to the IDC Global DataSphere and StorageSphere \\nForecasts‚Äù press release \\n‚Ä¢ ‚ÄúRowise vs. Columnar Database? Theory and in Practice‚Äù by Mangat Rai Modi \\n‚Ä¢ ‚ÄúSnowflake Solution Anti-Patterns: The Probable Data Scientist‚Äù by John Aven \\n‚Ä¢ ‚ÄúWhat Is a Vector Database?‚Äù by Bryan Turriff \\n‚Ä¢ ‚ÄúWhat Is Object Storage? A Definition and Overview‚Äù by Alex Chan \\n‚Ä¢ ‚ÄúThe What, When, Why, and How of Incremental Loads‚Äù by Tim Mitchell'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 260, 'page_label': '261'}, page_content='|  253 \\nAdditional Resources'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 261, 'page_label': '262'}, page_content=''),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 262, 'page_label': '263'}, page_content='255 \\nCHAPTE\\nR 7 \\nInges\\ntion \\nYou‚Äôve learned about the various source systems you‚Äôll likely encounter as a data \\nengineer and about ways to store data. Let‚Äôs now turn our attention to the patterns and \\nchoices that apply to ingesting data from various source systems. In this chapter, we \\ndiscuss data ingestion (see Figure 7 -1), the key engineering considerations for the \\ningestion phase, the major patterns for batch and streaming ingestion, technologies \\nyou‚Äôll encounter, whom you‚Äôll work with as you develop your data ingestion pipeline, \\nand how the undercurrents feature in the ingestion phase.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 263, 'page_label': '264'}, page_content='256  | Chapter 7: Ingestion \\n \\nFigure 7-1. To begin processing data, we must ingest it \\nWhat Is Data Ingestion? \\nData ingestion is the process of moving data from one place to another. Data ingestion \\nimplies data movement from source systems into storage in the data engineering \\nlifecycle, with ingestion as an intermediate step (Figure 7-2). \\n \\nFigure 7-2. Data from system 1 is ingested into system 2 \\nIt‚Äôs worth quickly contrasting data ingestion with data integration. Whereas data \\ningestion is data movement from point A to B, data integration combines data from \\ndisparate sources into a new dataset. For example, you can use data integration to \\ncombine data from a CRM system, advertising analytics data, and web analytics to \\ncreate a user profile, which is saved to your data warehouse. Furthermor e, using \\nreverse ETL, you can send this newly created user profile back to your CRM so \\nsalespeople can use the data for prioritizing leads. We describe data integration more \\nfully in Chapter 8, where we discuss data transformations; reverse ETL is covered in \\nChapter 9. \\nWe also point out that data ingestion is different from internal ingestion within a \\nsystem. Data stored in a database is copied from one table to another, or data in a stream \\nis temporarily cached. We consider this another part of the general data transformation \\nprocess covered in Chapter 8.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 264, 'page_label': '265'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 257 \\nData Pipelines Defined \\nData pipelines begin in source systems, but ingestion is the stage where data engineers \\nbegin actively designing data pipeline activities. In the data engineering space, a good \\ndeal of ceremony occurs around data movement and processing patterns, with \\nestablished patterns such as ETL, newer patterns such as ELT, and new names for long-\\nestablished practices (reverse ETL) and data sharing. \\nAll of these concepts are encompassed in the idea of a data pipeline. It is essential to \\nunderstand the details of these various patterns and know that a modern data pipeline \\nincludes all of them. As the world moves away from a traditional monolithic approach \\nwith rigid constraints on data movement, and toward an open ecos ystem of cloud \\nservices that are assembled like LEGO bricks to realize products, data engineers \\nprioritize using the right tools to accomplish the desired outcome over adhering to a \\nnarrow philosophy of data movement. \\nIn general, here‚Äôs our definition of a data pipeline: \\nA data pipeline is the combination of architecture, systems, and processes that move data \\nthrough the stages of the data engineering lifecycle. \\nOur definition is deliberately fluid‚Äîand intentionally vague‚Äîto allow data engineers \\nto plug in whatever they need to accomplish the task at hand. A data pipeline could be \\na traditional ETL system, where data is ingested from an on -premises transactional \\nsystem, passed through a monolithic processor, and written into a data warehouse. Or \\nit could be a cloud -based data pipeline that pulls data from 100 sources, combines it \\ninto 20 wide tables, trains five other ML models, deploys them into production, and \\nmonitors ongoing performance. A data pipeline should be flexible enough to fit any \\nneeds along the data engineering lifecycle. \\nLet‚Äôs keep this notion of data pipelines in mind as we proceed through this chapter. \\nKey Engineering Considerations for the \\nIngestion Phase \\nWhen preparing to architect or build an ingestion system, here are some primary \\nconsiderations and questions to ask yourself related to data ingestion: \\n‚Ä¢ What‚Äôs the use case for the data I‚Äôm ingesting? \\n‚Ä¢ Can I reuse this data and avoid ingesting multiple versions of the same dataset? \\n‚Ä¢ Where is the data going? What‚Äôs the destination? \\n‚Ä¢ How often should the data be updated from the source? \\n‚Ä¢ What is the expected data volume?'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 265, 'page_label': '266'}, page_content='258  | Chapter 7: Ingestion \\n‚Ä¢ What format is the data in? Can downstream storage and transformation accept \\nthis format? \\n‚Ä¢ Is the source data in good shape for immediate downstream use? That is, is the \\ndata of good quality? What post-processing is required to serve it? What are data-\\nquality risks (e.g., could bot traffic to a website contaminate the data)? \\n‚Ä¢ Does the data require in-flight processing for downstream ingestion if the data is \\nfrom a streaming source? \\nThese questions undercut batch and streaming ingestion and apply to the underlying \\narchitecture you‚Äôll create, build, and maintain. Regardless of how often the data is \\ningested, you‚Äôll want to consider these factors when designing your ingestion \\narchitecture: \\n‚Ä¢ Bounded versus unbounded \\n‚Ä¢ Frequency \\n‚Ä¢ Synchronous versus asynchronous \\n‚Ä¢ Serialization and deserialization \\n‚Ä¢ Throughput and scalability \\n‚Ä¢ Reliability and durability \\n‚Ä¢ Payload \\n‚Ä¢ Push versus pull versus poll patterns Let‚Äôs look at each of these. \\nBounded Versus Unbounded Data \\nAs you might recall from Chapter 3, data comes in two forms: bounded and unbounded \\n(Figure 7-3). Unbounded data is data as it exists in reality, as events happen, either \\nsporadically or continuously, ongoing and flowing. Bounded data is a convenient way \\nof bucketing data across some sort of boundary, such as time.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 266, 'page_label': '267'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 259 \\n \\nFigure 7-3. Bounded versus unbounded data \\nLet us adopt this mantra: All data is unbounded until it‚Äô s bounded. Like many mantras, \\nthis one is not precisely accurate 100% of the time. The grocery list that I scribbled \\nthis afternoon is bounded data. I wrote it as a stream of consciousness (unbounded \\ndata) onto a piece of scrap paper, where the thoughts now exist as  a list of things \\n(bounded data) I need to buy at the grocery store. However, the idea is correct for \\npractical purposes for the vast majority of data you‚Äôll handle in a business context. For \\nexample, an online retailer will process customer transactions 24 hours a day until the \\nbusiness fails, the economy grinds to a halt, or the sun explodes. \\nBusiness processes have long imposed artificial bounds on data by cutting discrete \\nbatches. Keep in mind the true unboundedness of your data; streaming ingestion \\nsystems are simply a tool for preserving the unbounded nature of data so that \\nsubsequent steps in the lifecycle can also process it continuously. \\nFrequency \\nOne of the critical decisions that data engineers must make in designing dataingestion \\nprocesses is the data -ingestion frequency. Ingestion processes can be batch, micro -\\nbatch, or real-time.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 267, 'page_label': '268'}, page_content='260  | Chapter 7: Ingestion \\nIngestion frequencies vary dramatically from slow to fast ( Figure 7-4). On the slow \\nend, a business might ship its tax data to an accounting firm once a year. On the faster \\nside, a CDC system could retrieve new log updates from a source database once a \\nminute. Even faster, a system might continuously ingest events from IoT sensors and \\nprocess these within seconds. Data -ingestion frequencies are often mixed in a \\ncompany, depending on the use case and technologies. \\n \\nFigure 7-4. The spectrum batch to real-time ingestion frequencies \\nWe note that ‚Äúreal -time‚Äù ingestion patterns are becoming increasingly common. We \\nput ‚Äúreal-time‚Äù in quotation marks because no ingestion system is genuinely real-time. \\nAny database, queue or pipeline has inherent latency in delivering data to a target \\nsystem. It is more accurate to speak of near real-time, but we often use real-time for \\nbrevity. The near real -time pattern generally does away with an explicit update \\nfrequency; events are processed in the pipeline either one by one as they arrive or in \\nmicro-batches (i.e., batches over concise time intervals). For this book, we will use \\nreal-time and streaming interchangeably. \\nEven with a streaming data -ingestion process, batch processing downstream is \\nrelatively standard. At the time of this writing, ML models are typically trained on a \\nbatch basis, although continuous online training is becoming more prevalent. Rarely \\ndo data engineers have the option to build a purely near real -time pipeline with no \\nbatch components. Instead, they choose where batch boundaries will occur ‚Äîi.e., the \\ndata engineering lifecycle data will be broken into batches. Once data reaches a batch \\nprocess, the batch frequency becomes a bottleneck for all downstream processing. \\nIn addition, streaming systems are the best fit for many data source types. In IoT \\napplications, the typical pattern is for each sensor to write events or measurements to \\nstreaming systems as they happen. While this data can be written directly into a \\ndatabase, a streaming ingestion platform such as Amazon Kinesis or Apache Kafka is \\na better fit for the application. Software applications can adopt similar patterns by \\nwriting events to a message queue as they happen rather than waiting for an extraction \\nprocess to pull events and state information from a backend database. This pattern \\nworks exceptionally well for event-driven architectures already exchanging messages \\nthrough queues. And again, streaming architectures generally coexist with batch \\nprocessing.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 268, 'page_label': '269'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 261 \\nSynchronous Versus Asynchronous Ingestion \\nWith synchronous ingestion , the source, ingestion, and destination have complex \\ndependencies and are tightly coupled. As you can see in Figure 7-5, each stage of the \\ndata engineering lifecycle has processes A, B, and C directly dependent upon one \\nanother. If process A fails, processes B and C cannot start; if process B fails, process \\nC doesn‚Äôt start. This type of synchronous workflow is common in older ETL systems, \\nwhere data extracted from a source system must then be transformed before being \\nloaded into a data warehouse. Processes downstream of ingestion can‚Äôt start until all \\ndata in the batch has been ingested. If the ingestion or transformation process fails, the \\nentire process must be rerun. \\n \\nFigure 7-5. A synchronous ingestion process runs as discrete batch steps \\nHere‚Äôs a mini case study of how not to design your data pipelines. At one company, the \\ntransformation process itself was a series of dozens of tightly coupled synchronous \\nworkflows, with the entire process taking over 24 hours to finish. If any step of that \\ntransformation pipeline failed, t he whole transformation process had to be restarted \\nfrom the beginning! In this instance, we saw process after process fail, and because of \\nnonexistent or cryptic error messages, fixing the pipeline was a game of whack-a-mole \\nthat took over a week to diagnose and cure. Meanwhile, the business didn‚Äôt have \\nupdated reports during that time. People weren‚Äôt happy. \\nWith asynchronous ingestion, dependencies can now operate at the level of individual \\nevents, much as they would in a software backend built from microservices (Figure 7-\\n6). Individual events become available in storage as soon as they are ingested \\nindividually. Take the example of a web application on AWS that emits events into \\nAmazon Kinesis Data Streams (here acting as a buffer). The stream is read by Apache \\nBeam, which parses and enriches events, and then forwards them to a second Kinesis \\nstream; Kinesis Data Firehose rolls up events and writes objects to Amazon S3. \\n \\nFigure 7-6. Asynchronous processing of an event stream in AWS'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 269, 'page_label': '270'}, page_content='262  | Chapter 7: Ingestion \\nThe big idea is that rather than relying on asynchronous processing, where a batch \\nprocess runs for each stage as the input batch closes and certain time conditions are \\nmet, each stage of the asynchronous pipeline can process data items as they become \\navailable in parallel across the Beam cluster. The processing rate depends on available \\nresources. The Kinesis Data Stream acts as the shock absorber, moderating the load so \\nthat event rate spikes will not overwhelm downstream processing. Events will move \\nthrough the pipeline quickly when the event rate is low, and any backlog has cleared. \\nNote that we could modify the scenario and use a Kinesis Data Stream for storage, \\neventually extracting events to S3 before they expire out of the stream. \\nSerialization and Deserialization \\nMoving data from source to destination involves serialization and deserialization. As a \\nreminder, serialization means encoding the data from a source and preparing data \\nstructures for transmission and intermediate storage stages. \\nWhen ingesting data, ensure that your destination can deserialize the data it receives. \\nWe‚Äôve seen data ingested from a source but then sitting inert and unusable in the \\ndestination because the data cannot be properly deserialized. See the more extensive \\ndiscussion of serialization in Appendix A. \\nThroughput and Scalability \\nIn theory, your ingestion should never be a bottleneck. In practice, ingestion \\nbottlenecks are pretty standard. Data throughput and system scalability become critical \\nas your data volumes grow and requirements change. Design your systems to scale and \\nshrink to flexibly match the desired data throughput. \\nWhere you‚Äôre ingesting data from matters a lot. If you‚Äôre receiving data as it‚Äôs \\ngenerated, will the upstream system have any issues that might impact your \\ndownstream ingestion pipelines? For example, suppose a source database goes down. \\nWhen it comes back  online and attempts to backfill the lapsed data loads, will your \\ningestion be able to keep up with this sudden influx of backlogged data? \\nAnother thing to consider is your ability to handle bursty data ingestion. Data \\ngeneration rarely happens at a constant rate and often ebbs and flows. Built -in \\nbuffering is required to collect events during rate spikes to prevent data from getting \\nlost. Buffering bridges the time while the system scales and allows storage systems to \\naccommodate bursts even in a dynamically scalable system. \\nWhenever possible, use managed services that handle the throughput scaling for you. \\nWhile you can manually accomplish these tasks by adding more servers, shards, or \\nworkers, often this isn‚Äôt value -added work, and there‚Äôs a good chance you‚Äôll miss \\nsomething. Much of this heavy lifting is now automated. Don‚Äôt reinvent the data \\ningestion wheel if you don‚Äôt have to.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 270, 'page_label': '271'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 263 \\nReliability and Durability \\nReliability and durability are vital in the ingestion stages of data pipelines. Reliability \\nentails high uptime and proper failover for ingestion systems. Durability entails \\nmaking sure that data isn‚Äôt lost or corrupted. \\nSome data sources (e.g., IoT devices and caches) may not retain data if it is not \\ncorrectly ingested. Once lost, it is gone for good. In this sense, the reliability of \\ningestion systems leads directly to the durability of generated data. If data is ingested, \\ndownstream processes can theoretically run late if they break temporarily. \\nOur advice is to evaluate the risks and build an appropriate level of redundancy and \\nself-healing based on the impact and cost of losing data. Reliability and durability have \\nboth direct and indirect costs. For example, will your ingestion process continue if an \\nAWS zone goes down? How about a whole region? How about the power grid or the \\ninternet? Of course, nothing is free. How much will this cost you? You might be able \\nto build a highly redundant system and have a team on call 24 hours a day to handle \\noutages. This also means your cloud and labor costs become prohibitive (direct costs), \\nand the ongoing work takes a significant toll on your team (indirect costs). There‚Äôs no \\nsingle correct answer, and you need to evaluate the costs and benefits of your reliability \\nand durability decisions. \\nDon‚Äôt assume that you can build a system that will reliably and durably ingest data in \\nevery possible scenario. Even the nearly infinite budget of the US federal government \\ncan‚Äôt guarantee this. In many extreme scenarios, ingesting data actually won‚Äôt matter. \\nThere will be little to ingest if the internet goes down, even if you build multiple \\nairgapped data centers in underground bunkers with independent power. Continually \\nevaluate the trade-offs and costs of reliability and durability. \\nPayload \\nA payload is the dataset you‚Äôre ingesting and has characteristics such as kind, shape, \\nsize, schema and data types, and metadata. Let‚Äôs look at some of these characteristics \\nto understand why this matters. \\nKind \\nThe kind of data you handle directly impacts how it‚Äôs dealt with downstream in the \\ndata engineering lifecycle. Kind consists of type and format. Data has a type‚Äîtabular, \\nimage, video, text, etc. The type directly influences the data format or the way it is \\nexpressed in bytes, names, and file extensions. For example, a tabular kind of data may \\nbe in formats such as CSV or Parquet, with each of these formats having different byte \\npatterns for serialization and deserialization. Another kind of data is an image, which \\nhas a format of JPG or PNG and is inherently unstructured.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 271, 'page_label': '272'}, page_content='264  | Chapter 7: Ingestion \\nShape \\nEvery payload has a shape that describes its dimensions. Data shape is critical across \\nthe data engineering lifecycle. For instance, an image‚Äôs pixel and red, green, blue \\n(RGB) dimensions are necessary for training deep learning models. As another \\nexample, if you‚Äôre trying to import a CSV file into a database table, and your CSV has \\nmore columns than the database table, you‚Äôll likely get an error during the import \\nprocess. Here are some examples of the shapes of various kinds of data: \\nTabular \\nThe number of rows and columns in the dataset, commonly expressed as M rows \\nand N columns \\nSemistructured JSON \\nThe key-value pairs and nesting depth occur with subelements \\nUnstructured text \\nNumber of words, characters, or bytes in the text body \\nImages \\nThe width, height, and RGB color depth (e.g., 8 bits per pixel) \\nUncompressed audio \\nNumber of channels (e.g., two for stereo), sample depth (e.g., 16 bits per sample), \\nsample rate (e.g., 48 kHz), and length (e.g., 10,003 seconds) \\nSize \\nThe size of the data describes the number of bytes of a payload. A payload may range \\nin size from single bytes to terabytes and larger. To reduce the size of a payload, it may \\nbe compressed into various formats such as ZIP and TAR (see the discussion of \\ncompression in Appendix A). \\nA massive payload can also be split into chunks, which effectively reduces the size of \\nthe payload into smaller subsections. When loading a huge file into a cloud object \\nstorage or data warehouse, this is a common practice as the small individual files are  \\neasier to transmit over a network (especially if they‚Äôre compressed). The smaller \\nchunked files are sent to their destination and then reassembled after all data has \\narrived. \\nSchema and data types \\nMany data payloads have a schema, such as tabular and semistructured data. As \\nmentioned earlier in this book, a schema describes the fields and types of data within \\nthose fields. Other data, such as unstructured text, images, and audio, will not have an'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 272, 'page_label': '273'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 265 \\nexplicit schema or data types. However, they might come with technical file \\ndescriptions on shape, data and file format, encoding, size, etc. \\nAlthough you can connect to databases in various ways (such as file export, CDC, \\nJDBC/ODBC), the connection is easy. The great engineering challenge is \\nunderstanding the underlying schema. Applications organize data in various ways, and \\nengineers need to be intimately familiar with the organization of the data and relevant \\nupdate patterns to make sense of it. The problem has been somewhat exacerbated by \\nthe popularity of object -relational mapping (ORM), which automatically generates \\nschemas based on object structure in languages such as Java or Python. Natural \\nstructures in an object -oriented language often map to something messy in an \\noperational database. Data engineers may need to familiarize themselves with the class \\nstructure of application code. \\nSchema is not only for databases. As we‚Äôve discussed, APIs present their schema \\ncomplications. Many vendor APIs have friendly reporting methods that prepare data \\nfor analytics. In other cases, engineers are not so lucky. The API is a thin wrapper \\naround underlying systems, requiring engineers to understand application internals to \\nuse the data. \\nMuch of the work associated with ingesting from source schemas happens in the data \\nengineering lifecycle transformation stage, which we discuss in Chapter 8 . We‚Äôve \\nplaced this discussion here because data engineers need to begin studying source \\nschemas as soon they plan to ingest data from a new source. \\nCommunication is critical for understanding source data, and engineers also have the \\nopportunity to reverse the flow of communication and help software engineers improve \\ndata where it is produced. Later in this chapter, we‚Äôll return to this topic in ‚ÄúWhom \\nYou‚Äôll Work With‚Äù on page 262. \\nDetecting and handling upstream and downstream schema changes.    \\nChanges in schema frequently occur in source systems and are often well out of data \\nengineers‚Äô control. Examples of schema changes include the following: \\n‚Ä¢ Adding a new column \\n‚Ä¢ Changing a column type \\n‚Ä¢ Creating a new table \\n‚Ä¢ Renaming a column \\nIt‚Äôs becoming increasingly common for ingestion tools to automate the detection of \\nschema changes and even auto-update target tables. Ultimately, this is something of a \\nmixed blessing. Schema changes can still break pipelines downstream of staging and \\ningestion.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 273, 'page_label': '274'}, page_content='266  | Chapter 7: Ingestion \\nEngineers must still implement strategies to respond to changes automatically and alert \\non changes that cannot be accommodated automatically. Automation is excellent, but \\nthe analysts and data scientists who rely on this data should be informed of the schema \\nchanges that violate existing assumptions. Even if automation can accommodate a \\nchange, the new schema may adversely affect the performance of reports and models. \\nCommunication between those making schema changes and those impacted by these \\nchanges is as important as reliable automation that checks for schema changes. \\nSchema registries.    In streaming data, every message has a schema, and these \\nschemas may evolve between producers and consumers. A schema registry  is a \\nmetadata repository used to maintain schema and data type integrity in the face of \\nconstantly changing schemas. Schema registries can also track schema versions and \\nhistory. It describes the data model for messages, allowing consistent serialization and \\ndeserialization between producers and consumers. Schema registries are used in most \\nmajor data tools and clouds. \\nMetadata \\nIn addition to the apparent characteristics we‚Äôve just covered, a payload often contains \\nmetadata, which we first discussed in Chapter 2. Metadata is data about data. Metadata \\ncan be as critical as the data itself. One of the significant limitations of the early \\napproach to the data lake‚Äîor data swamp, which could become a data superfund site‚Äî\\nwas a complete lack of attention to metadata. Without a detailed description of the data, \\nit may be of little value. We‚Äôve already discussed some types of metadata (e.g., schema) \\nand will address them many times throughout this chapter. \\nPush Versus Pull Versus Poll Patterns \\nWe mentioned push versus pull when we introduced the data engineering lifecycle in \\nChapter 2. A push strategy (Figure 7-7) involves a source system sending data to a \\ntarget, while a pull strategy (Figure 7-8) entails a target reading data directly from a \\nsource. As we mentioned in that discussion, the lines between these strategies are \\nblurry.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 274, 'page_label': '275'}, page_content='Key Engineering Considerations for the Ingestion Phase  | \\n 267 \\n \\nFigure 7-8. A destination pulling data from a source \\nAnother pattern related to pulling is polling for data ( Figure 7-9). Polling involves \\nperiodically checking a data source for any changes. When changes are detected, the \\ndestination pulls the data as it would in a regular pull situation. \\n \\nFigure 7-9. Polling for changes in a source system \\nBatch Ingestion Considerations \\nBatch ingestion, which involves processing data in bulk, is often a convenient way to \\ningest data. This means that data is ingested by taking a subset of data from a source \\nsystem, based either on a time interval or the size of accumulated data (Figure 7-10).'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 275, 'page_label': '276'}, page_content='268  | Chapter 7: Ingestion \\n \\nFigure 7-10. Time-interval batch ingestion \\nTime-interval batch ingestion  is widespread in traditional business ETL for data \\nwarehousing. This pattern is often used to process data once a day, overnight during \\noff-hours, to provide daily reporting, but other frequencies can also be used. Size-based \\nbatch ingestion (Figure 7-11) is quite common when data is moved from a streaming-\\nbased system into object storage; ultimately, you must cut the data into discrete blocks \\nfor future processing in a data lake. Some size-based ingestion systems can break data \\ninto objects based on various criteria, such as the size in bytes of the total number of \\nevents. \\n \\nFigure 7-11. Size-based batch ingestion \\nSome commonly used batch ingestion patterns, which we discuss in this section, \\ninclude the following: \\n‚Ä¢ Snapshot or differential extraction \\n‚Ä¢ File-based export and ingestion \\n‚Ä¢ ETL versus ELT \\n‚Ä¢ Inserts, updates, and batch size \\n‚Ä¢ Data migration'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 276, 'page_label': '277'}, page_content='|  269 \\nBatch Ingestion Considerations  \\nSnapshot or Differential Extraction \\nData engineers must choose whether to capture full snapshots of a source system or \\ndifferential (sometimes called incremental) updates. With full snapshots, engineers \\ngrab the entire current state of the source system on each update read. With the \\ndifferential update pattern, engineers can pull only the updates and changes since the \\nlast read from the source system. While differential updates are ideal for minimizing \\nnetwork traffic and target storage usage, full snapshot reads remain extremely common \\nbecause of their simplicity. \\nFile-Based Export and Ingestion \\nData is quite often moved between databases and systems using files. Data is serialized \\ninto files in an exchangeable format, and these files are provided to an ingestion \\nsystem. We consider file -based export to be a push-based ingestion pattern. This is \\nbecause data export and preparation work is done on the source system side. \\nFile-based ingestion has several potential advantages over a direct database connection \\napproach. It is often undesirable to allow direct access to backend systems for security \\nreasons. With file-based ingestion, export processes are run on the data -source side, \\ngiving source system engineers complete control over what data gets exported and how \\nthe data is preprocessed. Once files are done, they can be provided to the target system \\nin various ways. Common file -exchange methods are object storage, secure fi le \\ntransfer protocol (SFTP), electronic data interchange (EDI), \\nor secure copy (SCP). \\nETL Versus ELT \\nChapter 3 introduced ETL and ELT, both extremely common ingestion, storage, and \\ntransformation patterns you‚Äôll encounter in batch workloads. The following are brief \\ndefinitions of the extract and load parts of ETL and ELT: \\nExtract \\nThis means getting data from a source system. While extract seems to imply \\npulling data, it can also be push based. Extraction may also require reading \\nmetadata and schema changes. \\nLoad \\nOnce data is extracted, it can either be transformed (ETL) before loading it into a \\nstorage destination or simply loaded into storage for future transformation. When \\nloading data, you should be mindful of the type of system you‚Äôre loading, the \\nschema of the data, and the performance impact of loading. \\nWe cover ETL and ELT in greater detail in Chapter 8.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 277, 'page_label': '278'}, page_content='270  | Chapter 7: Ingestion \\nInserts, Updates, and Batch Size \\nBatch-oriented systems often perform poorly when users attempt to perform many \\nsmall-batch operations rather than a smaller number of large operations. For example, \\nwhile it is common to insert one row at a time in a transactional database, this is a bad \\npattern for many columnar databases, as it forces the creation of many small, \\nsuboptimal files and forces the system to run a high number of create object operations. \\nRunning many small in-place update operations is an even bigger problem because it \\ncauses the database to scan each existing column file to run the update. \\nUnderstand the appropriate update patterns for the database or data store you‚Äôre \\nworking with. Also, understand that certain technologies are purpose -built for high \\ninsert rates. For example, Apache Druid and Apache Pinot can handle high insert rates. \\nSingleStore can manage hybrid workloads that combine OLAP and OLTP \\ncharacteristics. BigQuery performs poorly on a high rate of vanilla SQL single -row \\ninserts but extremely well if data is fed in through its stream buffer. Know the limits \\nand characteristics of your tools. \\nData Migration \\nMigrating data to a new database or environment is not usually trivial, and data needs \\nto be moved in bulk. Sometimes this means moving data sizes that are hundreds of \\nterabytes or much larger, often involving the migration of specific tables and moving \\nentire databases and systems. \\nData migrations probably aren‚Äôt a regular occurrence as a data engineer, but you should \\nbe familiar with them. As is often the case for data ingestion, schema management is \\na crucial consideration. Suppose you‚Äôre migrating data from one database system to a \\ndifferent one (say, SQL Server to Snowflake). No matter how closely the two databases \\nresemble each other, subtle differences almost always exist in the way they handle \\nschema. Fortunately, it is generally easy to test ingestion of a sample of data and find \\nschema issues before undertaking a complete table migration. \\nMost data systems perform best when data is moved in bulk rather than as individual \\nrows or events. File or object storage is often an excellent intermediate stage for \\ntransferring data. Also, one of the biggest challenges of database migration is not the \\nmovement of the data itself but the movement of data pipeline connections from the \\nold system to the new one. \\nBe aware that many tools are available to automate various types of data migrations. \\nEspecially for large and complex migrations, we suggest looking at these options \\nbefore doing this manually or writing your own migration solution.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 278, 'page_label': '279'}, page_content='|  271 \\nBatch Ingestion Considerations  \\nMessage and Stream Ingestion \\nConsiderations \\nIngesting event data is common. This section covers issues you should consider when \\ningesting events, drawing on topics covered in Chapters 5 and 6. \\nSchema Evolution \\nSchema evolution is common when handling event data; fields may be added or \\nremoved, or value types might change (say, a string to an integer). Schema evolution \\ncan have unintended impacts on your data pipelines and destinations. For example, an \\nIoT device gets a firmware update that adds a new field to the event it transmits, or a \\nthird-party API introduces changes to its event payload or countless other scenarios. \\nAll of these potentially impact your downstream capabilities. \\nTo alleviate issues related to schema evolution, here are a few suggestions. First, if \\nyour event -processing framework has a schema registry (discussed earlier in this \\nchapter), use it to version your schema changes. Next, a dead -letter queue (described \\nin ‚ÄúError Handling and Dead -Letter Queues‚Äù on page 249 ) can help you investigate \\nissues with events that are not properly handled. Finally, the low-fidelity route (and the \\nmost effective) is regularly communicating with upstream stakeholders about potential \\nschema changes and proactively addressing schema changes with the teams \\nintroducing these changes instead of reacting to the receiving end of breaking changes. \\nLate-Arriving Data \\nThough you probably prefer all event data to arrive on time, event data might arrive \\nlate. A group of events might occur around the same time frame (similar event times), \\nbut some might arrive later than others (late ingestion times) because of various \\ncircumstances. \\nFor example, an IoT device might be late sending a message because of internet latency \\nissues. This is common when ingesting data. You should be aware of latearriving data \\nand the impact on downstream systems and uses. Suppose you assume that ingestion \\nor process time is the same as the event time. You may get some strange results if your \\nreports or analysis depend on an accurate portrayal of when events occur. To handle \\nlate-arriving data, you need to set a cutoff time for when late -arriving data will no \\nlonger be processed. \\nOrdering and Multiple Delivery'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 279, 'page_label': '280'}, page_content='272  | Chapter 7: Ingestion \\nStreaming platforms are generally built out of distributed systems, which can cause \\nsome complications. Specifically, messages may be delivered out of order and more \\nthan once (at -least-once delivery). See the event -streaming platforms discussion in \\nChapter 5 for more details. \\nReplay \\nReplay allows readers to request a range of messages from the history, allowing you to \\nrewind your event history to a particular point in time. Replay is a key capability in \\nmany streaming ingestion platforms and is particularly useful when you need to \\nreingest and reprocess data for a specific time range. For example, RabbitMQ typically \\ndeletes messages after all subscribers consume them. Kafka, Kinesis, and Pub/Sub all \\nsupport event retention and replay. \\nTime to Live \\nHow long will you preserve your event record? A key parameter is maximum message \\nretention time, also known as the time to live (TTL). TTL is usually a configuration \\nyou‚Äôll set for how long you want events to live before they are acknowledged and \\ningested. Any unacknowledged event that‚Äôs not ingested after its TTL expires \\nautomatically disappears. This is helpful to reduce backpressure and unnecessary event \\nvolume in your event-ingestion pipeline. \\nFind the right balance of TTL impact on our data pipeline. An extremely short TTL \\n(milliseconds or seconds) might cause most messages to disappear before processing. \\nA very long TTL (several weeks or months) will create a backlog of many unprocessed \\nmessages, resulting in long wait times. \\nLet‚Äôs look at how some popular platforms handle TTL at the time of this writing. \\nGoogle Cloud Pub/Sub supports retention periods of up to 7 days. Amazon Kinesis \\nData Streams retention can be turned up to 365 days. Kafka can be configured for \\nindefinite retention, limited by available disk space. (Kafka also supports the option to \\nwrite older messages to cloud object storage, unlocking virtually unlimited storage \\nspace and retention.) \\nMessage Size \\nMessage size is an easily overlooked issue: you must ensure that the streaming \\nframework in question can handle the maximum expected message size. Amazon \\nKinesis supports a maximum message size of 1 MB. Kafka defaults to this maximum \\nsize but can be config ured for a maximum of 20 MB or more. (Configurability may \\nvary on managed service platforms.)'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 280, 'page_label': '281'}, page_content='|  273 \\nError Handling and Dead-Letter Queues \\nSometimes events aren‚Äôt successfully ingested. Perhaps an event is sent to a \\nnonexistent topic or message queue, the message size may be too large, or the event \\nhas expired past its TTL. Events that cannot be ingested need to be rerouted and stored \\nin a separate location called a dead-letter queue. \\nMessage and Stream Ingestion Considerations  \\nA dead-letter queue segregates problematic events from events that can be accepted by \\nthe consumer ( Figure 7-12). If events are not rerouted to a dead -letter queue, these \\nerroneous events risk blocking other messages from being ingested. Data engineers \\ncan use a dead -letter queue to diagnose why event ingestions errors occur and solve \\ndata pipeline problems, and mi ght be able to reprocess some messages in the queue \\nafter fixing the underlying cause of errors. \\n \\nFigure 7-12. ‚ÄúGood‚Äù events are passed to the consumer, whereas ‚Äúbad‚Äù events are \\nstored in a dead-letter queue \\nConsumer Pull and Push \\nA consumer subscribing to a topic can get events in two ways: push and pull. Let‚Äôs \\nlook at the ways some streaming technologies pull and push data. Kafka and Kinesis \\nsupport only pull subscriptions. Subscribers read messages from a topic and confirm \\nwhen t hey have been processed. In addition to pull subscriptions, Pub/Sub and \\nRabbitMQ support push subscriptions, allowing these services to write messages to a \\nlistener. \\nPull subscriptions are the default choice for most data engineering applications, but \\nyou may want to consider push capabilities for specialized applications. Note that pull-\\nonly message ingestion systems can still push if you add an extra layer to handle this. \\nLocation \\nIt is often desirable to integrate streaming across several locations for enhanced \\nredundancy and to consume data close to where it is generated. As a general rule, the'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 281, 'page_label': '282'}, page_content='274  | Chapter 7: Ingestion \\ncloser your ingestion is to where data originates, the better your bandwidth and latency. \\nHowever, you need to balance this against the costs of moving data between regions \\nto run analytics on a combined dataset. As always, data egress costs can spiral quickly. \\nDo a careful evaluation of the trade-offs as you build out your architecture. \\nWays to Ingest Data \\nNow that we‚Äôve described some of the significant patterns underlying batch and \\nstreaming ingestion, let‚Äôs focus on ways you can ingest data. Although we will cite'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 282, 'page_label': '283'}, page_content='some common ways, keep in mind that the universe of data ingestion practices and \\ntechnologies is vast and growing daily. \\nDirect Database Connection \\nData can be pulled from databases for ingestion by querying and reading over a \\nnetwork connection. Most commonly, this connection is made using ODBC or JDBC. \\nODBC uses a driver hosted by a client accessing the database to translate commands \\nissued to the standard ODBC API into commands issued to the database. The database \\nreturns query results over the wire, where the driver receives them and translates them \\nback into a standard form to be read by the client. For ingestion, the application \\nutilizing the ODBC driver is an ingestion tool. The ingestion tool may pull data through \\nmany small queries or a single large query. \\nJDBC is conceptually remarkably similar to ODBC. A Java driver connects to a remote \\ndatabase and serves as a translation layer between the standard JDBC API and the \\nnative network interface of the target database. It might seem strange to have a database \\nAPI dedicated to a single programming language, but there are strong motivations for \\nthis. The Java Virtual Machine (JVM) is standard, portable across hardware \\narchitectures and operating systems, and provides the performance of compiled code \\nthrough a just-in-time (JIT) compiler. The JVM is an extremely popular compiling VM \\nfor running code in a portable manner. \\nJDBC provides extraordinary database driver portability. ODBC drivers are shipped as \\nOS and architecture native binaries; database vendors must maintain versions for each \\narchitecture/OS version that they wish to support. On the other hand, vendors can ship \\na single JDBC driver that is compatible with any JVM language (e.g., Java, Scala, \\nClojure, or Kotlin) and JVM data framework (i.e., Spark.) JDBC has become so \\npopular that it is also used as an interface for non-JVM languages such as Python; the \\nPython ecosystem provides translation tools that allow Python code to talk to a JDBC \\ndriver running on a local JVM. \\nJDBC and ODBC are used extensively for data ingestion from relational databases, \\nreturning to the general concept of direct database connections. Various enhancements \\nare used to accelerate data ingestion. Many data frameworks can parallelize several \\nsimultaneous connections and partition queries to pull data in parallel. On the other \\nhand, nothing is free; using parallel connections also increases the load on the source \\ndatabase. \\nJDBC and ODBC were long the gold standards for data ingestion from databases, but \\nthese connection standards are beginning to show their age for many data engineering \\napplications. These connection standards struggle with nested data, and they send data'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 283, 'page_label': '284'}, page_content='as rows. This means that native nested data must be reencoded as string data to be sent \\nover the wire, and columns from columnar databases must be reserialized as rows. \\nAs discussed in ‚ÄúFile-Based Export and Ingestion‚Äù on page 246, many databases now \\nsupport native file export that bypasses JDBC/ODBC and exports data directly in \\nformats such as Parquet, ORC, and Avro. Alternatively, many cloud data warehouses \\nprovide direct REST APIs. \\nJDBC connections should generally be integrated with other ingestion technologies. \\nFor example, we commonly use a reader process to connect to a database with JDBC, \\nwrite the extracted data into multiple objects, and then orchestrate ingestion into a \\ndownstream system (see Figure 7 -13). The reader process can run in a wholly \\nephemeral cloud instance or in an orchestration system. \\n \\nFigure 7-13. An ingestion process reads from a source database using JDBC, and \\nthen writes objects into object storage. A target database (not shown) can be \\ntriggered to ingest the data with an API call from an orchestration system. \\nChange Data Capture \\nChange data capture  (CDC), introduced in Chapter 2 , is the process of ingesting \\nchanges from a source database system. For example, we might have a source \\nPostgreSQL system that supports an application and periodically or continuously \\ningests table changes for analytics. \\nNote that our discussion here is by no means exhaustive. We introduce you to common \\npatterns but suggest that you read the documentation on a particular database to handle \\nthe details of CDC strategies. \\nBatch-oriented CDC \\nIf the database table in question has an updated_at field containing the last time a \\nrecord was written or updated, we can query the table to find all updated rows since a \\nspecified time. We set the filter timestamp based on when we last captured changed \\nrows from the tables. This process allows us to pull changes and differentially update \\na target table. \\nThis form of batch-oriented CDC has a key limitation: while we can easily determine \\nwhich rows have changed since a point in time, we don‚Äôt necessarily obtain all changes \\nthat were applied to these rows. Consider the example of running batch CDC on a bank \\naccount table every 24 hours. This operational table shows the current account balance'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 284, 'page_label': '285'}, page_content='for each account. When money is moved in and out of accounts, the banking \\napplication runs a transaction to update the balance. \\nWhen we run a query to return all rows in the account table that changed in the last 24 \\nhours, we‚Äôll see records for each account that recorded a transaction. Suppose that a \\ncertain customer withdrew money five times using a debit card in the last 24 hours . \\nOur query will return only the last account balance recorded in the 24 hour period; \\nother records over the period won‚Äôt appear. This issue can be mitigated by utilizing an \\ninsert-only schema, where each account transaction is recorded as a new record in the \\ntable (see ‚ÄúInsert-Only‚Äù on page 162). \\nContinuous CDC \\nContinuous CDC  captures all table history and can support near real -time data \\ningestion, either for real -time database replication or to feed real -time streaming \\nanalytics. Rather than running periodic queries to get a batch of table changes, \\ncontinuous CDC treats each write to the database as an event. \\nWe can capture an event stream for continuous CDC in a couple of ways. One of the \\nmost common approaches with a transactional database such as PostgreSQL is log-\\nbased CDC. The database binary log records every change to the database sequentially \\n(see ‚ÄúDatabase Logs‚Äù on page 161). A CDC tool can read this log and send the events \\nto a target, such as the Apache Kafka Debezium streaming platform. \\nSome databases support a simplified, managed CDC paradigm. For instance, many \\ncloud-hosted databases can be configured to directly trigger a serverless function or \\nwrite to an event stream every time a change happens in the database. This completely \\nfrees engineers from worrying about the details of how events are captured in the \\ndatabase and forwarded. \\nCDC and database replication \\nCDC can be used to replicate between databases: events are buffered into a stream and \\nasynchronously written into a second database. However, many databases natively \\nsupport a tightly coupled version of replication (synchronous replication) that keeps \\nthe replica fully in sync with the primary database. Synchronous replication typically \\nrequires that the  primary database and the replica are of the same type (e.g., \\nPostgreSQL to PostgreSQL). The advantage of synchronous replication is that the \\nsecondary database can offload work from the primary database by acting as a read \\nreplica; read queries can be redirected to the replica. The query will return the same \\nresults that would be returned from the primary database. \\nRead replicas are often used in batch data ingestion patterns to allow large scans to run \\nwithout overloading the primary production database. In addition, an application can'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 285, 'page_label': '286'}, page_content='be configured to fail over to the replica if the primary database becomes unavailable. \\nNo data will be lost in the failover because the replica is entirely in sync with the \\nprimary database. \\nThe advantage of asynchronous CDC replication is a loosely coupled architecture \\npattern. While the replica might be slightly delayed from the primary database, this is \\noften not a problem for analytics applications, and events can now be directed to a \\nvariety of targets; we might run CDC replication while simultaneously directing events \\nto object storage and a streaming analytics processor. \\nCDC considerations \\nLike anything in technology, CDC is not free. CDC consumes various database \\nresources, such as memory, disk bandwidth, storage, CPU time, and network \\nbandwidth. Engineers should work with production teams and run tests before turning \\non CDC on production systems to avoid operational problems. Similar considerations \\napply to synchronous replication. \\nFor batch CDC, be aware that running any large batch query against a transactional \\nproduction system can cause excessive load. Either run such queries only at off-hours \\nor use a read replica to avoid burdening the primary database. \\nAPIs \\nThe bulk of software engineering is just plumbing. \\n‚ÄîKarl Hughes74 \\nAs we mentioned in Chapter 5 , APIs are a data source that continues to grow in \\nimportance and popularity. A typical organization may have hundreds of external data \\nsources such as SaaS platforms or partner companies. The hard reality is that no proper \\nstandard exists for data exchang e over APIs. Data engineers can spend a significant \\namount of time reading documentation, communicating with external data owners, and \\nwriting and maintaining API connection code. \\nThree trends are slowly changing this situation. First, many vendors provide API client \\nlibraries for various programming languages that remove much of the complexity of \\nAPI access. \\nSecond, numerous data connector platforms are available now as SaaS, open source, \\nor managed open source. These platforms provide turnkey data connectivity to many \\n \\n74 Karl Hughes, ‚ÄúThe Bulk of Software Engineering Is Just Plumbing,‚Äù Karl Hughes website, July 8, 2018, \\nhttps://oreil.ly/uIuqJ.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 286, 'page_label': '287'}, page_content='data sources; they offer frameworks for writing custom connectors for unsupported \\ndata sources. See ‚ÄúManaged Data Connectors‚Äù on page 256. \\nThe third trend is the emergence of data sharing (discussed in Chapter 5)‚Äîi.e., the \\nability to exchange data through a standard platform such as BigQuery, Snowflake, \\nRedshift, or S3. Once data lands on one of these platforms, it is straightforward to store \\nit, process it, or move it somewhere else. Data sharing has had a large and rapid impact \\nin the data engineering space. \\nDon‚Äôt reinvent the wheel when data sharing is not an option and direct API access is \\nnecessary. While a managed service might look like an expensive option, consider the \\nvalue of your time and the opportunity cost of building API connectors when you could \\nbe spending your time on higher-value work. \\nIn addition, many managed services now support building custom API connectors. This \\nmay provide API technical specifications in a standard format or writing connector \\ncode that runs in a serverless function framework (e.g., AWS Lambda) while letting \\nthe managed service handle the details of scheduling and synchronization. Again, these \\nservices can be a huge time -saver for engineers, both for development and ongoing \\nmaintenance. \\nReserve your custom connection work for APIs that aren‚Äôt well supported by existing \\nframeworks; you will find that there are still plenty of these to work on. Handling \\ncustom API connections has two main aspects: software development and ops. Follow \\nsoftware development best practices; you should use version control, continuous \\ndelivery, and automated testing. In addition to following DevOps best practices, \\nconsider an orchestration framework, which can dramatically streamline the \\noperational burden of data ingestion. \\nMessage Queues and Event-Streaming Platforms \\nMessage queues and event-streaming platforms are widespread ways to ingest realtime \\ndata from web and mobile applications, IoT sensors, and smart devices. As real -time \\ndata becomes more ubiquitous, you‚Äôll often find yourself either introducing or \\nretrofitting ways to handle real -time data in your ingestion workflows. As such, it‚Äôs \\nessential to know how to ingest real -time data. Popular real -time data ingestion \\nincludes message queues or event-streaming platforms, which we covered in Chapter \\n5. Though these are both source systems, they also act as ways to ingest data. In both \\ncases, you consume events from the publisher you subscribe to. \\nRecall the differences between messages and streams. A message is handled at the \\nindividual event level and is meant to be transient. Once a message is consumed, it is \\nacknowledged and removed from the queue. On the other hand, a stream ingests events \\ninto an ordered log. The log persists for as long as you wish, allowing events to be'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 287, 'page_label': '288'}, page_content='queried over various ranges, aggregated, and combined with other streams to create \\nnew transformations published to downstream consumers. In Figure 7-14, we have two \\nproducers (producers 1 and 2) sending events to two consumers (consumers 1 and 2). \\nThese events are combined into a new dataset and sent to a producer for downstream \\nconsumption. \\n \\nFigure 7-14. Two datasets are produced and consumed (producers 1 and 2) and then \\ncombined, with the combined data published to a new producer (producer 3) \\nThe last point is an essential difference between batch and streaming ingestion. \\nWhereas batch usually involves static workflows (ingest data, store it, transform it, and \\nserve it), messages and streams are fluid. Ingestion can be nonlinear, with data bein g \\npublished, consumed, republished, and reconsumed. When designing your real -time \\ningestion workflows, keep in mind how data will flow. \\nAnother consideration is the throughput of your real-time data pipelines. Messages and \\nevents should flow with as little latency as possible, meaning you should provision \\nadequate partition (or shard) bandwidth and throughput. Provide sufficient memory, \\ndisk, and CPU resources for event processing, and if you‚Äôre managing your real -time \\npipelines, incorporate autoscaling to handle spikes and save money as load decreases. \\nFor these reasons, managing your streaming platform can entail significant overhead. \\nConsider managed services for your real -time ingestion pipelines, and focus your \\nattention on ways to get value from your real-time data. \\nManaged Data Connectors \\nThese days, if you‚Äôre considering writing a data ingestion connector to a database or \\nAPI, ask yourself: has this already been created? Furthermore, is there a service that \\nwill manage the nitty -gritty details of this connection for me? ‚ÄúAPIs‚Äù on page 254  \\nmentions the popularity of managed data connector platforms and frameworks. These \\ntools aim to provide a standard set of connectors available out of the box to spare data \\nengineers building complicated plumbing to connect to a particular source. Instead of \\ncreating and managing a data connector, you outsource this service to a third party. \\nGenerally, options in the space allow users to set a target and source, ingest in various \\nways (e.g., CDC, replication, truncate and reload), set permissions and credentials,'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 288, 'page_label': '289'}, page_content='configure an update frequency, and begin syncing data. The vendor or cloud behind \\nthe scenes fully manages and monitors data syncs. If data synchronization fails, you‚Äôll \\nreceive an alert with logged information on the cause of the error. \\nWe suggest using managed connector platforms instead of creating and managing your \\nconnectors. V endors and OSS projects each typically have hundreds of prebuilt \\nconnector options and can easily create custom connectors. The creation and \\nmanagement of data connectors is largely undifferentiated heavy lifting these days and \\nshould be outsourced whenever possible. \\nMoving Data with Object Storage \\nObject storage is a multitenant system in public clouds, and it supports storing massive \\namounts of data. This makes object storage ideal for moving data in and out of data \\nlakes, between teams, and transferring data between organizations. You can even \\nprovide short-term access to an object with a signed URL, giving a user temporary \\npermission. \\nIn our view, object storage is the most optimal and secure way to handle file exchange. \\nPublic cloud storage implements the latest security standards, has a robust track record \\nof scalability and reliability, accepts files of arbitrary types and sizes, and  provides \\nhigh-performance data movement. We discussed object storage much more extensively \\nin Chapter 6. \\nEDI \\nAnother practical reality for data engineers is electronic data interchange (EDI). The \\nterm is vague enough to refer to any data movement method. It usually refers to \\nsomewhat archaic means of file exchange, such as by email or flash drive. Data \\nengineers will find that some data sources do not support more modern means of data \\ntransport, often because of archaic IT systems or human process limitations. \\nEngineers can at least enhance EDI through automation. For example, they can set up \\na cloud-based email server that saves files onto company object storage as soon as they \\nare received. This can trigger orchestration processes to ingest and process data. This \\nis much more robust than an employee downloading the attached file and manually \\nuploading it to an internal system, which we still frequently see. \\nDatabases and File Export \\nEngineers should be aware of how the source database systems handle file export. \\nExport involves large data scans that significantly load the database for many \\ntransactional systems. Source system engineers must assess when these scans can be \\nrun without affecting application performance and might opt for a strategy to mitigate \\nthe load. Export queries can be broken into smaller exports by querying over key'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 289, 'page_label': '290'}, page_content='ranges or one partition at a time. Alternatively, a read replica can reduce load. Read \\nreplicas are especially appropriate if exports happen many times a day and coincide \\nwith a high source system load. \\nMajor cloud data warehouses are highly optimized for direct file export. For example, \\nSnowflake, BigQuery, Redshift, and others support direct export to object storage in \\nvarious formats. \\nPractical Issues with Common File Formats \\nEngineers should also be aware of the file formats to export. CSV is still ubiquitous \\nand highly error prone at the time of this writing. Namely, CSV‚Äôs default delimiter is \\nalso one of the most familiar characters in the English language ‚Äîthe comma! But it \\ngets worse. \\nCSV is by no means a uniform format. Engineers must stipulate the delimiter, quote \\ncharacters, and escaping to appropriately handle the export of string data. CSV also \\ndoesn‚Äôt natively encode schema information or directly support nested structures. CSV \\nfile encoding and schema information must be configured in the target system to ensure \\nappropriate ingestion. Autodetection is a convenience feature provided in many cloud \\nenvironments but is inappropriate for production ingestion. As a best practice, \\nengineers should record CSV encoding and schema details in file metadata. \\nMore robust and expressive export formats include Parquet, Avro, Arrow, and ORC or \\nJSON. These formats natively encode schema information and handle arbitrary string \\ndata with no particular intervention. Many of them also handle nested data structures \\nnatively so that JSON fields are stored using internal nested structures rather than \\nsimple strings. For columnar databases, columnar formats (Parquet, Arrow, ORC) \\nallow more efficient data export because columns can be directly transcoded between \\nformats. These formats are also generally more optimized for query engines. The \\nArrow file format i s designed to map data directly into processing engine memory, \\nproviding high performance in data lake environments. \\nThe disadvantage of these newer formats is that many of them are not natively \\nsupported by source systems. Data engineers are often forced to work with CSV data \\nand then build robust exception handling and error detection to ensure data quality on \\ningestion. See Appendix A for a more extensive discussion of file formats. \\nShell \\nThe shell is an interface by which you may execute commands to ingest data. The shell \\ncan be used to script workflows for virtually any software tool, and shell scripting is \\nstill used extensively in ingestion processes. A shell script might read data from a \\ndatabase, reserialize it into a different file format, upload it to object storage, and'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 290, 'page_label': '291'}, page_content='trigger an ingestion process in a target database. While storing data on a single instance \\nor server is not highly scalable, many of our data sources are not particularly large, and \\nsuch approaches work just fine. \\nIn addition, cloud vendors generally provide robust CLI -based tools. It is possible to \\nrun complex ingestion processes simply by issuing commands to the AWS CLI. As \\ningestion processes grow more complicated and the SLA grows more stringent, \\nengineers should consider moving to a proper orchestration system. \\nSSH \\nSSH is not an ingestion strategy but a protocol used with other ingestion strategies. We \\nuse SSH in a few ways. First, SSH can be used for file transfer with SCP, as mentioned \\nearlier. Second, SSH tunnels are used to allow secure, isolated connections to \\ndatabases. \\nApplication databases should never be directly exposed on the internet. Instead, \\nengineers can set up a bastion host‚Äîi.e., an intermediate host instance that can connect \\nto the database in question. This host machine is exposed on the internet, although \\nlocked down for minimal access from only specified IP addresses to specified ports. \\nTo connect to the database, a remote machine first opens an SSH tunnel connection to \\nthe bastion host and then connects from the host machine to the database. \\nSFTP and SCP \\nAccessing and sending data both from secure FTP (SFTP) and secure copy (SCP) are \\ntechniques you should be familiar with, even if data engineers do not typically use \\nthese regularly (IT or security/secOps will handle this). \\nEngineers rightfully cringe at the mention of SFTP (occasionally, we even hear \\ninstances of FTP being used in production). Regardless, SFTP is still a practical reality \\nfor many businesses. They work with partner businesses that consume or provide data \\nusing SFTP and are unwilling to rely on other standards. To avoid data leaks, security \\nanalysis is critical in these situations. \\nSCP is a file-exchange protocol that runs over an SSH connection. SCP can be a secure \\nfile-transfer option if it is configured correctly. Again, adding additional network \\naccess control (defense in depth) to enhance SCP security is highly recommended. \\nWebhooks \\nWebhooks, as we discussed in Chapter 5, are often referred to as reverse APIs. For a \\ntypical REST data API, the data provider gives engineers API specifications that they \\nuse to write their data ingestion code. The code makes requests and receives data in \\nresponses.'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 291, 'page_label': '292'}, page_content='With a webhook (Figure 7-15), the data provider defines an API request specification, \\nbut the data provider makes API calls  rather than receiving them; it‚Äôs the data \\nconsumer‚Äôs responsibility to provide an API endpoint for the provider to call. The \\nconsumer is responsible for ingesting each request and handling data aggregation, \\nstorage, and processing. \\n \\nFigure 7-15. A basic webhook ingestion architecture built from cloud services \\nWebhook-based data ingestion architectures can be brittle, difficult to maintain, and \\ninefficient. Using appropriate off-the-shelf tools, data engineers can build more robust \\nwebhook architectures with lower maintenance and infrastructure costs. For example, \\na webhook pattern in AWS might use a serverless function framework (Lambda) to \\nreceive incoming events, a managed event -streaming platform to store and buffer \\nmessages (Kinesis), a stream -processing framework to handle real -time analytics \\n(Flink), and an object store for long-term storage (S3). \\nYou‚Äôll notice that this architecture does much more than simply ingest the data. This \\nunderscores ingestion‚Äôs entanglement with the other stages of the data engineering \\nlifecycle; it is often impossible to define your ingestion architecture without making \\ndecisions about storage and processing. \\nWeb Interface \\nWeb interfaces for data access remain a practical reality for data engineers. We \\nfrequently run into situations where not all data and functionality in a SaaS platform is \\nexposed through automated interfaces such as APIs and file drops. Instead, someone \\nmust manually access a web interface, generate a report, and download a file to a local \\nmachine. This has obvious drawbacks, such as people forgetting to run the report or \\nhaving their laptop die. Where possible, choose tools and workflows that allow for \\nautomated access to data. \\nWeb Scraping \\nWeb scraping automatically extracts data from web pages, often by combing the web \\npage‚Äôs various HTML elements. You might scrape ecommerce sites to extract product \\npricing information or scrape multiple news sites for your news aggregator. Web'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 292, 'page_label': '293'}, page_content='scraping is widespread, and you may encounter it as a data engineer. It‚Äôs also a murky \\narea where ethical and legal lines are blurry. \\nHere is some top -level advice to be aware of before undertaking any web -scraping \\nproject. First, ask yourself if you should be web scraping or if data is available from a \\nthird party. If your decision is to web scrape, be a good citizen. Don‚Äôt inadvertentl y \\ncreate a denial -of-service (DoS) attack, and don‚Äôt get your IP address blocked. \\nUnderstand how much traffic you generate and pace your web -crawling activities \\nappropriately. Just because you can spin up thousands of simultaneous Lambda \\nfunctions to scrape doesn‚Äôt mean you should; excessive web scraping could lead to the \\ndisabling of your AWS account. \\nSecond, be aware of the legal implications of your activities. Again, generating DoS \\nattacks can entail legal consequences. Actions that violate terms of service may cause \\nheadaches for your employer or you personally. \\nThird, web pages constantly change their HTML element structure, making it tricky to \\nkeep your web scraper updated. Ask yourself, is the headache of maintaining these \\nsystems worth the effort? \\nWeb scraping has interesting implications for the data engineering lifecycle processing \\nstage; engineers should think about various factors at the beginning of a webscraping \\nproject. What do you intend to do with the data? Are you just pulling required fie lds \\nfrom the scraped HTML by using Python code and then writing these values to a \\ndatabase? Do you intend to maintain the complete HTML code of the scraped websites \\nand process this data using a framework like Spark? These decisions may lead to very \\ndifferent architectures downstream of ingestion. \\nTransfer Appliances for Data Migration \\nFor massive quantities of data (100 TB or more), transferring data directly over the \\ninternet may be a slow and costly process. At this scale, the fastest, most efficient way \\nto move data is not over the wire but by truck. Cloud vendors offer the ability to send \\nyour data via a physical ‚Äúbox of hard drives.‚Äù Simply order a storage device, called a \\ntransfer appliance, load your data from your servers, and then send it back to the cloud \\nvendor, which will upload your data. \\nThe suggestion is to consider using a transfer appliance if your data size hovers around \\n100 TB. On the extreme end, AWS even offers Snowmobile, a transfer appliance sent \\nto you in a semitrailer! Snowmobile is intended to lift and shift an entire data center, \\nin which data sizes are in the petabytes or greater. \\nTransfer appliances are handy for creating hybrid -cloud or multicloud setups. For \\nexample, Amazon‚Äôs data transfer appliance (AWS Snowball) supports import and \\nexport. To migrate into a second cloud, users can export their data into a Snowball'),\n",
       " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250809174521', 'source': '/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf', 'total_pages': 294, 'page': 293, 'page_label': '294'}, page_content='device and then import it into a second transfer appliance to move data into GCP or \\nAzure. This might sound awkward, but even when it‚Äôs feasible to push data over the \\ninternet between clouds, data egress fees make this a costly proposition. Physical \\ntransfer appliances are a cheaper alternative when the data volumes are significant. \\nRemember that transfer appliances and data migration services are one -time data \\ningestion events and are not suggested for ongoing workloads. Suppose you have \\nworkloads requiring constant data movement in either a hybrid or multicloud scenario. \\nIn that case, your data sizes are presumably batching or streaming much smaller data \\nsizes on an ongoing basis. \\nData Sharing \\nData sharing is growing as a popular option for consuming data (see Chapters 5 and \\n6). Data providers will offer datasets to third -party subscribers, either for free or at a \\ncost. These datasets are often shared in a read-only fashion, meaning you can integrate \\nthese datasets with your own data (and other third-party datasets), but you do not own \\nthe shared dataset. In the strict sense, this isn‚Äôt ingestion, where you get physical \\npossession of the dataset. If the data provider decides to remove your access  to a \\ndataset, you‚Äôll no longer have access to it. \\nMany cloud platforms offer data sharing, allowing you to share your data and consume \\ndata from various providers. Some of these platforms also provide data marketplaces \\nwhere companies and organizations can offer their data for sale. \\nWhom You‚Äôll Work With \\nData ingestion sits at several organizational boundaries. In developing and managing \\ndata ingestion pipelines, data engineers will work with both people and systems sitting \\nupstream (data producers) and downstream (data consumers). \\nUpstream Stakeholders \\nA significant disconnect often exists between those responsible for generating data ‚Äî\\ntypically, software engineers‚Äîand the data engineers who will prepare this data for \\nanalytics and data science. Software engineers and data engineers usually sit in \\nseparate organizational silos; if they think about data engineers, they typically see them \\nsimply as downstream consumers of the data exhaust from their application, not as \\nstakeholders. \\nWe see this current state of affairs as a problem and a significant opportunity. Data \\nengineers can improve the quality of their data by inviting software engineers to be \\nstakeholders in data engineering outcomes. The vast majority of software engineers')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/Users/akash/ProIT/Gen_ai_workspace/data engineering.pdf\")\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a28e7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ef951e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ \n",
    "        (\"system\", \"What is data engineering and why it's needed?:\\n\\n{context}\")\n",
    "    ]\n",
    ")\n",
    "llm = ChatGroq(model='openai/gpt-oss-120b', api_key=os.environ['groq_api'])\n",
    "documents_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ffa4626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Data engineering is the discipline of designing, building, and maintaining the infrastructure that allows organizations to collect, store, process, and analyze large volumes of data. It creates reliable, accessible data pipelines, databases, and warehouses that serve analytics, machine‚Äëlearning, and business‚Äëintelligence applications.',\n",
       " 'core_tasks': ['Data ingestion ‚Äì acquiring raw data from diverse sources.',\n",
       "  'Data transformation ‚Äì cleaning, normalising, and reshaping data (ETL/ELT).',\n",
       "  'Data integration ‚Äì combining data from multiple systems into unified views.',\n",
       "  'Data quality management ‚Äì monitoring, validating, and ensuring data accuracy and consistency.',\n",
       "  'Infrastructure provisioning ‚Äì setting up storage, compute, and networking resources (on‚Äëpremise or cloud).',\n",
       "  'Performance optimisation ‚Äì tuning pipelines, queries, and storage for scalability and cost‚Äëefficiency.'],\n",
       " 'key_technologies': ['ETL/ELT tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue)',\n",
       "  'Big‚Äëdata platforms (e.g., Apache Spark, Hadoop, Flink)',\n",
       "  'Cloud services (e.g., AWS S3, Azure Data Lake, Google Cloud Storage, Snowflake, Redshift, BigQuery)',\n",
       "  'Database systems (relational: PostgreSQL, MySQL; NoSQL: MongoDB, Cassandra; columnar: ClickHouse, Parquet)',\n",
       "  'Orchestration frameworks (e.g., Apache Airflow, Prefect, Dagster)',\n",
       "  'Containerisation & orchestration (Docker, Kubernetes) for reproducible pipelines'],\n",
       " 'collaboration': 'Data engineers work closely with data scientists, analysts, product managers, and business stakeholders to understand data requirements, ensure the data platform meets analytical needs, and provide reliable, timely data for decision‚Äëmaking.',\n",
       " 'business_value': ['Enables data‚Äëdriven decision making',\n",
       "  'Supports scalable analytics and machine‚Äëlearning workloads',\n",
       "  'Improves data reliability, governance, and security',\n",
       "  'Reduces time‚Äëto‚Äëinsight by automating data flows',\n",
       "  'Provides a competitive advantage through faster, more accurate insights']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(page_content= \"Data engineering is the process of designing, building, and maintaining the infrastructure and systems that enable the collection, storage, processing, and analysis of large volumes of data. It involves creating data pipelines, databases, and data warehouses to ensure that data is accessible, reliable, and usable for various applications such as analytics, machine learning, and business intelligence. Data engineering is essential because it provides the foundation for organizations to leverage their data effectively, make informed decisions, and gain insights that drive innovation and competitive advantage.\"),\n",
    "    Document(page_content= \"Data engineering involves various tasks such as data ingestion, data transformation, data integration, and data quality management. Data engineers work with technologies like ETL (Extract, Transform, Load) tools, big data platforms, cloud computing services, and database management systems to build scalable and efficient data architectures. They collaborate closely with data scientists, analysts, and other stakeholders to understand their data needs and ensure that the data infrastructure supports their requirements. Overall, data engineering plays a crucial role in enabling organizations to harness the power of their data and drive business growth.\"),\n",
    "]\n",
    "\n",
    "chain.invoke({\"query\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d209f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFaceBgeEmbeddings instead of OllamaEmbeddings to avoid local server errors\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "r1 = embeddings.embed_documents(\n",
    "    [\n",
    "        \"Data engineering is the process of designing, building, and maintaining the infrastructure and systems that enable the collection, storage, processing, and analysis of large volumes of data.\",\n",
    "        \"Data engineering involves various tasks such as data ingestion, data transformation, data integration, and data quality management.\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "034a37c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.027398614212870598,\n",
       "  0.033962614834308624,\n",
       "  0.017163019627332687,\n",
       "  -0.013406231068074703,\n",
       "  -0.03556375578045845,\n",
       "  -0.16296210885047913,\n",
       "  0.0026663311291486025,\n",
       "  -0.029463937506079674,\n",
       "  -0.043080877512693405,\n",
       "  -0.006952214986085892,\n",
       "  -0.03983607515692711,\n",
       "  0.0038078557699918747,\n",
       "  0.13135932385921478,\n",
       "  -0.0262091513723135,\n",
       "  -0.023204639554023743,\n",
       "  0.02773486077785492,\n",
       "  -0.05207686126232147,\n",
       "  -0.06304121762514114,\n",
       "  0.0035639037378132343,\n",
       "  -0.022369075566530228,\n",
       "  0.04736129194498062,\n",
       "  0.010829229839146137,\n",
       "  -0.07063561677932739,\n",
       "  -0.017077339813113213,\n",
       "  -0.0012326836585998535,\n",
       "  0.09751705825328827,\n",
       "  0.11906144767999649,\n",
       "  -0.09043829888105392,\n",
       "  0.022859718650579453,\n",
       "  -0.040094081312417984,\n",
       "  -0.028524596244096756,\n",
       "  -0.011036590673029423,\n",
       "  -0.0014812479494139552,\n",
       "  0.0629267767071724,\n",
       "  -0.01176141481846571,\n",
       "  -0.0003134893486276269,\n",
       "  0.054274991154670715,\n",
       "  -0.011781112290918827,\n",
       "  -0.020573005080223083,\n",
       "  0.019387569278478622,\n",
       "  -0.08024975657463074,\n",
       "  -0.04223500192165375,\n",
       "  -0.013277816586196423,\n",
       "  -0.015956824645400047,\n",
       "  -0.08382909744977951,\n",
       "  0.02676405943930149,\n",
       "  0.03256363049149513,\n",
       "  -0.0974406898021698,\n",
       "  -0.01705906353890896,\n",
       "  0.00388911459594965,\n",
       "  -0.09947257488965988,\n",
       "  0.020258381962776184,\n",
       "  -0.0027914089150726795,\n",
       "  0.03474890813231468,\n",
       "  0.0013438626192510128,\n",
       "  0.027880672365427017,\n",
       "  0.06280799955129623,\n",
       "  0.02786104753613472,\n",
       "  -0.08526550233364105,\n",
       "  0.03258691728115082,\n",
       "  0.012223409488797188,\n",
       "  -0.04162813723087311,\n",
       "  0.009644337929785252,\n",
       "  0.009328976273536682,\n",
       "  -0.03497643768787384,\n",
       "  0.017392734065651894,\n",
       "  -0.01852407492697239,\n",
       "  0.05043657869100571,\n",
       "  0.055010631680488586,\n",
       "  -0.1015482246875763,\n",
       "  0.02634778991341591,\n",
       "  -0.009824402630329132,\n",
       "  -0.018711116164922714,\n",
       "  0.0170296560972929,\n",
       "  0.045328374952077866,\n",
       "  -0.032481513917446136,\n",
       "  -0.03356704115867615,\n",
       "  0.019838273525238037,\n",
       "  0.09358056634664536,\n",
       "  -0.05444342643022537,\n",
       "  0.014405909925699234,\n",
       "  0.02947065979242325,\n",
       "  -0.0460464283823967,\n",
       "  0.04554785043001175,\n",
       "  0.03844877704977989,\n",
       "  -0.018287917599081993,\n",
       "  -0.008825075812637806,\n",
       "  0.04689395800232887,\n",
       "  -0.076820507645607,\n",
       "  0.044117119163274765,\n",
       "  0.015494897961616516,\n",
       "  0.006084232125431299,\n",
       "  -0.03607434406876564,\n",
       "  0.07057014107704163,\n",
       "  -0.0012740020174533129,\n",
       "  -0.012670676223933697,\n",
       "  -0.03317952901124954,\n",
       "  -0.07596510648727417,\n",
       "  0.08335878700017929,\n",
       "  -0.05178229510784149,\n",
       "  -0.03164320066571236,\n",
       "  0.08176706731319427,\n",
       "  0.0019529405981302261,\n",
       "  0.07551505416631699,\n",
       "  -0.05857664719223976,\n",
       "  -0.1292523443698883,\n",
       "  0.002692262176424265,\n",
       "  -0.021441219374537468,\n",
       "  -0.055946361273527145,\n",
       "  -0.04696409031748772,\n",
       "  -0.019323259592056274,\n",
       "  0.03188307210803032,\n",
       "  -0.021181853488087654,\n",
       "  -0.054583966732025146,\n",
       "  -0.014438203535974026,\n",
       "  -0.055797919631004333,\n",
       "  -0.1700054556131363,\n",
       "  0.021154135465621948,\n",
       "  -0.07867305725812912,\n",
       "  0.0032601337879896164,\n",
       "  -0.029833851382136345,\n",
       "  -0.02997972071170807,\n",
       "  -0.06072855368256569,\n",
       "  -0.013683888129889965,\n",
       "  0.003335880348458886,\n",
       "  -0.05804947018623352,\n",
       "  -0.022632408887147903,\n",
       "  -8.567669921836923e-33,\n",
       "  -0.05808860436081886,\n",
       "  -0.04250100627541542,\n",
       "  -0.03549684211611748,\n",
       "  0.010926785878837109,\n",
       "  -0.005987098440527916,\n",
       "  -0.08798912167549133,\n",
       "  -0.04195686802268028,\n",
       "  0.011393033899366856,\n",
       "  -0.03646602854132652,\n",
       "  0.02925703302025795,\n",
       "  -0.004703220445662737,\n",
       "  0.03470522165298462,\n",
       "  -0.03242103382945061,\n",
       "  0.0971451997756958,\n",
       "  0.12730057537555695,\n",
       "  0.04447484761476517,\n",
       "  -0.006391892675310373,\n",
       "  0.09151232987642288,\n",
       "  -0.0015643570804968476,\n",
       "  0.00951448455452919,\n",
       "  0.052123021334409714,\n",
       "  -0.08633486181497574,\n",
       "  0.11241597682237625,\n",
       "  0.048057328909635544,\n",
       "  0.08422131091356277,\n",
       "  0.0001618743990547955,\n",
       "  -0.02392514795064926,\n",
       "  0.040085382759571075,\n",
       "  0.01671379990875721,\n",
       "  0.0013084650272503495,\n",
       "  -0.07400024682283401,\n",
       "  -0.07406142354011536,\n",
       "  0.07928464561700821,\n",
       "  0.060415174812078476,\n",
       "  0.011544360779225826,\n",
       "  0.03537529706954956,\n",
       "  0.011050072498619556,\n",
       "  -0.026460615918040276,\n",
       "  0.00010407785885035992,\n",
       "  0.09904476255178452,\n",
       "  0.05553494393825531,\n",
       "  -0.005440285429358482,\n",
       "  0.0015513995895162225,\n",
       "  -0.023222599178552628,\n",
       "  -0.06726222485303879,\n",
       "  0.05567481741309166,\n",
       "  0.014167300425469875,\n",
       "  -0.059902373701334,\n",
       "  0.05692204460501671,\n",
       "  -0.06493423134088516,\n",
       "  0.04894089698791504,\n",
       "  -0.017206793650984764,\n",
       "  0.07108205556869507,\n",
       "  0.029656823724508286,\n",
       "  0.04839051142334938,\n",
       "  0.02284347265958786,\n",
       "  0.002783742966130376,\n",
       "  -0.07928605377674103,\n",
       "  -0.008382066152989864,\n",
       "  0.052250634878873825,\n",
       "  -0.09654196351766586,\n",
       "  -0.02258806675672531,\n",
       "  0.034812990576028824,\n",
       "  0.04363543912768364,\n",
       "  0.07339762896299362,\n",
       "  0.015559026040136814,\n",
       "  0.030404875054955482,\n",
       "  -0.002239986788481474,\n",
       "  0.09322723746299744,\n",
       "  -0.03622504323720932,\n",
       "  -0.07340465486049652,\n",
       "  0.02707056701183319,\n",
       "  0.037308841943740845,\n",
       "  -0.02203001268208027,\n",
       "  0.05804847553372383,\n",
       "  -0.040781065821647644,\n",
       "  -0.021179191768169403,\n",
       "  0.03538716211915016,\n",
       "  -0.013443523086607456,\n",
       "  0.050892315804958344,\n",
       "  -0.045553144067525864,\n",
       "  -0.009018192067742348,\n",
       "  -0.02897515334188938,\n",
       "  -0.029660645872354507,\n",
       "  0.04290410503745079,\n",
       "  0.05561479926109314,\n",
       "  -0.038004178553819656,\n",
       "  0.036561913788318634,\n",
       "  -0.062231969088315964,\n",
       "  -0.0065497420728206635,\n",
       "  -0.07516861706972122,\n",
       "  0.0069309500977396965,\n",
       "  -0.07190339267253876,\n",
       "  -0.0025032733101397753,\n",
       "  0.02945052459836006,\n",
       "  2.422072130735745e-33,\n",
       "  -0.026214884594082832,\n",
       "  0.038346726447343826,\n",
       "  -0.035807229578495026,\n",
       "  -3.14725621137768e-05,\n",
       "  0.03567500039935112,\n",
       "  0.009709570556879044,\n",
       "  -0.0042330119758844376,\n",
       "  0.00586724141612649,\n",
       "  0.054906006902456284,\n",
       "  -0.00468931021168828,\n",
       "  -0.025583416223526,\n",
       "  -0.055151768028736115,\n",
       "  0.050357699394226074,\n",
       "  -0.060851771384477615,\n",
       "  -0.05834482982754707,\n",
       "  -0.004119774792343378,\n",
       "  -0.036320917308330536,\n",
       "  -0.09307725727558136,\n",
       "  -0.09660627692937851,\n",
       "  0.011911008507013321,\n",
       "  -0.053931135684251785,\n",
       "  0.04182739555835724,\n",
       "  -0.013989719562232494,\n",
       "  -0.057861898094415665,\n",
       "  0.045731496065855026,\n",
       "  -0.01244171429425478,\n",
       "  -0.05581273138523102,\n",
       "  -0.0048432438634335995,\n",
       "  0.1015821248292923,\n",
       "  0.04341020807623863,\n",
       "  0.05200480669736862,\n",
       "  -0.017514077946543694,\n",
       "  0.06209081783890724,\n",
       "  -0.05418083444237709,\n",
       "  -0.02775067836046219,\n",
       "  -0.03046511486172676,\n",
       "  0.12053701281547546,\n",
       "  0.029892846941947937,\n",
       "  0.03939427435398102,\n",
       "  -0.009279971942305565,\n",
       "  -0.008408206515014172,\n",
       "  0.04469939321279526,\n",
       "  -0.026276763528585434,\n",
       "  -0.04887819290161133,\n",
       "  0.048466555774211884,\n",
       "  -0.022980069741606712,\n",
       "  -0.03973463922739029,\n",
       "  0.06004168838262558,\n",
       "  -0.023669550195336342,\n",
       "  -0.0771062970161438,\n",
       "  0.07663927227258682,\n",
       "  0.08199531584978104,\n",
       "  0.06915577501058578,\n",
       "  -0.06864095479249954,\n",
       "  0.04266464710235596,\n",
       "  -0.04800793528556824,\n",
       "  0.007673806045204401,\n",
       "  0.01365925557911396,\n",
       "  -0.022644780576229095,\n",
       "  -0.008909064345061779,\n",
       "  -0.061679765582084656,\n",
       "  -0.021857764571905136,\n",
       "  0.08459071815013885,\n",
       "  0.054319560527801514,\n",
       "  -0.08872883021831512,\n",
       "  -0.0010655163787305355,\n",
       "  0.02377534843981266,\n",
       "  -0.04334191232919693,\n",
       "  -0.12872126698493958,\n",
       "  -0.009191101416945457,\n",
       "  -0.006757532246410847,\n",
       "  0.014783314429223537,\n",
       "  -0.048307765275239944,\n",
       "  0.07994373142719269,\n",
       "  0.01148986630141735,\n",
       "  -0.06296548992395401,\n",
       "  -0.10017647594213486,\n",
       "  0.026560142636299133,\n",
       "  0.018263882026076317,\n",
       "  0.0953371673822403,\n",
       "  0.0538925975561142,\n",
       "  -0.003720184089615941,\n",
       "  0.048085376620292664,\n",
       "  0.027177074924111366,\n",
       "  0.03828887268900871,\n",
       "  -0.01351142767816782,\n",
       "  -0.009521515108644962,\n",
       "  -0.01269339770078659,\n",
       "  0.049327243119478226,\n",
       "  -0.06242687627673149,\n",
       "  -0.12684504687786102,\n",
       "  -0.07521961629390717,\n",
       "  -0.12437529861927032,\n",
       "  0.0032364323269575834,\n",
       "  -0.024503987282514572,\n",
       "  -4.151617361003446e-08,\n",
       "  -0.031103039160370827,\n",
       "  -0.0942244604229927,\n",
       "  0.03788551688194275,\n",
       "  -0.04607192426919937,\n",
       "  0.007183303590863943,\n",
       "  0.06798411905765533,\n",
       "  -0.032093293964862823,\n",
       "  0.12555666267871857,\n",
       "  0.03752328082919121,\n",
       "  -0.07416768372058868,\n",
       "  0.07861946523189545,\n",
       "  0.017245939001441002,\n",
       "  -0.03650502488017082,\n",
       "  0.00133942358661443,\n",
       "  -0.011434771120548248,\n",
       "  -0.006906504742801189,\n",
       "  0.06809699535369873,\n",
       "  -0.07118358463048935,\n",
       "  0.0424419604241848,\n",
       "  -0.016638731583952904,\n",
       "  0.018784910440444946,\n",
       "  0.0028248096350580454,\n",
       "  -0.022760525345802307,\n",
       "  0.0167966540902853,\n",
       "  0.12466772645711899,\n",
       "  0.001128114527091384,\n",
       "  -0.010458423756062984,\n",
       "  -0.08110009878873825,\n",
       "  -0.03765713796019554,\n",
       "  -0.06872786581516266,\n",
       "  0.04820415377616882,\n",
       "  -0.04854653403162956,\n",
       "  0.05360940843820572,\n",
       "  0.051356881856918335,\n",
       "  0.04707012325525284,\n",
       "  -0.054136693477630615,\n",
       "  0.10842292010784149,\n",
       "  0.05403554067015648,\n",
       "  -0.0267034899443388,\n",
       "  -0.021146265789866447,\n",
       "  -0.009330975823104382,\n",
       "  0.07240559160709381,\n",
       "  -0.05217503011226654,\n",
       "  -0.017532967031002045,\n",
       "  0.030401619151234627,\n",
       "  0.05911560356616974,\n",
       "  -0.0550038143992424,\n",
       "  0.0619799979031086,\n",
       "  0.00813230313360691,\n",
       "  0.03694167360663414,\n",
       "  0.002400127239525318,\n",
       "  0.005671898368746042,\n",
       "  -0.04688957706093788,\n",
       "  0.034142978489398956,\n",
       "  -0.015491336584091187,\n",
       "  0.01671469397842884,\n",
       "  -0.0045281946659088135,\n",
       "  0.004945879336446524,\n",
       "  0.001485577318817377,\n",
       "  -0.028081942349672318,\n",
       "  -0.007044815458357334,\n",
       "  0.07618226855993271,\n",
       "  0.00557556701824069,\n",
       "  -0.0312986858189106],\n",
       " [0.002754751592874527,\n",
       "  -0.015091649256646633,\n",
       "  0.05581982433795929,\n",
       "  -0.017813194543123245,\n",
       "  -0.05126000940799713,\n",
       "  -0.19663174450397491,\n",
       "  -0.006677288096398115,\n",
       "  -0.07856545597314835,\n",
       "  -0.05212784558534622,\n",
       "  -0.01587311550974846,\n",
       "  -0.01721799373626709,\n",
       "  -0.026653384789824486,\n",
       "  0.11866346746683121,\n",
       "  -0.052305519580841064,\n",
       "  0.023980680853128433,\n",
       "  0.006227035075426102,\n",
       "  -0.01844906248152256,\n",
       "  -0.001658584806136787,\n",
       "  -0.05377986282110214,\n",
       "  -0.06295022368431091,\n",
       "  0.05734521523118019,\n",
       "  0.02335367351770401,\n",
       "  -0.05543166771531105,\n",
       "  -0.0186726413667202,\n",
       "  -0.008654273115098476,\n",
       "  0.04960063472390175,\n",
       "  0.08622847497463226,\n",
       "  -0.06947311013936996,\n",
       "  0.03811008855700493,\n",
       "  -0.06657707691192627,\n",
       "  -0.01610739901661873,\n",
       "  0.039760369807481766,\n",
       "  0.012606590054929256,\n",
       "  0.06950362026691437,\n",
       "  -0.04377451911568642,\n",
       "  0.03221478313207626,\n",
       "  0.04045868664979935,\n",
       "  -0.0431436225771904,\n",
       "  0.011361065320670605,\n",
       "  0.006723774131387472,\n",
       "  -0.026671821251511574,\n",
       "  -0.041163135319948196,\n",
       "  -0.014768499881029129,\n",
       "  -0.04300570487976074,\n",
       "  -0.07005920261144638,\n",
       "  0.03699406981468201,\n",
       "  0.015389621257781982,\n",
       "  -0.11086297780275345,\n",
       "  -0.061157457530498505,\n",
       "  0.0019224992720410228,\n",
       "  -0.0812075287103653,\n",
       "  0.025090070441365242,\n",
       "  -0.028849124908447266,\n",
       "  0.018238820135593414,\n",
       "  0.008832097984850407,\n",
       "  0.03225977346301079,\n",
       "  0.05505760386586189,\n",
       "  0.049591515213251114,\n",
       "  -0.04808264970779419,\n",
       "  0.038792725652456284,\n",
       "  -0.012398768216371536,\n",
       "  -0.0008884203853085637,\n",
       "  0.0047461530193686485,\n",
       "  0.0027433945797383785,\n",
       "  0.014755526557564735,\n",
       "  -0.03970830887556076,\n",
       "  0.0016216991934925318,\n",
       "  0.03181914612650871,\n",
       "  0.016680702567100525,\n",
       "  -0.10063952207565308,\n",
       "  0.005517835728824139,\n",
       "  -0.06012994423508644,\n",
       "  -0.05677126348018646,\n",
       "  0.013867727480828762,\n",
       "  0.03408554196357727,\n",
       "  -0.03156508505344391,\n",
       "  -0.013198280707001686,\n",
       "  0.017504548653960228,\n",
       "  0.08387790620326996,\n",
       "  -0.02139238640666008,\n",
       "  -0.00653358269482851,\n",
       "  0.0384514220058918,\n",
       "  -0.02156812883913517,\n",
       "  0.07183121144771576,\n",
       "  0.05124299228191376,\n",
       "  -0.025103118270635605,\n",
       "  -0.0012950721429660916,\n",
       "  0.0454556904733181,\n",
       "  -0.07695703208446503,\n",
       "  0.03802890703082085,\n",
       "  0.0006764838472008705,\n",
       "  -0.029280195012688637,\n",
       "  -0.007497347891330719,\n",
       "  0.06326718628406525,\n",
       "  0.0032390537671744823,\n",
       "  0.013055238872766495,\n",
       "  -0.012785571627318859,\n",
       "  -0.054566238075494766,\n",
       "  0.09701641649007797,\n",
       "  -0.08222320675849915,\n",
       "  -0.04632502421736717,\n",
       "  0.07231468707323074,\n",
       "  -0.011875619180500507,\n",
       "  0.03159432113170624,\n",
       "  -0.05100320279598236,\n",
       "  -0.10592437535524368,\n",
       "  -0.025645960122346878,\n",
       "  0.01327099371701479,\n",
       "  -0.033054374158382416,\n",
       "  0.015935195609927177,\n",
       "  -0.05083796754479408,\n",
       "  0.05155029520392418,\n",
       "  0.027454642578959465,\n",
       "  -0.08069024980068207,\n",
       "  0.011264180764555931,\n",
       "  -0.02144414186477661,\n",
       "  -0.13927070796489716,\n",
       "  0.03022374026477337,\n",
       "  -0.0613296814262867,\n",
       "  -0.015116582624614239,\n",
       "  -0.028509320691227913,\n",
       "  -0.0011463426053524017,\n",
       "  -0.03225259110331535,\n",
       "  0.008704419247806072,\n",
       "  -0.004131130874156952,\n",
       "  -0.03033953346312046,\n",
       "  -0.006250225007534027,\n",
       "  -2.0021405492289064e-33,\n",
       "  -0.08218922466039658,\n",
       "  -0.05302811414003372,\n",
       "  -0.017465071752667427,\n",
       "  0.016215166077017784,\n",
       "  0.008774667046964169,\n",
       "  -0.05610600858926773,\n",
       "  -0.03270181640982628,\n",
       "  0.017768312245607376,\n",
       "  0.009263423271477222,\n",
       "  0.00494662020355463,\n",
       "  -0.01239841803908348,\n",
       "  0.047719743102788925,\n",
       "  -0.07152557373046875,\n",
       "  0.08008897304534912,\n",
       "  0.08717983216047287,\n",
       "  0.025634832680225372,\n",
       "  -0.037379100918769836,\n",
       "  0.11937419325113297,\n",
       "  -0.030230257660150528,\n",
       "  0.02055906504392624,\n",
       "  0.06463272869586945,\n",
       "  -0.07713755965232849,\n",
       "  0.09401507675647736,\n",
       "  0.06115448847413063,\n",
       "  0.10116942971944809,\n",
       "  0.01968560926616192,\n",
       "  -0.011600608006119728,\n",
       "  0.058932140469551086,\n",
       "  0.018521353602409363,\n",
       "  -0.009313841350376606,\n",
       "  -0.06737547367811203,\n",
       "  -0.048902951180934906,\n",
       "  0.05024679750204086,\n",
       "  0.03882239758968353,\n",
       "  0.022971268743276596,\n",
       "  0.027079172432422638,\n",
       "  0.03930619731545448,\n",
       "  -0.03670836612582207,\n",
       "  0.0006547627272084355,\n",
       "  0.1282110959291458,\n",
       "  0.02207508124411106,\n",
       "  0.016117038205266,\n",
       "  0.003268997883424163,\n",
       "  -0.01325058564543724,\n",
       "  -0.014188424684107304,\n",
       "  0.006421820726245642,\n",
       "  0.041605278849601746,\n",
       "  -0.0593646802008152,\n",
       "  0.10090404748916626,\n",
       "  -0.052966393530368805,\n",
       "  0.039168015122413635,\n",
       "  -0.024129942059516907,\n",
       "  0.11978507786989212,\n",
       "  -0.012079412117600441,\n",
       "  0.04568273574113846,\n",
       "  0.039677150547504425,\n",
       "  0.054459236562252045,\n",
       "  -0.10289773344993591,\n",
       "  -0.02503778226673603,\n",
       "  0.050077877938747406,\n",
       "  -0.11912602931261063,\n",
       "  -0.014636814594268799,\n",
       "  -0.0015355247305706143,\n",
       "  0.021032830700278282,\n",
       "  0.08185861259698868,\n",
       "  0.006771953776478767,\n",
       "  0.05007414519786835,\n",
       "  0.013314379379153252,\n",
       "  0.04569685459136963,\n",
       "  -0.059580497443675995,\n",
       "  -0.07019015401601791,\n",
       "  0.0347171351313591,\n",
       "  0.0223692674189806,\n",
       "  -0.015050532296299934,\n",
       "  0.057451289147138596,\n",
       "  -0.044043999165296555,\n",
       "  -0.05629369243979454,\n",
       "  0.035713035613298416,\n",
       "  0.023429343476891518,\n",
       "  0.054344963282346725,\n",
       "  -0.034696225076913834,\n",
       "  0.029328953474760056,\n",
       "  0.004797575995326042,\n",
       "  -0.06189178302884102,\n",
       "  0.0347227118909359,\n",
       "  0.020642369985580444,\n",
       "  -0.021216580644249916,\n",
       "  -0.007486362475901842,\n",
       "  -0.026397528126835823,\n",
       "  0.00396047392860055,\n",
       "  -0.024062296375632286,\n",
       "  -0.01373661682009697,\n",
       "  -0.07060199230909348,\n",
       "  0.03615197166800499,\n",
       "  0.0523582361638546,\n",
       "  -1.4609658171488686e-33,\n",
       "  -0.05170468986034393,\n",
       "  0.026454340666532516,\n",
       "  -0.044842272996902466,\n",
       "  0.07737558335065842,\n",
       "  0.008931055665016174,\n",
       "  0.001915451604872942,\n",
       "  0.008543767966330051,\n",
       "  -0.026349078863859177,\n",
       "  0.11403211206197739,\n",
       "  -0.010641030967235565,\n",
       "  -0.07169146090745926,\n",
       "  -0.04182189330458641,\n",
       "  0.014346362091600895,\n",
       "  -0.09385024011135101,\n",
       "  -0.07114369422197342,\n",
       "  -0.008465781807899475,\n",
       "  -0.028282133862376213,\n",
       "  -0.0672607421875,\n",
       "  -0.13346147537231445,\n",
       "  0.02395489811897278,\n",
       "  0.0054486901499331,\n",
       "  0.06850920617580414,\n",
       "  -0.02807154692709446,\n",
       "  -0.04535984247922897,\n",
       "  0.07664745301008224,\n",
       "  -0.007607852108776569,\n",
       "  -0.029076553881168365,\n",
       "  0.02201284095644951,\n",
       "  0.11904732882976532,\n",
       "  0.03867379203438759,\n",
       "  0.0607144795358181,\n",
       "  -0.01760505512356758,\n",
       "  0.06604279577732086,\n",
       "  -0.0685356855392456,\n",
       "  -0.061921633780002594,\n",
       "  -0.054361313581466675,\n",
       "  0.10581918060779572,\n",
       "  0.01627463288605213,\n",
       "  0.03188629448413849,\n",
       "  -0.018969818949699402,\n",
       "  0.007458250969648361,\n",
       "  0.06365825235843658,\n",
       "  -0.014151069335639477,\n",
       "  -0.021272139623761177,\n",
       "  0.03044995665550232,\n",
       "  0.028606487438082695,\n",
       "  -0.04380901902914047,\n",
       "  0.05070536211133003,\n",
       "  -0.021261349320411682,\n",
       "  -0.07260153442621231,\n",
       "  0.08391037583351135,\n",
       "  0.06135302037000656,\n",
       "  0.04240826144814491,\n",
       "  -0.011117414571344852,\n",
       "  0.03209305554628372,\n",
       "  -0.0439133420586586,\n",
       "  -0.02624054253101349,\n",
       "  -0.01461760699748993,\n",
       "  -0.04404103755950928,\n",
       "  -0.027938775718212128,\n",
       "  -0.052451711148023605,\n",
       "  0.024179793894290924,\n",
       "  0.12518343329429626,\n",
       "  0.02276357263326645,\n",
       "  -0.10540745407342911,\n",
       "  0.010666429065167904,\n",
       "  0.02986699901521206,\n",
       "  -0.010953308083117008,\n",
       "  -0.09323301911354065,\n",
       "  -0.04289572685956955,\n",
       "  0.006808542180806398,\n",
       "  -0.014656231738626957,\n",
       "  0.029164450243115425,\n",
       "  0.020754586905241013,\n",
       "  0.003845090977847576,\n",
       "  -0.058807604014873505,\n",
       "  -0.08342445641756058,\n",
       "  0.017356904223561287,\n",
       "  -0.0016561984084546566,\n",
       "  0.07817550003528595,\n",
       "  0.05249535292387009,\n",
       "  -0.006672745570540428,\n",
       "  0.00549751752987504,\n",
       "  0.04922238364815712,\n",
       "  0.017634842544794083,\n",
       "  0.047117505222558975,\n",
       "  -0.01975288800895214,\n",
       "  -0.011642285622656345,\n",
       "  0.04565862566232681,\n",
       "  -0.06924767792224884,\n",
       "  -0.12494737654924393,\n",
       "  -0.0811774805188179,\n",
       "  -0.09525100886821747,\n",
       "  0.06235938519239426,\n",
       "  -0.015100788325071335,\n",
       "  -2.8710370969520227e-08,\n",
       "  -0.016279807314276695,\n",
       "  -0.0882987231016159,\n",
       "  0.0190524123609066,\n",
       "  -0.06808241456747055,\n",
       "  0.0006668827263638377,\n",
       "  0.05052058771252632,\n",
       "  -0.09717124700546265,\n",
       "  0.1506156325340271,\n",
       "  0.054838430136442184,\n",
       "  -0.01026696152985096,\n",
       "  0.036186445504426956,\n",
       "  0.03446754068136215,\n",
       "  -0.0670187696814537,\n",
       "  -0.010247883386909962,\n",
       "  0.058384448289871216,\n",
       "  -0.028566690161824226,\n",
       "  0.08563165366649628,\n",
       "  -0.05447251349687576,\n",
       "  0.04585163667798042,\n",
       "  -0.03925492987036705,\n",
       "  -0.027108727023005486,\n",
       "  0.00735295657068491,\n",
       "  -0.042260169982910156,\n",
       "  0.0026248465292155743,\n",
       "  0.12330161780118942,\n",
       "  -0.005501146428287029,\n",
       "  -0.017597511410713196,\n",
       "  -0.05245881900191307,\n",
       "  -0.04878291115164757,\n",
       "  -0.05035300925374031,\n",
       "  0.032306358218193054,\n",
       "  -0.06790231913328171,\n",
       "  0.07365082204341888,\n",
       "  0.0691121369600296,\n",
       "  0.035122618079185486,\n",
       "  -0.014537611044943333,\n",
       "  0.1137511283159256,\n",
       "  0.03594689816236496,\n",
       "  -0.04188670590519905,\n",
       "  -0.0141056003049016,\n",
       "  0.02457435056567192,\n",
       "  0.08920873701572418,\n",
       "  -0.05207512900233269,\n",
       "  0.008320119231939316,\n",
       "  -0.00939587689936161,\n",
       "  0.03952464461326599,\n",
       "  -0.03950159251689911,\n",
       "  0.05228981003165245,\n",
       "  0.005269400309771299,\n",
       "  -0.027566460892558098,\n",
       "  0.014276329427957535,\n",
       "  0.017746152356266975,\n",
       "  -0.05835346877574921,\n",
       "  0.002399542834609747,\n",
       "  -0.010902944020926952,\n",
       "  0.011014045216143131,\n",
       "  -0.0094684399664402,\n",
       "  -0.0072041843086481094,\n",
       "  0.023423654958605766,\n",
       "  0.0006676167831756175,\n",
       "  0.03787210211157799,\n",
       "  0.049658745527267456,\n",
       "  0.04711813107132912,\n",
       "  -0.02065909281373024]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481a904",
   "metadata": {},
   "source": [
    " # Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4b888ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['groq_api'] = os.getenv('groq_api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a4729c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e4b9f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x31e054950>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x31e4d8950>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model = 'llama-3.1-8b-instant', api_key=os.environ['groq_api'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33f5d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87e8a812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Akash, nice to meet you. I'm here to help you learn and answer any questions you may have. What subject or topic are you interested in learning more about?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 47, 'total_tokens': 84, 'completion_time': 0.044289225, 'prompt_time': 0.003886919, 'queue_time': 0.056247531, 'total_time': 0.048176144}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e2fbdc74-b12f-4a59-9e80-785a92e400b8-0', usage_metadata={'input_tokens': 47, 'output_tokens': 37, 'total_tokens': 84})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content = 'Hello , My name is Akash and I am a learner')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f68b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a1b5bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Akash, and you mentioned you are a learner. That's all I know about you so far. You're a learner, which means you're someone who is seeking knowledge and wanting to learn new things. That's a great attitude to have!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 104, 'total_tokens': 158, 'completion_time': 0.115032573, 'prompt_time': 0.008723306, 'queue_time': 0.055947594, 'total_time': 0.123755879}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--ab744d7a-968c-4c7e-9d3c-ac9980b5ca31-0', usage_metadata={'input_tokens': 104, 'output_tokens': 54, 'total_tokens': 158})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content = 'Hello , My name is Akash and I am a learner'),\n",
    "              AIMessage(content = 'Hello Akash, nice to meet you. Im here to help you learn and answer any questions you may have. What subject or topic are you interested in learning more about?'),\n",
    "              HumanMessage(content = 'Hey, what is my name? and who i am?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c8751813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92fa2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aad137ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable':{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56168e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello again Akash, it's great to know that you're still a learner and a cricket enthusiast. Playing cricket is a fantastic way to stay active, have fun, and socialize with others.\\n\\nSo, do you play cricket professionally or just as a hobby? Are you part of a local team or school team?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 402, 'total_tokens': 467, 'completion_time': 0.204714665, 'prompt_time': 0.028261758, 'queue_time': 0.057354242, 'total_time': 0.232976423}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--faa5daaa-8a9c-4ba1-9470-9350c9a041c5-0', usage_metadata={'input_tokens': 402, 'output_tokens': 65, 'total_tokens': 467})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = 'Hello , My name is Akash and I am a learner and i love to play cricket')],\n",
    "    config = config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7356dcf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m with_message_history.invoke(\n\u001b[32m      2\u001b[39m     [HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is my name? and what do I love to do?\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     config=\u001b[43mconfig1\u001b[49m,\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'config1' is not defined"
     ]
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name? and what do I love to do?\")],\n",
    "    config=config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "40011f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Akash.\\n\\nYou love to play cricket.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 489, 'total_tokens': 502, 'completion_time': 0.010733719, 'prompt_time': 0.038409605, 'queue_time': 0.053580535, 'total_time': 0.049143324}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c8fc107d-baea-4c16-98b0-13631e7eaf0a-0', usage_metadata={'input_tokens': 489, 'output_tokens': 13, 'total_tokens': 502})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name? and what do I love to do?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f51a26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {'configurable':{\"session_id\":\"chat2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf843bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Unfortunately, I'm a large language model, I don't have any prior information about you. Our conversation just started, and I don't know your name or your preferences. If you'd like to share that information with me, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 48, 'total_tokens': 104, 'completion_time': 0.081929307, 'prompt_time': 0.002349633, 'queue_time': 0.050505656, 'total_time': 0.08427894}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--80623601-773f-4b99-8393-be971e8f91ad-0', usage_metadata={'input_tokens': 48, 'output_tokens': 56, 'total_tokens': 104})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name? and what do I love to do?\")],\n",
    "    config=config1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6e8fb480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Akash.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 515, 'total_tokens': 522, 'completion_time': 0.011600915, 'prompt_time': 0.111542845, 'queue_time': 0.052218775, 'total_time': 0.12314376}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2a17d5ec-d4df-4010-b27e-03b1b89d54b6-0', usage_metadata={'input_tokens': 515, 'output_tokens': 7, 'total_tokens': 522})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"My name is ??\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d8a7f",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6507ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9b4105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant that remembers previous conversations.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2df2c063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Akash, nice to chat with you again.  I remember we didn't have a previous conversation, so we're starting from scratch. I'm here to help you with any questions or topics you'd like to discuss. What would you like to learn about today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 57, 'total_tokens': 113, 'completion_time': 0.108707754, 'prompt_time': 0.002835464, 'queue_time': 0.050407485, 'total_time': 0.111543218}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--00a0ec98-0ad7-4746-99b5-3283f6605fef-0', usage_metadata={'input_tokens': 57, 'output_tokens': 56, 'total_tokens': 113})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages':[HumanMessage(content=\"Hello , My name is Akash and I am a learner\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9565f6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Akash.\\n\\nYou love to play cricket.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respones = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name? and what do I love to do?\")],\n",
    "    config=config,\n",
    ")\n",
    "respones.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "21e2c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that can answer questions based on the previous conversation and also provide additional information when needed{language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bc9423a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour Akash, nice to meet you. Is there something I can help you with today?'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respones = chain.invoke({'messages':[HumanMessage(content=\"Hello , My name is Akash\")],'language':\"French\"})\n",
    "respones.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4611a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02e3017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bonjour Akash, it's nice to meet you.  Is there something I can help you with today?\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "responses = with_message_history.invoke(\n",
    "\t{'messages': [HumanMessage(content=\"Hello , My name is Akash\")], 'language': \"French\"},\n",
    "\tconfig=config3\n",
    ")\n",
    "responses.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f45713c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is still Akash.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "responses = with_message_history.invoke(\n",
    "\t{'messages': [HumanMessage(content=\"whats my name?\")], 'language': \"French\"},\n",
    "\tconfig=config3\n",
    ")\n",
    "responses.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "860560bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=60,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "acf32af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You mentioned earlier that you like vanilla ice cream!'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages = itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content = \"What ice cream do I like?\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a47ec99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked the math problem: 2 + 2\\nThe answer is: 4'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone = chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content = \"what math problem did i ask and what is the answer\")],\n",
    "        \"language\": \"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "respone.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c842b9",
   "metadata": {},
   "source": [
    "# Search engine with tools and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ec6753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arxiv -- Research\n",
    "## Tools creation\n",
    "\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e949c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f9c5f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3ddd6f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## used the inbuilt tool of wikipedia\n",
    "\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dde38612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8c1991a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki,arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c57d5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom tools [RAG tools]\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1c240cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31e49d940>, search_kwargs={})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "document = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb = FAISS.from_documents(document,HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')) \n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a6cd1031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith-search\",\"Search any information about Langsmith \"\n",
    ")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2ac2ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cad42b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x318d34220>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31e49d940>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x3164c3d80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31e49d940>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "33c3db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all this tools with agents and llm models\n",
    "# tools, LLM --> agent executor\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api = os.getenv('groq_api')\n",
    "\n",
    "llm = ChatGroq(api_key = os.getenv('groq_api'), model = 'openai/gpt-oss-120b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cca03a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prompt template\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d4f6fe25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x129cfd6c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x129cfd6c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x399f885f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x399f890d0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm,tools,prompt)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d3207451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x129cfd6c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x129cfd6c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x399f885f0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x399f890d0>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any information about Langsmith ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/Users/akash/ProIT/Gen_ai_workspace/ven1/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='langsmith-search', description='Search any information about Langsmith ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x318d34220>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31e49d940>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x3164c3d80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x31e49d940>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agent executor\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent = agent,tools = tools,verbose = True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e31552f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mLangSmith docs - Docs by LangChainSkip to main contentüöÄ Share how you're building agents for a chance to win LangChain swag!Docs by LangChain home pageLangSmithSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangSmith docsGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderPlatform setupReferenceOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementAdditional resourcesPolly (Beta)Data managementAccess control & AuthenticationScalability & resilienceFAQsRegions FAQPricing FAQLangSmith statusLangSmith docsCopy pageCopy pageLangSmith provides tools for developing, debugging, and deploying LLM applications.\n",
      "It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.\n",
      "LangSmith is framework agnostic, so you can use it with or without LangChain‚Äôs open-source libraries\n",
      "langchain and langgraph.\n",
      "\n",
      "LangSmith is framework agnostic, so you can use it with or without LangChain‚Äôs open-source libraries\n",
      "langchain and langgraph.\n",
      "Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\n",
      "LangGraph Platform is now LangSmith Deployment. For more information, check out the Changelog.\n",
      "‚ÄãGet started\n",
      "Create an accountSign up at smith.langchain.com (no credit card required).\n",
      "You can log in with Google, GitHub, or email.Create an API keyGo to your Settings page ‚Üí API Keys ‚Üí Create API Key.\n",
      "Copy the key and save it securely.\n",
      "Once your account and API key are ready, choose a quickstart to begin building with LangSmith:\n",
      "\n",
      "‚ÄãMore ways to build\n",
      "Platform setupUse LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.Choose how to set up LangSmithAgent Builder (Beta)Design and deploy AI agents visually with a no-code interface‚Äîperfect for rapid prototyping and getting started without writing code.Build an agentStudioUse a visual interface to design, test, and refine applications end-to-end.Develop with StudioPrompt testingIterate on prompts with built-in versioning and collaboration to ship improvements faster.Test your prompts\n",
      "LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.\n",
      "‚ÄãWorkflow\n",
      "LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow‚Äîfrom local development to production.\n",
      "\n",
      "Edit the source of this page on GitHub.\n",
      "Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoPricing plansNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\u001b[0m\u001b[32;1m\u001b[1;3m**LangSmith ‚Äì An Observability & Development Platform for LLM Applications**  \n",
      "\n",
      "LangSmith (formerly the ‚ÄúLangGraph Platform‚Äù) is a SaaS product built by the LangChain team that gives developers a single place to **build, test, debug, monitor, evaluate, and deploy** any large‚Äëlanguage‚Äëmodel (LLM)‚Äëpowered application‚Äîwhether the code uses LangChain, LangGraph, or any other framework.\n",
      "\n",
      "Below is a concise overview of what LangSmith is, why it matters, and the core features it provides.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. What Is LangSmith?\n",
      "\n",
      "| Aspect | Description |\n",
      "|--------|-------------|\n",
      "| **Purpose** | End‚Äëto‚Äëend observability and lifecycle management for LLM apps. |\n",
      "| **Scope** | Works with any Python (or other language) code that calls an LLM ‚Äì it is **framework‚Äëagnostic** (you can use it with LangChain, LangGraph, or pure API calls). |\n",
      "| **Target users** | Prompt engineers, data scientists, ML engineers, product teams, and anyone who ships AI‚Äëdriven features. |\n",
      "| **Deployment options** | Managed cloud, self‚Äëhosted, or hybrid (you choose the environment that fits your compliance and latency needs). |\n",
      "| **Security & compliance** | HIPAA, SOC‚ÄØ2‚ÄØType‚ÄØ2, GDPR‚Äëcompliant; data is encrypted at rest and in transit. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Feature Set\n",
      "\n",
      "### 2.1 Observability (Tracing & Logging)\n",
      "\n",
      "| Feature | What it does | Why it matters |\n",
      "|---------|--------------|----------------|\n",
      "| **Request tracing** | Every LLM call (prompt, model, parameters, response) is automatically recorded as a ‚Äútrace‚Äù. | Gives you a complete audit trail to reproduce bugs or understand model behavior. |\n",
      "| **Metadata capture** | You can attach custom tags (e.g., user ID, experiment name, environment) to each trace. | Enables powerful filtering, cost‚Äëanalysis, and downstream analytics. |\n",
      "| **Dashboard & UI** | Visual timeline of each trace, showing prompt, model output, latency, token usage, and any downstream code execution. | Quickly spot slow steps, hallucinations, or token‚Äëbudget overruns. |\n",
      "| **Alerting & notifications** | Set thresholds on latency, error rates, or custom metrics; receive Slack/Email/webhook alerts. | Prevents production incidents from slipping through. |\n",
      "\n",
      "### 2.2 Prompt Engineering & Testing (Prompt Studio)\n",
      "\n",
      "| Feature | What it does | Why it matters |\n",
      "|---------|--------------|----------------|\n",
      "| **Prompt versioning** | Every edit creates a new version that can be compared side‚Äëby‚Äëside. | Guarantees reproducibility and lets you roll back to a known‚Äëgood version. |\n",
      "| **Collaborative editing** | Teams can comment, suggest changes, and approve prompts directly in the UI. | Reduces hand‚Äëoff friction between data scientists and product managers. |\n",
      "| **Prompt testing sandbox** | Run a prompt against a set of test inputs, see the model‚Äôs responses, and capture metrics (accuracy, toxicity, etc.). | Enables rapid iteration before code deployment. |\n",
      "| **A/B testing** | Deploy two prompt versions simultaneously and compare performance on live traffic. | Empirical evidence for choosing the best prompt. |\n",
      "\n",
      "### 2.3 Evaluation & Metrics\n",
      "\n",
      "| Feature | What it does | Why it matters |\n",
      "|---------|--------------|----------------|\n",
      "| **Built‚Äëin evaluators** | Common metrics (BLEU, ROUGE, exact match, semantic similarity, factuality, toxicity, cost). | Gives you a quantitative view of output quality. |\n",
      "| **Custom evaluators** | Upload a Python function that receives a trace and returns a score; can be stored in the platform. | Supports domain‚Äëspecific criteria (e.g., legal compliance, business KPIs). |\n",
      "| **Batch evaluation** | Run an evaluator over a large dataset (thousands of traces) and view aggregated results. | Helps you spot systematic failures. |\n",
      "| **Evaluation dashboards** | Trend charts, heatmaps, and per‚Äëprompt breakdowns. | Makes it easy to monitor drift over time. |\n",
      "\n",
      "### 2.4 Deployment & Model Management\n",
      "\n",
      "| Feature | What it does | Why it matters |\n",
      "|---------|--------------|----------------|\n",
      "| **Deployment UI** | Define a ‚Äúdeployment‚Äù that points to a specific model, prompt version, and runtime configuration (e.g., temperature, max tokens). | Provides a stable endpoint for production services. |\n",
      "| **Versioned deployments** | Deployments are immutable; a new version creates a new endpoint, preserving the old one. | Guarantees backward compatibility and safe rollouts. |\n",
      "| **Canary & blue‚Äëgreen releases** | Route a fraction of traffic to a new deployment and monitor metrics before full cut‚Äëover. | Reduces risk of breaking changes. |\n",
      "| **Self‚Äëhosted option** | Run the LangSmith server inside your VPC or on‚Äëpremises, with the same UI and API. | Meets strict data‚Äë residency or latency requirements. |\n",
      "\n",
      "### 2.5 Agent Builder (Beta ‚Äì No‚ÄëCode UI)\n",
      "\n",
      "* Drag‚Äëand‚Äëdrop interface for building multi‚Äëstep LLM agents (e.g., tool‚Äëusing assistants, retrieval‚Äëaugmented generation pipelines).  \n",
      "* Generates runnable code that can be exported to your repo or deployed directly from the platform.  \n",
      "* Great for rapid prototyping, proof‚Äëof‚Äëconcepts, or for product managers who want to experiment without writing code.\n",
      "\n",
      "### 2.6 API & SDK Integration\n",
      "\n",
      "| Integration | Details |\n",
      "|------------|---------|\n",
      "| **Python SDK** | `langsmith` package provides decorators (`@trace`, `@evaluate`) and helper functions to log traces automatically. |\n",
      "| **REST API** | Full CRUD on traces, evaluations, deployments, and tags ‚Äì useful for custom dashboards or CI pipelines. |\n",
      "| **LangChain & LangGraph hooks** | Out‚Äëof‚Äëthe‚Äëbox support; just enable `langsmith=True` when constructing a chain/graph. |\n",
      "| **VS‚ÄØCode & Claude extensions** | Inline ‚ÄúAsk LangSmith‚Äù commands that fetch trace details or evaluation results without leaving the editor. |\n",
      "\n",
      "### 2.7 Collaboration & Governance\n",
      "\n",
      "* **Workspaces & organizations** ‚Äì Separate environments for dev, staging, and prod; role‚Äëbased access control (viewer, editor, admin).  \n",
      "* **Resource tagging** ‚Äì Tag traces with cost centers, project IDs, or regulatory tags for auditability.  \n",
      "* **Export & import** ‚Äì Export trace data to CSV/JSON or import external logs for unified analysis.  \n",
      "* **Audit logs** ‚Äì Every change to a deployment, prompt, or evaluation is recorded for compliance.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Typical Workflow\n",
      "\n",
      "1. **Prototype locally** ‚Äì Use the SDK to instrument your code; run a few prompts and see traces in the LangSmith UI.  \n",
      "2. **Iterate on prompts** ‚Äì Edit, version, and test prompts in Prompt Studio; run unit‚Äëtest‚Äëstyle evaluations.  \n",
      "3. **Define evaluations** ‚Äì Attach built‚Äëin or custom evaluators; monitor quality metrics in real time.  \n",
      "4. **Create a deployment** ‚Äì Pin a prompt version and model configuration; expose the endpoint to your app.  \n",
      "5. **Canary rollout** ‚Äì Shift a small % of traffic to the new deployment; watch latency, error, and evaluation dashboards.  \n",
      "6. **Full production** ‚Äì Promote the canary to 100‚ÄØ% traffic; continue to monitor traces for drift.  \n",
      "7. **Continuous improvement** ‚Äì Use the trace history to discover new failure modes, create new evaluators, and repeat.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Pricing & Availability (as of the latest docs)\n",
      "\n",
      "| Plan | Main limits | Who it‚Äôs for |\n",
      "|------|-------------|--------------|\n",
      "| **Free** | Up to 10‚ÄØk traces/month, 1‚ÄØk evaluation runs, community support | Individual developers, hobby projects |\n",
      "| **Pro** | Higher trace/eval caps, private workspaces, SSO, SLA‚Äëbacked support | Small teams, startups |\n",
      "| **Enterprise** | Unlimited usage, dedicated VPC/self‚Äëhosted option, compliance contracts, custom SLAs | Large organizations, regulated industries |\n",
      "\n",
      "All plans include the same core feature set; only limits and support levels differ.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Why Use LangSmith?\n",
      "\n",
      "| Pain point | LangSmith solution |\n",
      "|------------|--------------------|\n",
      "| **Hard to reproduce bugs** | Full request trace + prompt versioning lets you replay any failure. |\n",
      "| **No systematic way to measure output quality** | Built‚Äëin evaluators + custom functions give quantitative metrics. |\n",
      "| **Risky rollouts** | Canary deployments, A/B testing, and alerting reduce production incidents. |\n",
      "| **Team coordination** | Shared UI, comments, role‚Äëbased access, and version control keep everyone aligned. |\n",
      "| **Compliance & audit** | Immutable logs, tagging, and export for regulatory reporting. |\n",
      "| **Framework lock‚Äëin** | Works with LangChain, LangGraph, or raw OpenAI/Anthropic API calls. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Getting Started in Minutes\n",
      "\n",
      "1. **Create an account** at `smith.langchain.com` (Google/GitHub/email; no credit card needed).  \n",
      "2. **Generate an API key** in Settings ‚Üí API Keys.  \n",
      "3. **Install the SDK**: `pip install langsmith`.  \n",
      "4. **Add a decorator** to your code:\n",
      "\n",
      "```python\n",
      "from langsmith import trace\n",
      "\n",
      "@trace\n",
      "def answer_question(question: str) -> str:\n",
      "    # call your LLM here\n",
      "    return llm.complete(prompt=question)\n",
      "```\n",
      "\n",
      "5. **Run your function** ‚Äì traces appear instantly in the LangSmith UI.  \n",
      "6. **Open Prompt Studio** ‚Üí create a new prompt version ‚Üí test it against sample inputs.  \n",
      "7. **Create a deployment** ‚Üí point your production service to the generated endpoint.\n",
      "\n",
      "That‚Äôs it‚Äîyour LLM app now has full observability, testing, and safe deployment pipelines.\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "LangSmith is a **comprehensive, framework‚Äëagnostic platform** that gives you:\n",
      "\n",
      "* **Observability** (traces, dashboards, alerts)  \n",
      "* **Prompt engineering** (versioning, collaborative testing)  \n",
      "* **Evaluation** (built‚Äëin & custom metrics, batch runs)  \n",
      "* **Deployment** (versioned endpoints, canary releases, self‚Äëhosted option)  \n",
      "* **Collaboration & governance** (workspaces, tagging, audit logs)\n",
      "\n",
      "All of these pieces are tightly integrated, letting you move from a local notebook prototype to a production‚Äëgrade AI service with confidence and compliance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain Langsmith and its features',\n",
       " 'output': '**LangSmith ‚Äì An Observability & Development Platform for LLM Applications**  \\n\\nLangSmith (formerly the ‚ÄúLangGraph Platform‚Äù) is a SaaS product built by the LangChain team that gives developers a single place to **build, test, debug, monitor, evaluate, and deploy** any large‚Äëlanguage‚Äëmodel (LLM)‚Äëpowered application‚Äîwhether the code uses LangChain, LangGraph, or any other framework.\\n\\nBelow is a concise overview of what LangSmith is, why it matters, and the core features it provides.\\n\\n---\\n\\n## 1. What Is LangSmith?\\n\\n| Aspect | Description |\\n|--------|-------------|\\n| **Purpose** | End‚Äëto‚Äëend observability and lifecycle management for LLM apps. |\\n| **Scope** | Works with any Python (or other language) code that calls an LLM ‚Äì it is **framework‚Äëagnostic** (you can use it with LangChain, LangGraph, or pure API calls). |\\n| **Target users** | Prompt engineers, data scientists, ML engineers, product teams, and anyone who ships AI‚Äëdriven features. |\\n| **Deployment options** | Managed cloud, self‚Äëhosted, or hybrid (you choose the environment that fits your compliance and latency needs). |\\n| **Security & compliance** | HIPAA, SOC\\u202f2\\u202fType\\u202f2, GDPR‚Äëcompliant; data is encrypted at rest and in transit. |\\n\\n---\\n\\n## 2. Core Feature Set\\n\\n### 2.1 Observability (Tracing & Logging)\\n\\n| Feature | What it does | Why it matters |\\n|---------|--------------|----------------|\\n| **Request tracing** | Every LLM call (prompt, model, parameters, response) is automatically recorded as a ‚Äútrace‚Äù. | Gives you a complete audit trail to reproduce bugs or understand model behavior. |\\n| **Metadata capture** | You can attach custom tags (e.g., user ID, experiment name, environment) to each trace. | Enables powerful filtering, cost‚Äëanalysis, and downstream analytics. |\\n| **Dashboard & UI** | Visual timeline of each trace, showing prompt, model output, latency, token usage, and any downstream code execution. | Quickly spot slow steps, hallucinations, or token‚Äëbudget overruns. |\\n| **Alerting & notifications** | Set thresholds on latency, error rates, or custom metrics; receive Slack/Email/webhook alerts. | Prevents production incidents from slipping through. |\\n\\n### 2.2 Prompt Engineering & Testing (Prompt Studio)\\n\\n| Feature | What it does | Why it matters |\\n|---------|--------------|----------------|\\n| **Prompt versioning** | Every edit creates a new version that can be compared side‚Äëby‚Äëside. | Guarantees reproducibility and lets you roll back to a known‚Äëgood version. |\\n| **Collaborative editing** | Teams can comment, suggest changes, and approve prompts directly in the UI. | Reduces hand‚Äëoff friction between data scientists and product managers. |\\n| **Prompt testing sandbox** | Run a prompt against a set of test inputs, see the model‚Äôs responses, and capture metrics (accuracy, toxicity, etc.). | Enables rapid iteration before code deployment. |\\n| **A/B testing** | Deploy two prompt versions simultaneously and compare performance on live traffic. | Empirical evidence for choosing the best prompt. |\\n\\n### 2.3 Evaluation & Metrics\\n\\n| Feature | What it does | Why it matters |\\n|---------|--------------|----------------|\\n| **Built‚Äëin evaluators** | Common metrics (BLEU, ROUGE, exact match, semantic similarity, factuality, toxicity, cost). | Gives you a quantitative view of output quality. |\\n| **Custom evaluators** | Upload a Python function that receives a trace and returns a score; can be stored in the platform. | Supports domain‚Äëspecific criteria (e.g., legal compliance, business KPIs). |\\n| **Batch evaluation** | Run an evaluator over a large dataset (thousands of traces) and view aggregated results. | Helps you spot systematic failures. |\\n| **Evaluation dashboards** | Trend charts, heatmaps, and per‚Äëprompt breakdowns. | Makes it easy to monitor drift over time. |\\n\\n### 2.4 Deployment & Model Management\\n\\n| Feature | What it does | Why it matters |\\n|---------|--------------|----------------|\\n| **Deployment UI** | Define a ‚Äúdeployment‚Äù that points to a specific model, prompt version, and runtime configuration (e.g., temperature, max tokens). | Provides a stable endpoint for production services. |\\n| **Versioned deployments** | Deployments are immutable; a new version creates a new endpoint, preserving the old one. | Guarantees backward compatibility and safe rollouts. |\\n| **Canary & blue‚Äëgreen releases** | Route a fraction of traffic to a new deployment and monitor metrics before full cut‚Äëover. | Reduces risk of breaking changes. |\\n| **Self‚Äëhosted option** | Run the LangSmith server inside your VPC or on‚Äëpremises, with the same UI and API. | Meets strict data‚Äë residency or latency requirements. |\\n\\n### 2.5 Agent Builder (Beta ‚Äì No‚ÄëCode UI)\\n\\n* Drag‚Äëand‚Äëdrop interface for building multi‚Äëstep LLM agents (e.g., tool‚Äëusing assistants, retrieval‚Äëaugmented generation pipelines).  \\n* Generates runnable code that can be exported to your repo or deployed directly from the platform.  \\n* Great for rapid prototyping, proof‚Äëof‚Äëconcepts, or for product managers who want to experiment without writing code.\\n\\n### 2.6 API & SDK Integration\\n\\n| Integration | Details |\\n|------------|---------|\\n| **Python SDK** | `langsmith` package provides decorators (`@trace`, `@evaluate`) and helper functions to log traces automatically. |\\n| **REST API** | Full CRUD on traces, evaluations, deployments, and tags ‚Äì useful for custom dashboards or CI pipelines. |\\n| **LangChain & LangGraph hooks** | Out‚Äëof‚Äëthe‚Äëbox support; just enable `langsmith=True` when constructing a chain/graph. |\\n| **VS\\u202fCode & Claude extensions** | Inline ‚ÄúAsk LangSmith‚Äù commands that fetch trace details or evaluation results without leaving the editor. |\\n\\n### 2.7 Collaboration & Governance\\n\\n* **Workspaces & organizations** ‚Äì Separate environments for dev, staging, and prod; role‚Äëbased access control (viewer, editor, admin).  \\n* **Resource tagging** ‚Äì Tag traces with cost centers, project IDs, or regulatory tags for auditability.  \\n* **Export & import** ‚Äì Export trace data to CSV/JSON or import external logs for unified analysis.  \\n* **Audit logs** ‚Äì Every change to a deployment, prompt, or evaluation is recorded for compliance.\\n\\n---\\n\\n## 3. Typical Workflow\\n\\n1. **Prototype locally** ‚Äì Use the SDK to instrument your code; run a few prompts and see traces in the LangSmith UI.  \\n2. **Iterate on prompts** ‚Äì Edit, version, and test prompts in Prompt Studio; run unit‚Äëtest‚Äëstyle evaluations.  \\n3. **Define evaluations** ‚Äì Attach built‚Äëin or custom evaluators; monitor quality metrics in real time.  \\n4. **Create a deployment** ‚Äì Pin a prompt version and model configuration; expose the endpoint to your app.  \\n5. **Canary rollout** ‚Äì Shift a small % of traffic to the new deployment; watch latency, error, and evaluation dashboards.  \\n6. **Full production** ‚Äì Promote the canary to 100\\u202f% traffic; continue to monitor traces for drift.  \\n7. **Continuous improvement** ‚Äì Use the trace history to discover new failure modes, create new evaluators, and repeat.\\n\\n---\\n\\n## 4. Pricing & Availability (as of the latest docs)\\n\\n| Plan | Main limits | Who it‚Äôs for |\\n|------|-------------|--------------|\\n| **Free** | Up to 10\\u202fk traces/month, 1\\u202fk evaluation runs, community support | Individual developers, hobby projects |\\n| **Pro** | Higher trace/eval caps, private workspaces, SSO, SLA‚Äëbacked support | Small teams, startups |\\n| **Enterprise** | Unlimited usage, dedicated VPC/self‚Äëhosted option, compliance contracts, custom SLAs | Large organizations, regulated industries |\\n\\nAll plans include the same core feature set; only limits and support levels differ.\\n\\n---\\n\\n## 5. Why Use LangSmith?\\n\\n| Pain point | LangSmith solution |\\n|------------|--------------------|\\n| **Hard to reproduce bugs** | Full request trace + prompt versioning lets you replay any failure. |\\n| **No systematic way to measure output quality** | Built‚Äëin evaluators + custom functions give quantitative metrics. |\\n| **Risky rollouts** | Canary deployments, A/B testing, and alerting reduce production incidents. |\\n| **Team coordination** | Shared UI, comments, role‚Äëbased access, and version control keep everyone aligned. |\\n| **Compliance & audit** | Immutable logs, tagging, and export for regulatory reporting. |\\n| **Framework lock‚Äëin** | Works with LangChain, LangGraph, or raw OpenAI/Anthropic API calls. |\\n\\n---\\n\\n## 6. Getting Started in Minutes\\n\\n1. **Create an account** at `smith.langchain.com` (Google/GitHub/email; no credit card needed).  \\n2. **Generate an API key** in Settings ‚Üí API Keys.  \\n3. **Install the SDK**: `pip install langsmith`.  \\n4. **Add a decorator** to your code:\\n\\n```python\\nfrom langsmith import trace\\n\\n@trace\\ndef answer_question(question: str) -> str:\\n    # call your LLM here\\n    return llm.complete(prompt=question)\\n```\\n\\n5. **Run your function** ‚Äì traces appear instantly in the LangSmith UI.  \\n6. **Open Prompt Studio** ‚Üí create a new prompt version ‚Üí test it against sample inputs.  \\n7. **Create a deployment** ‚Üí point your production service to the generated endpoint.\\n\\nThat‚Äôs it‚Äîyour LLM app now has full observability, testing, and safe deployment pipelines.\\n\\n---\\n\\n### TL;DR\\n\\nLangSmith is a **comprehensive, framework‚Äëagnostic platform** that gives you:\\n\\n* **Observability** (traces, dashboards, alerts)  \\n* **Prompt engineering** (versioning, collaborative testing)  \\n* **Evaluation** (built‚Äëin & custom metrics, batch runs)  \\n* **Deployment** (versioned endpoints, canary releases, self‚Äëhosted option)  \\n* **Collaboration & governance** (workspaces, tagging, audit logs)\\n\\nAll of these pieces are tightly integrated, letting you move from a local notebook prototype to a production‚Äëgrade AI service with confidence and compliance.'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({'input':'Explain Langsmith and its features'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79b818cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Machine learning (ML)** is a subfield of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to **learn from data** and make predictions or decisions without being explicitly programmed for each specific task.\n",
      "\n",
      "### Core Idea\n",
      "- **Learning from Experience:** Just as humans improve performance on a task by practicing and observing outcomes, an ML system improves its performance by analyzing examples (data) and extracting patterns.\n",
      "- **Generalization:** The goal is not merely to memorize the training data but to **generalize** to new, unseen data so the model can make accurate predictions in real‚Äëworld situations.\n",
      "\n",
      "### How It Works (High‚ÄëLevel Steps)\n",
      "\n",
      "| Step | Description |\n",
      "|------|-------------|\n",
      "| **1. Data Collection** | Gather a dataset relevant to the problem (e.g., images, text, sensor readings). |\n",
      "| **2. Preprocessing** | Clean, normalize, and transform the data (e.g., handling missing values, scaling features). |\n",
      "| **3. Model Selection** | Choose a learning algorithm (e.g., linear regression, decision trees, neural networks). |\n",
      "| **4. Training** | Feed the data to the algorithm, allowing it to adjust internal parameters (weights, thresholds) to minimize error. |\n",
      "| **5. Validation/Evaluation** | Test the trained model on a separate validation set to assess performance and tune hyper‚Äëparameters. |\n",
      "| **6. Deployment** | Deploy the model to make predictions on new data in production. |\n",
      "| **7. Monitoring & Updating** | Continuously monitor performance and retrain as needed when data distributions shift. |\n",
      "\n",
      "### Main Categories of Machine Learning\n",
      "\n",
      "| Category | Typical Tasks | Typical Algorithms |\n",
      "|----------|---------------|--------------------|\n",
      "| **Supervised Learning** | Predict a known label from input features (e.g., classification, regression). | Linear regression, logistic regression, support vector machines, random forests, deep neural networks. |\n",
      "| **Unsupervised Learning** | Discover hidden structure in unlabeled data (e.g., clustering, dimensionality reduction). | K‚Äëmeans, hierarchical clustering, principal component analysis (PCA), autoencoders. |\n",
      "| **Semi‚ÄëSupervised Learning** | Combine a small amount of labeled data with a large amount of unlabeled data to improve learning. | Graph‚Äëbased methods, self‚Äëtraining, consistency regularization. |\n",
      "| **Reinforcement Learning** | Learn to make sequential decisions by interacting with an environment and receiving rewards/punishments. | Q‚Äëlearning, policy gradients, deep Q‚Äënetworks (DQN), proximal policy optimization (PPO). |\n",
      "| **Self‚ÄëSupervised Learning** | Generate supervisory signals from the data itself (often used in natural language processing and computer vision). | Masked language modeling (BERT), contrastive learning (SimCLR). |\n",
      "| **Transfer Learning** | Leverage knowledge from a source task/domain to improve performance on a related target task. | Fine‚Äëtuning pre‚Äëtrained language models (GPT, BERT) or vision models (ResNet, Vision Transformers). |\n",
      "\n",
      "### Key Concepts & Terminology\n",
      "\n",
      "- **Feature:** An individual measurable property or attribute used as input to a model (e.g., pixel intensity, word frequency).\n",
      "- **Label/Target:** The output the model aims to predict (e.g., cat vs. dog, house price).\n",
      "- **Training Set / Validation Set / Test Set:** Splits of the data used respectively for learning, hyper‚Äëparameter tuning, and final performance assessment.\n",
      "- **Loss Function:** A metric that quantifies how far the model‚Äôs predictions are from the true labels (e.g., mean squared error, cross‚Äëentropy).\n",
      "- **Optimization Algorithm:** Method for adjusting model parameters to minimize the loss (e.g., gradient descent, Adam).\n",
      "- **Overfitting / Underfitting:** Overfitting occurs when a model captures noise in the training data and performs poorly on new data; underfitting occurs when the model is too simple to capture underlying patterns.\n",
      "- **Regularization:** Techniques (e.g., L1/L2 penalties, dropout) that discourage overfitting.\n",
      "- **Hyper‚Äëparameters:** Settings not learned from data (e.g., learning rate, number of layers) that must be chosen before training.\n",
      "\n",
      "### Applications\n",
      "\n",
      "- **Computer Vision:** Image classification, object detection, facial recognition.\n",
      "- **Natural Language Processing:** Sentiment analysis, machine translation, chatbots, text summarization.\n",
      "- **Healthcare:** Disease diagnosis from medical images, drug discovery, patient risk prediction.\n",
      "- **Finance:** Credit scoring, algorithmic trading, fraud detection.\n",
      "- **Robotics & Autonomous Vehicles:** Perception, path planning, control.\n",
      "- **Recommendation Systems:** Product/movie recommendations, personalized content feeds.\n",
      "- **Speech & Audio:** Speech recognition, speaker identification, music recommendation.\n",
      "\n",
      "### Why It Matters\n",
      "\n",
      "- **Scalability:** ML models can automatically process massive datasets far beyond human capability.\n",
      "- **Adaptability:** They can be retrained or fine‚Äëtuned as new data becomes available, keeping systems up‚Äëto‚Äëdate.\n",
      "- **Performance:** In many domains, ML has surpassed traditional rule‚Äëbased approaches, achieving higher accuracy and efficiency.\n",
      "\n",
      "### Common Misconceptions\n",
      "\n",
      "| Misconception | Reality |\n",
      "|---------------|---------|\n",
      "| ‚ÄúMachine learning = AI‚Äù | ML is a **subset** of AI; AI also includes symbolic reasoning, expert systems, etc. |\n",
      "| ‚ÄúMore data always improves performance‚Äù | Quality matters; noisy or biased data can degrade performance or embed unfairness. |\n",
      "| ‚ÄúML models understand the world‚Äù | They capture statistical patterns, not true comprehension or reasoning. |\n",
      "| ‚ÄúAny problem can be solved with ML‚Äù | Some tasks are better addressed with deterministic algorithms, domain knowledge, or hybrid approaches. |\n",
      "\n",
      "### Getting Started (Practical Steps)\n",
      "\n",
      "1. **Pick a Problem** ‚Äì Define a clear objective (e.g., predict churn, classify images).\n",
      "2. **Gather Data** ‚Äì Use open datasets (Kaggle, UCI) or collect your own.\n",
      "3. **Explore & Visualize** ‚Äì Understand distributions, spot outliers, and assess feature relevance.\n",
      "4. **Choose a Baseline Model** ‚Äì Start simple (e.g., logistic regression) to set a performance baseline.\n",
      "5. **Iterate** ‚Äì Try more complex models, feature engineering, and hyper‚Äëparameter tuning.\n",
      "6. **Validate Properly** ‚Äì Use cross‚Äëvalidation and hold‚Äëout test sets to avoid optimistic estimates.\n",
      "7. **Deploy & Monitor** ‚Äì Serve the model via an API, track metrics, and set up alerts for drift.\n",
      "\n",
      "---\n",
      "\n",
      "**In summary:** Machine learning is the science of building systems that improve automatically through experience (data). By leveraging statistical methods and computational power, ML enables computers to perform tasks ranging from simple predictions to complex perception, fundamentally changing how we extract value from data across virtually every industry.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is machine learning',\n",
       " 'output': '**Machine learning (ML)** is a subfield of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to **learn from data** and make predictions or decisions without being explicitly programmed for each specific task.\\n\\n### Core Idea\\n- **Learning from Experience:** Just as humans improve performance on a task by practicing and observing outcomes, an ML system improves its performance by analyzing examples (data) and extracting patterns.\\n- **Generalization:** The goal is not merely to memorize the training data but to **generalize** to new, unseen data so the model can make accurate predictions in real‚Äëworld situations.\\n\\n### How It Works (High‚ÄëLevel Steps)\\n\\n| Step | Description |\\n|------|-------------|\\n| **1. Data Collection** | Gather a dataset relevant to the problem (e.g., images, text, sensor readings). |\\n| **2. Preprocessing** | Clean, normalize, and transform the data (e.g., handling missing values, scaling features). |\\n| **3. Model Selection** | Choose a learning algorithm (e.g., linear regression, decision trees, neural networks). |\\n| **4. Training** | Feed the data to the algorithm, allowing it to adjust internal parameters (weights, thresholds) to minimize error. |\\n| **5. Validation/Evaluation** | Test the trained model on a separate validation set to assess performance and tune hyper‚Äëparameters. |\\n| **6. Deployment** | Deploy the model to make predictions on new data in production. |\\n| **7. Monitoring & Updating** | Continuously monitor performance and retrain as needed when data distributions shift. |\\n\\n### Main Categories of Machine Learning\\n\\n| Category | Typical Tasks | Typical Algorithms |\\n|----------|---------------|--------------------|\\n| **Supervised Learning** | Predict a known label from input features (e.g., classification, regression). | Linear regression, logistic regression, support vector machines, random forests, deep neural networks. |\\n| **Unsupervised Learning** | Discover hidden structure in unlabeled data (e.g., clustering, dimensionality reduction). | K‚Äëmeans, hierarchical clustering, principal component analysis (PCA), autoencoders. |\\n| **Semi‚ÄëSupervised Learning** | Combine a small amount of labeled data with a large amount of unlabeled data to improve learning. | Graph‚Äëbased methods, self‚Äëtraining, consistency regularization. |\\n| **Reinforcement Learning** | Learn to make sequential decisions by interacting with an environment and receiving rewards/punishments. | Q‚Äëlearning, policy gradients, deep Q‚Äënetworks (DQN), proximal policy optimization (PPO). |\\n| **Self‚ÄëSupervised Learning** | Generate supervisory signals from the data itself (often used in natural language processing and computer vision). | Masked language modeling (BERT), contrastive learning (SimCLR). |\\n| **Transfer Learning** | Leverage knowledge from a source task/domain to improve performance on a related target task. | Fine‚Äëtuning pre‚Äëtrained language models (GPT, BERT) or vision models (ResNet, Vision Transformers). |\\n\\n### Key Concepts & Terminology\\n\\n- **Feature:** An individual measurable property or attribute used as input to a model (e.g., pixel intensity, word frequency).\\n- **Label/Target:** The output the model aims to predict (e.g., cat vs. dog, house price).\\n- **Training Set / Validation Set / Test Set:** Splits of the data used respectively for learning, hyper‚Äëparameter tuning, and final performance assessment.\\n- **Loss Function:** A metric that quantifies how far the model‚Äôs predictions are from the true labels (e.g., mean squared error, cross‚Äëentropy).\\n- **Optimization Algorithm:** Method for adjusting model parameters to minimize the loss (e.g., gradient descent, Adam).\\n- **Overfitting / Underfitting:** Overfitting occurs when a model captures noise in the training data and performs poorly on new data; underfitting occurs when the model is too simple to capture underlying patterns.\\n- **Regularization:** Techniques (e.g., L1/L2 penalties, dropout) that discourage overfitting.\\n- **Hyper‚Äëparameters:** Settings not learned from data (e.g., learning rate, number of layers) that must be chosen before training.\\n\\n### Applications\\n\\n- **Computer Vision:** Image classification, object detection, facial recognition.\\n- **Natural Language Processing:** Sentiment analysis, machine translation, chatbots, text summarization.\\n- **Healthcare:** Disease diagnosis from medical images, drug discovery, patient risk prediction.\\n- **Finance:** Credit scoring, algorithmic trading, fraud detection.\\n- **Robotics & Autonomous Vehicles:** Perception, path planning, control.\\n- **Recommendation Systems:** Product/movie recommendations, personalized content feeds.\\n- **Speech & Audio:** Speech recognition, speaker identification, music recommendation.\\n\\n### Why It Matters\\n\\n- **Scalability:** ML models can automatically process massive datasets far beyond human capability.\\n- **Adaptability:** They can be retrained or fine‚Äëtuned as new data becomes available, keeping systems up‚Äëto‚Äëdate.\\n- **Performance:** In many domains, ML has surpassed traditional rule‚Äëbased approaches, achieving higher accuracy and efficiency.\\n\\n### Common Misconceptions\\n\\n| Misconception | Reality |\\n|---------------|---------|\\n| ‚ÄúMachine learning = AI‚Äù | ML is a **subset** of AI; AI also includes symbolic reasoning, expert systems, etc. |\\n| ‚ÄúMore data always improves performance‚Äù | Quality matters; noisy or biased data can degrade performance or embed unfairness. |\\n| ‚ÄúML models understand the world‚Äù | They capture statistical patterns, not true comprehension or reasoning. |\\n| ‚ÄúAny problem can be solved with ML‚Äù | Some tasks are better addressed with deterministic algorithms, domain knowledge, or hybrid approaches. |\\n\\n### Getting Started (Practical Steps)\\n\\n1. **Pick a Problem** ‚Äì Define a clear objective (e.g., predict churn, classify images).\\n2. **Gather Data** ‚Äì Use open datasets (Kaggle, UCI) or collect your own.\\n3. **Explore & Visualize** ‚Äì Understand distributions, spot outliers, and assess feature relevance.\\n4. **Choose a Baseline Model** ‚Äì Start simple (e.g., logistic regression) to set a performance baseline.\\n5. **Iterate** ‚Äì Try more complex models, feature engineering, and hyper‚Äëparameter tuning.\\n6. **Validate Properly** ‚Äì Use cross‚Äëvalidation and hold‚Äëout test sets to avoid optimistic estimates.\\n7. **Deploy & Monitor** ‚Äì Serve the model via an API, track metrics, and set up alerts for drift.\\n\\n---\\n\\n**In summary:** Machine learning is the science of building systems that improve automatically through experience (data). By leveraging statistical methods and computational power, ML enables computers to perform tasks ranging from simple predictions to complex perception, fundamentally changing how we extract value from data across virtually every industry.'}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "29f63952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on c\u001b[0m\u001b[32;1m\u001b[1;3m**Paper ID:**‚ÄØarXiv:1706.03762  \n",
      "**Title:**‚ÄØ*Attention Is All You Need*  \n",
      "**Authors:**‚ÄØAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, ≈Åukasz Kaiser, Aidan N.‚ÄØGomez,‚ÄØIllia‚ÄØPolosukhin (Google Brain)  \n",
      "**Submitted:**‚ÄØJune‚ÄØ2017 (published in *Advances in Neural Information Processing Systems* 2017)\n",
      "\n",
      "---\n",
      "\n",
      "### What the paper is about\n",
      "\n",
      "The paper introduces the **Transformer** architecture, a new way to model sequences (e.g., sentences, time‚Äëseries) that relies **solely on attention mechanisms** and completely discards recurrence (RNNs) and convolution.\n",
      "\n",
      "#### Core ideas\n",
      "\n",
      "| Concept | Description |\n",
      "|---------|-------------|\n",
      "| **Self‚Äëattention (scaled dot‚Äëproduct attention)** | Each token computes a weighted sum of all other tokens in the same sequence, where the weights are derived from dot‚Äëproducts of learned *query*, *key*, and *value* vectors. |\n",
      "| **Multi‚Äëhead attention** | The model runs several attention ‚Äúheads‚Äù in parallel, allowing it to capture different types of relationships (e.g., syntactic vs. semantic) simultaneously. |\n",
      "| **Positional encodings** | Since attention has no intrinsic notion of order, the authors add sinusoidal (or learned) positional vectors to the input embeddings to give the model a sense of token position. |\n",
      "| **Feed‚Äëforward sub‚Äëlayers** | After each attention block, a position‚Äëwise fully‚Äëconnected feed‚Äëforward network is applied, with a ReLU activation and dropout. |\n",
      "| **Layer normalization & residual connections** | Both the attention and feed‚Äëforward sub‚Äëlayers are wrapped with residual connections followed by layer‚Äënorm, which stabilizes training. |\n",
      "| **Encoder‚Äëdecoder structure** | The model consists of a stack of identical encoder layers and a stack of identical decoder layers. The decoder attends both to its own previous outputs (masked self‚Äëattention) and to the encoder‚Äôs final representations (encoder‚Äëdecoder attention). |\n",
      "\n",
      "#### Why it matters\n",
      "\n",
      "* **Speed & parallelism:** Because there are no recurrent connections, the whole sequence can be processed in parallel on GPUs/TPUs, dramatically reducing training time compared with LSTM/GRU‚Äëbased models.\n",
      "* **State‚Äëof‚Äëthe‚Äëart performance:** On the WMT‚ÄØ2014 English‚ÄëGerman and English‚ÄëFrench translation benchmarks, the Transformer achieved **BLEU scores of 28.4 and 41.8**, respectively‚Äîsubstantially better than the previous best (which used deep LSTMs with attention).\n",
      "* **Scalability:** The architecture scales well to larger datasets and model sizes; later work (e.g., BERT, GPT, T5) builds directly on the Transformer core.\n",
      "* **General purpose:** Since its release, the Transformer has become the de‚Äëfacto backbone for virtually all modern NLP models and has also been adapted to vision (ViT), speech, reinforcement learning, and even protein folding (AlphaFold).\n",
      "\n",
      "#### Architecture snapshot\n",
      "\n",
      "```\n",
      "Input ‚Üí Embedding + Positional Encoding\n",
      "      ‚îî‚îÄ‚ñ∫ Encoder (N identical layers)\n",
      "          ‚îú‚îÄ‚ñ∫ Multi‚Äëhead Self‚ÄëAttention\n",
      "          ‚îî‚îÄ‚ñ∫ Feed‚ÄëForward Network\n",
      "      ‚îî‚îÄ‚ñ∫ Decoder (N identical layers)\n",
      "          ‚îú‚îÄ‚ñ∫ Masked Multi‚Äëhead Self‚ÄëAttention\n",
      "          ‚îú‚îÄ‚ñ∫ Multi‚Äëhead Encoder‚ÄëDecoder Attention\n",
      "          ‚îî‚îÄ‚ñ∫ Feed‚ÄëForward Network\n",
      "      ‚îî‚îÄ‚ñ∫ Linear + Softmax ‚Üí Output tokens\n",
      "```\n",
      "\n",
      "#### Key equations (simplified)\n",
      "\n",
      "1. **Scaled dot‚Äëproduct attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V)=\\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "2. **Multi‚Äëhead**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where each head \\( \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\).\n",
      "\n",
      "3. **Positional encoding** (sinusoidal)  \n",
      "   \\[\n",
      "   PE_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   PE_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "### Bottom line\n",
      "\n",
      "*‚ÄúAttention Is All You Need‚Äù* proposes a **pure‚Äëattention architecture** that replaces the traditional recurrent or convolutional components of sequence models. Its simplicity, efficiency, and strong empirical results sparked a wave of research that now dominates natural‚Äëlanguage processing, computer vision, and many other AI fields.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the paper 1706.03762 about?\",\n",
       " 'output': '**Paper ID:**\\u202farXiv:1706.03762  \\n**Title:**\\u202f*Attention Is All You Need*  \\n**Authors:**\\u202fAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, ≈Åukasz Kaiser, Aidan N.\\u202fGomez,\\u202fIllia\\u202fPolosukhin (Google Brain)  \\n**Submitted:**\\u202fJune\\u202f2017 (published in *Advances in Neural Information Processing Systems* 2017)\\n\\n---\\n\\n### What the paper is about\\n\\nThe paper introduces the **Transformer** architecture, a new way to model sequences (e.g., sentences, time‚Äëseries) that relies **solely on attention mechanisms** and completely discards recurrence (RNNs) and convolution.\\n\\n#### Core ideas\\n\\n| Concept | Description |\\n|---------|-------------|\\n| **Self‚Äëattention (scaled dot‚Äëproduct attention)** | Each token computes a weighted sum of all other tokens in the same sequence, where the weights are derived from dot‚Äëproducts of learned *query*, *key*, and *value* vectors. |\\n| **Multi‚Äëhead attention** | The model runs several attention ‚Äúheads‚Äù in parallel, allowing it to capture different types of relationships (e.g., syntactic vs. semantic) simultaneously. |\\n| **Positional encodings** | Since attention has no intrinsic notion of order, the authors add sinusoidal (or learned) positional vectors to the input embeddings to give the model a sense of token position. |\\n| **Feed‚Äëforward sub‚Äëlayers** | After each attention block, a position‚Äëwise fully‚Äëconnected feed‚Äëforward network is applied, with a ReLU activation and dropout. |\\n| **Layer normalization & residual connections** | Both the attention and feed‚Äëforward sub‚Äëlayers are wrapped with residual connections followed by layer‚Äënorm, which stabilizes training. |\\n| **Encoder‚Äëdecoder structure** | The model consists of a stack of identical encoder layers and a stack of identical decoder layers. The decoder attends both to its own previous outputs (masked self‚Äëattention) and to the encoder‚Äôs final representations (encoder‚Äëdecoder attention). |\\n\\n#### Why it matters\\n\\n* **Speed & parallelism:** Because there are no recurrent connections, the whole sequence can be processed in parallel on GPUs/TPUs, dramatically reducing training time compared with LSTM/GRU‚Äëbased models.\\n* **State‚Äëof‚Äëthe‚Äëart performance:** On the WMT\\u202f2014 English‚ÄëGerman and English‚ÄëFrench translation benchmarks, the Transformer achieved **BLEU scores of 28.4 and 41.8**, respectively‚Äîsubstantially better than the previous best (which used deep LSTMs with attention).\\n* **Scalability:** The architecture scales well to larger datasets and model sizes; later work (e.g., BERT, GPT, T5) builds directly on the Transformer core.\\n* **General purpose:** Since its release, the Transformer has become the de‚Äëfacto backbone for virtually all modern NLP models and has also been adapted to vision (ViT), speech, reinforcement learning, and even protein folding (AlphaFold).\\n\\n#### Architecture snapshot\\n\\n```\\nInput ‚Üí Embedding + Positional Encoding\\n      ‚îî‚îÄ‚ñ∫ Encoder (N identical layers)\\n          ‚îú‚îÄ‚ñ∫ Multi‚Äëhead Self‚ÄëAttention\\n          ‚îî‚îÄ‚ñ∫ Feed‚ÄëForward Network\\n      ‚îî‚îÄ‚ñ∫ Decoder (N identical layers)\\n          ‚îú‚îÄ‚ñ∫ Masked Multi‚Äëhead Self‚ÄëAttention\\n          ‚îú‚îÄ‚ñ∫ Multi‚Äëhead Encoder‚ÄëDecoder Attention\\n          ‚îî‚îÄ‚ñ∫ Feed‚ÄëForward Network\\n      ‚îî‚îÄ‚ñ∫ Linear + Softmax ‚Üí Output tokens\\n```\\n\\n#### Key equations (simplified)\\n\\n1. **Scaled dot‚Äëproduct attention**  \\n   \\\\[\\n   \\\\text{Attention}(Q,K,V)=\\\\text{softmax}\\\\!\\\\left(\\\\frac{QK^{\\\\top}}{\\\\sqrt{d_k}}\\\\right)V\\n   \\\\]\\n2. **Multi‚Äëhead**  \\n   \\\\[\\n   \\\\text{MultiHead}(Q,K,V)=\\\\text{Concat}(\\\\text{head}_1,\\\\dots,\\\\text{head}_h)W^O\\n   \\\\]\\n   where each head \\\\( \\\\text{head}_i = \\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\).\\n\\n3. **Positional encoding** (sinusoidal)  \\n   \\\\[\\n   PE_{(pos,2i)} = \\\\sin\\\\!\\\\left(\\\\frac{pos}{10000^{2i/d_{\\\\text{model}}}}\\\\right),\\\\quad\\n   PE_{(pos,2i+1)} = \\\\cos\\\\!\\\\left(\\\\frac{pos}{10000^{2i/d_{\\\\text{model}}}}\\\\right)\\n   \\\\]\\n\\n---\\n\\n### Bottom line\\n\\n*‚ÄúAttention Is All You Need‚Äù* proposes a **pure‚Äëattention architecture** that replaces the traditional recurrent or convolutional components of sequence models. Its simplicity, efficiency, and strong empirical results sparked a wave of research that now dominates natural‚Äëlanguage processing, computer vision, and many other AI fields.'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What's the paper 1706.03762 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59c280",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
